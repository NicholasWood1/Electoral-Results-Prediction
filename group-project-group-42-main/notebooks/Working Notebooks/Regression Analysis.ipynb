{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1bc542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b799925",
   "metadata": {},
   "outputs": [],
   "source": [
    "election_data_path = '../../datasets/Clean Datasets/Electoral Data/'\n",
    "census_data_path = '../../datasets/Clean Datasets/Census Data/'\n",
    "master_data_path = '../../datasets/Clean Datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59df850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = pd.read_csv(election_data_path + 'Final_Vote_Count.csv')\n",
    "data = pd.read_csv(master_data_path + 'CED to All Demographics and Results.csv')\n",
    "ages = pd.read_csv(census_data_path + 'CED to Age (Individual years).csv')\n",
    "master = pd.read_csv(master_data_path + 'CED to All Demographics and Results Norm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326fb3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>LGA</th>\n",
       "      <th>AGE5P - 0-4 years</th>\n",
       "      <th>AGE5P - 5-9 years</th>\n",
       "      <th>AGE5P - 10-14 years</th>\n",
       "      <th>AGE5P - 15-19 years</th>\n",
       "      <th>AGE5P - 20-24 years</th>\n",
       "      <th>AGE5P - 25-29 years</th>\n",
       "      <th>AGE5P - 30-34 years</th>\n",
       "      <th>AGE5P - 35-39 years</th>\n",
       "      <th>...</th>\n",
       "      <th>RELP - Inadequately described</th>\n",
       "      <th>RELP - Not stated</th>\n",
       "      <th>SEXP - Male</th>\n",
       "      <th>SEXP - Female</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "      <th>MDCP - Married in a de facto marriage</th>\n",
       "      <th>MDCP - Not married</th>\n",
       "      <th>MDCP - Not applicable</th>\n",
       "      <th>PreferencePercent</th>\n",
       "      <th>PartyNm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>0.050723</td>\n",
       "      <td>0.045462</td>\n",
       "      <td>0.057361</td>\n",
       "      <td>0.096777</td>\n",
       "      <td>0.092626</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.089304</td>\n",
       "      <td>0.495587</td>\n",
       "      <td>0.504413</td>\n",
       "      <td>0.302321</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.364082</td>\n",
       "      <td>0.259811</td>\n",
       "      <td>58.18</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Aston</td>\n",
       "      <td>0.058553</td>\n",
       "      <td>0.058845</td>\n",
       "      <td>0.058346</td>\n",
       "      <td>0.064261</td>\n",
       "      <td>0.066246</td>\n",
       "      <td>0.064391</td>\n",
       "      <td>0.067479</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.070051</td>\n",
       "      <td>0.488707</td>\n",
       "      <td>0.511293</td>\n",
       "      <td>0.393949</td>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.305368</td>\n",
       "      <td>0.239929</td>\n",
       "      <td>60.13</td>\n",
       "      <td>Liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>0.062608</td>\n",
       "      <td>0.066055</td>\n",
       "      <td>0.061419</td>\n",
       "      <td>0.062910</td>\n",
       "      <td>0.065860</td>\n",
       "      <td>0.060216</td>\n",
       "      <td>0.059006</td>\n",
       "      <td>0.059537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005846</td>\n",
       "      <td>0.089031</td>\n",
       "      <td>0.483762</td>\n",
       "      <td>0.516238</td>\n",
       "      <td>0.323363</td>\n",
       "      <td>0.081842</td>\n",
       "      <td>0.301819</td>\n",
       "      <td>0.292976</td>\n",
       "      <td>60.98</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.059368</td>\n",
       "      <td>0.055980</td>\n",
       "      <td>0.060472</td>\n",
       "      <td>0.068598</td>\n",
       "      <td>0.074162</td>\n",
       "      <td>0.073559</td>\n",
       "      <td>0.066018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>0.069741</td>\n",
       "      <td>0.486358</td>\n",
       "      <td>0.513642</td>\n",
       "      <td>0.385881</td>\n",
       "      <td>0.044820</td>\n",
       "      <td>0.335227</td>\n",
       "      <td>0.234073</td>\n",
       "      <td>56.26</td>\n",
       "      <td>Liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Barker</td>\n",
       "      <td>0.055697</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>0.062771</td>\n",
       "      <td>0.059757</td>\n",
       "      <td>0.050169</td>\n",
       "      <td>0.051558</td>\n",
       "      <td>0.054241</td>\n",
       "      <td>0.054597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.092769</td>\n",
       "      <td>0.505114</td>\n",
       "      <td>0.494886</td>\n",
       "      <td>0.359438</td>\n",
       "      <td>0.078180</td>\n",
       "      <td>0.273848</td>\n",
       "      <td>0.288534</td>\n",
       "      <td>68.94</td>\n",
       "      <td>Liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>164</td>\n",
       "      <td>Werriwa</td>\n",
       "      <td>0.072655</td>\n",
       "      <td>0.078559</td>\n",
       "      <td>0.077535</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.072380</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>0.071789</td>\n",
       "      <td>0.070232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.083012</td>\n",
       "      <td>0.493321</td>\n",
       "      <td>0.506679</td>\n",
       "      <td>0.358736</td>\n",
       "      <td>0.039473</td>\n",
       "      <td>0.313827</td>\n",
       "      <td>0.287964</td>\n",
       "      <td>55.47</td>\n",
       "      <td>Labor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>165</td>\n",
       "      <td>Whitlam</td>\n",
       "      <td>0.060113</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>0.064873</td>\n",
       "      <td>0.066358</td>\n",
       "      <td>0.055673</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.073615</td>\n",
       "      <td>0.485514</td>\n",
       "      <td>0.514486</td>\n",
       "      <td>0.366147</td>\n",
       "      <td>0.067582</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.271771</td>\n",
       "      <td>60.91</td>\n",
       "      <td>Labor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>166</td>\n",
       "      <td>Wide Bay</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.060062</td>\n",
       "      <td>0.064301</td>\n",
       "      <td>0.056468</td>\n",
       "      <td>0.042020</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.049902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.494793</td>\n",
       "      <td>0.505207</td>\n",
       "      <td>0.350901</td>\n",
       "      <td>0.080181</td>\n",
       "      <td>0.284154</td>\n",
       "      <td>0.284764</td>\n",
       "      <td>63.15</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>167</td>\n",
       "      <td>Wills</td>\n",
       "      <td>0.062568</td>\n",
       "      <td>0.052921</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.043616</td>\n",
       "      <td>0.080062</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>0.108720</td>\n",
       "      <td>0.087122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.085782</td>\n",
       "      <td>0.489373</td>\n",
       "      <td>0.510627</td>\n",
       "      <td>0.304067</td>\n",
       "      <td>0.093583</td>\n",
       "      <td>0.361030</td>\n",
       "      <td>0.241320</td>\n",
       "      <td>58.17</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>168</td>\n",
       "      <td>Wright</td>\n",
       "      <td>0.060883</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>0.075367</td>\n",
       "      <td>0.070010</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.061605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.097011</td>\n",
       "      <td>0.498322</td>\n",
       "      <td>0.501678</td>\n",
       "      <td>0.362536</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>0.253436</td>\n",
       "      <td>0.302479</td>\n",
       "      <td>64.58</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       LGA  AGE5P - 0-4 years  AGE5P - 5-9 years  \\\n",
       "0        0  Adelaide           0.053945           0.050723   \n",
       "1        1     Aston           0.058553           0.058845   \n",
       "2        2  Ballarat           0.062608           0.066055   \n",
       "3        3     Banks           0.060613           0.059368   \n",
       "4        4    Barker           0.055697           0.063267   \n",
       "..     ...       ...                ...                ...   \n",
       "146    164   Werriwa           0.072655           0.078559   \n",
       "147    165   Whitlam           0.060113           0.065839   \n",
       "148    166  Wide Bay           0.048858           0.060062   \n",
       "149    167     Wills           0.062568           0.052921   \n",
       "150    168    Wright           0.060883           0.074120   \n",
       "\n",
       "     AGE5P - 10-14 years  AGE5P - 15-19 years  AGE5P - 20-24 years  \\\n",
       "0               0.045462             0.057361             0.096777   \n",
       "1               0.058346             0.064261             0.066246   \n",
       "2               0.061419             0.062910             0.065860   \n",
       "3               0.055980             0.060472             0.068598   \n",
       "4               0.062771             0.059757             0.050169   \n",
       "..                   ...                  ...                  ...   \n",
       "146             0.077535             0.077236             0.072380   \n",
       "147             0.064873             0.066358             0.055673   \n",
       "148             0.064301             0.056468             0.042020   \n",
       "149             0.043203             0.043616             0.080062   \n",
       "150             0.075367             0.070010             0.055936   \n",
       "\n",
       "     AGE5P - 25-29 years  AGE5P - 30-34 years  AGE5P - 35-39 years  ...  \\\n",
       "0               0.092626             0.088211             0.070221  ...   \n",
       "1               0.064391             0.067479             0.066700  ...   \n",
       "2               0.060216             0.059006             0.059537  ...   \n",
       "3               0.074162             0.073559             0.066018  ...   \n",
       "4               0.051558             0.054241             0.054597  ...   \n",
       "..                   ...                  ...                  ...  ...   \n",
       "146             0.065587             0.071789             0.070232  ...   \n",
       "147             0.052804             0.056448             0.056678  ...   \n",
       "148             0.040191             0.044199             0.049902  ...   \n",
       "149             0.112030             0.108720             0.087122  ...   \n",
       "150             0.051039             0.056818             0.061605  ...   \n",
       "\n",
       "     RELP - Inadequately described  RELP - Not stated  SEXP - Male  \\\n",
       "0                         0.005066           0.089304     0.495587   \n",
       "1                         0.005405           0.070051     0.488707   \n",
       "2                         0.005846           0.089031     0.483762   \n",
       "3                         0.002856           0.069741     0.486358   \n",
       "4                         0.004301           0.092769     0.505114   \n",
       "..                             ...                ...          ...   \n",
       "146                       0.002762           0.083012     0.493321   \n",
       "147                       0.003842           0.073615     0.485514   \n",
       "148                       0.004758           0.104341     0.494793   \n",
       "149                       0.006079           0.085782     0.489373   \n",
       "150                       0.004098           0.097011     0.498322   \n",
       "\n",
       "     SEXP - Female  MDCP - Married in a registered marriage  \\\n",
       "0         0.504413                                 0.302321   \n",
       "1         0.511293                                 0.393949   \n",
       "2         0.516238                                 0.323363   \n",
       "3         0.513642                                 0.385881   \n",
       "4         0.494886                                 0.359438   \n",
       "..             ...                                      ...   \n",
       "146       0.506679                                 0.358736   \n",
       "147       0.514486                                 0.366147   \n",
       "148       0.505207                                 0.350901   \n",
       "149       0.510627                                 0.304067   \n",
       "150       0.501678                                 0.362536   \n",
       "\n",
       "     MDCP - Married in a de facto marriage  MDCP - Not married  \\\n",
       "0                                 0.073786            0.364082   \n",
       "1                                 0.060754            0.305368   \n",
       "2                                 0.081842            0.301819   \n",
       "3                                 0.044820            0.335227   \n",
       "4                                 0.078180            0.273848   \n",
       "..                                     ...                 ...   \n",
       "146                               0.039473            0.313827   \n",
       "147                               0.067582            0.294500   \n",
       "148                               0.080181            0.284154   \n",
       "149                               0.093583            0.361030   \n",
       "150                               0.081549            0.253436   \n",
       "\n",
       "     MDCP - Not applicable  PreferencePercent  \\\n",
       "0                 0.259811              58.18   \n",
       "1                 0.239929              60.13   \n",
       "2                 0.292976              60.98   \n",
       "3                 0.234073              56.26   \n",
       "4                 0.288534              68.94   \n",
       "..                     ...                ...   \n",
       "146               0.287964              55.47   \n",
       "147               0.271771              60.91   \n",
       "148               0.284764              63.15   \n",
       "149               0.241320              58.17   \n",
       "150               0.302479              64.58   \n",
       "\n",
       "                                  PartyNm  \n",
       "0                  Australian Labor Party  \n",
       "1                                 Liberal  \n",
       "2                  Australian Labor Party  \n",
       "3                                 Liberal  \n",
       "4                                 Liberal  \n",
       "..                                    ...  \n",
       "146                                 Labor  \n",
       "147                                 Labor  \n",
       "148  Liberal National Party of Queensland  \n",
       "149                Australian Labor Party  \n",
       "150  Liberal National Party of Queensland  \n",
       "\n",
       "[151 rows x 142 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "No_NA_master = master.dropna().reset_index()\n",
    "No_NA_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42a5042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Australian Labor Party',\n",
       " 'Australian Labor Party (Northern Territory) Branch',\n",
       " 'Centre Alliance',\n",
       " 'Independent',\n",
       " \"Katter's Australian Party (KAP)\",\n",
       " 'Labor',\n",
       " 'Liberal',\n",
       " 'Liberal National Party of Queensland',\n",
       " 'The Greens (VIC)',\n",
       " 'The Nationals'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(No_NA_master['PartyNm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c979e",
   "metadata": {},
   "source": [
    "# 1) Adjusted Model partitioning into LNP, ALP, Other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65485614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>LGA</th>\n",
       "      <th>AGE5P - 0-4 years</th>\n",
       "      <th>AGE5P - 5-9 years</th>\n",
       "      <th>AGE5P - 10-14 years</th>\n",
       "      <th>AGE5P - 15-19 years</th>\n",
       "      <th>AGE5P - 20-24 years</th>\n",
       "      <th>AGE5P - 25-29 years</th>\n",
       "      <th>AGE5P - 30-34 years</th>\n",
       "      <th>AGE5P - 35-39 years</th>\n",
       "      <th>...</th>\n",
       "      <th>RELP - Inadequately described</th>\n",
       "      <th>RELP - Not stated</th>\n",
       "      <th>SEXP - Male</th>\n",
       "      <th>SEXP - Female</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "      <th>MDCP - Married in a de facto marriage</th>\n",
       "      <th>MDCP - Not married</th>\n",
       "      <th>MDCP - Not applicable</th>\n",
       "      <th>PreferencePercent</th>\n",
       "      <th>PartyNm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>0.050723</td>\n",
       "      <td>0.045462</td>\n",
       "      <td>0.057361</td>\n",
       "      <td>0.096777</td>\n",
       "      <td>0.092626</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.089304</td>\n",
       "      <td>0.495587</td>\n",
       "      <td>0.504413</td>\n",
       "      <td>0.302321</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.364082</td>\n",
       "      <td>0.259811</td>\n",
       "      <td>58.18</td>\n",
       "      <td>ALP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Aston</td>\n",
       "      <td>0.058553</td>\n",
       "      <td>0.058845</td>\n",
       "      <td>0.058346</td>\n",
       "      <td>0.064261</td>\n",
       "      <td>0.066246</td>\n",
       "      <td>0.064391</td>\n",
       "      <td>0.067479</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.070051</td>\n",
       "      <td>0.488707</td>\n",
       "      <td>0.511293</td>\n",
       "      <td>0.393949</td>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.305368</td>\n",
       "      <td>0.239929</td>\n",
       "      <td>60.13</td>\n",
       "      <td>LNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>0.062608</td>\n",
       "      <td>0.066055</td>\n",
       "      <td>0.061419</td>\n",
       "      <td>0.062910</td>\n",
       "      <td>0.065860</td>\n",
       "      <td>0.060216</td>\n",
       "      <td>0.059006</td>\n",
       "      <td>0.059537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005846</td>\n",
       "      <td>0.089031</td>\n",
       "      <td>0.483762</td>\n",
       "      <td>0.516238</td>\n",
       "      <td>0.323363</td>\n",
       "      <td>0.081842</td>\n",
       "      <td>0.301819</td>\n",
       "      <td>0.292976</td>\n",
       "      <td>60.98</td>\n",
       "      <td>ALP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.059368</td>\n",
       "      <td>0.055980</td>\n",
       "      <td>0.060472</td>\n",
       "      <td>0.068598</td>\n",
       "      <td>0.074162</td>\n",
       "      <td>0.073559</td>\n",
       "      <td>0.066018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>0.069741</td>\n",
       "      <td>0.486358</td>\n",
       "      <td>0.513642</td>\n",
       "      <td>0.385881</td>\n",
       "      <td>0.044820</td>\n",
       "      <td>0.335227</td>\n",
       "      <td>0.234073</td>\n",
       "      <td>56.26</td>\n",
       "      <td>LNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Barker</td>\n",
       "      <td>0.055697</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>0.062771</td>\n",
       "      <td>0.059757</td>\n",
       "      <td>0.050169</td>\n",
       "      <td>0.051558</td>\n",
       "      <td>0.054241</td>\n",
       "      <td>0.054597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.092769</td>\n",
       "      <td>0.505114</td>\n",
       "      <td>0.494886</td>\n",
       "      <td>0.359438</td>\n",
       "      <td>0.078180</td>\n",
       "      <td>0.273848</td>\n",
       "      <td>0.288534</td>\n",
       "      <td>68.94</td>\n",
       "      <td>LNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>164</td>\n",
       "      <td>Werriwa</td>\n",
       "      <td>0.072655</td>\n",
       "      <td>0.078559</td>\n",
       "      <td>0.077535</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.072380</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>0.071789</td>\n",
       "      <td>0.070232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.083012</td>\n",
       "      <td>0.493321</td>\n",
       "      <td>0.506679</td>\n",
       "      <td>0.358736</td>\n",
       "      <td>0.039473</td>\n",
       "      <td>0.313827</td>\n",
       "      <td>0.287964</td>\n",
       "      <td>55.47</td>\n",
       "      <td>ALP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>165</td>\n",
       "      <td>Whitlam</td>\n",
       "      <td>0.060113</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>0.064873</td>\n",
       "      <td>0.066358</td>\n",
       "      <td>0.055673</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.073615</td>\n",
       "      <td>0.485514</td>\n",
       "      <td>0.514486</td>\n",
       "      <td>0.366147</td>\n",
       "      <td>0.067582</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.271771</td>\n",
       "      <td>60.91</td>\n",
       "      <td>ALP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>166</td>\n",
       "      <td>Wide Bay</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.060062</td>\n",
       "      <td>0.064301</td>\n",
       "      <td>0.056468</td>\n",
       "      <td>0.042020</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.049902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.494793</td>\n",
       "      <td>0.505207</td>\n",
       "      <td>0.350901</td>\n",
       "      <td>0.080181</td>\n",
       "      <td>0.284154</td>\n",
       "      <td>0.284764</td>\n",
       "      <td>63.15</td>\n",
       "      <td>LNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>167</td>\n",
       "      <td>Wills</td>\n",
       "      <td>0.062568</td>\n",
       "      <td>0.052921</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.043616</td>\n",
       "      <td>0.080062</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>0.108720</td>\n",
       "      <td>0.087122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.085782</td>\n",
       "      <td>0.489373</td>\n",
       "      <td>0.510627</td>\n",
       "      <td>0.304067</td>\n",
       "      <td>0.093583</td>\n",
       "      <td>0.361030</td>\n",
       "      <td>0.241320</td>\n",
       "      <td>58.17</td>\n",
       "      <td>ALP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>168</td>\n",
       "      <td>Wright</td>\n",
       "      <td>0.060883</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>0.075367</td>\n",
       "      <td>0.070010</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.061605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.097011</td>\n",
       "      <td>0.498322</td>\n",
       "      <td>0.501678</td>\n",
       "      <td>0.362536</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>0.253436</td>\n",
       "      <td>0.302479</td>\n",
       "      <td>64.58</td>\n",
       "      <td>LNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       LGA  AGE5P - 0-4 years  AGE5P - 5-9 years  \\\n",
       "0        0  Adelaide           0.053945           0.050723   \n",
       "1        1     Aston           0.058553           0.058845   \n",
       "2        2  Ballarat           0.062608           0.066055   \n",
       "3        3     Banks           0.060613           0.059368   \n",
       "4        4    Barker           0.055697           0.063267   \n",
       "..     ...       ...                ...                ...   \n",
       "146    164   Werriwa           0.072655           0.078559   \n",
       "147    165   Whitlam           0.060113           0.065839   \n",
       "148    166  Wide Bay           0.048858           0.060062   \n",
       "149    167     Wills           0.062568           0.052921   \n",
       "150    168    Wright           0.060883           0.074120   \n",
       "\n",
       "     AGE5P - 10-14 years  AGE5P - 15-19 years  AGE5P - 20-24 years  \\\n",
       "0               0.045462             0.057361             0.096777   \n",
       "1               0.058346             0.064261             0.066246   \n",
       "2               0.061419             0.062910             0.065860   \n",
       "3               0.055980             0.060472             0.068598   \n",
       "4               0.062771             0.059757             0.050169   \n",
       "..                   ...                  ...                  ...   \n",
       "146             0.077535             0.077236             0.072380   \n",
       "147             0.064873             0.066358             0.055673   \n",
       "148             0.064301             0.056468             0.042020   \n",
       "149             0.043203             0.043616             0.080062   \n",
       "150             0.075367             0.070010             0.055936   \n",
       "\n",
       "     AGE5P - 25-29 years  AGE5P - 30-34 years  AGE5P - 35-39 years  ...  \\\n",
       "0               0.092626             0.088211             0.070221  ...   \n",
       "1               0.064391             0.067479             0.066700  ...   \n",
       "2               0.060216             0.059006             0.059537  ...   \n",
       "3               0.074162             0.073559             0.066018  ...   \n",
       "4               0.051558             0.054241             0.054597  ...   \n",
       "..                   ...                  ...                  ...  ...   \n",
       "146             0.065587             0.071789             0.070232  ...   \n",
       "147             0.052804             0.056448             0.056678  ...   \n",
       "148             0.040191             0.044199             0.049902  ...   \n",
       "149             0.112030             0.108720             0.087122  ...   \n",
       "150             0.051039             0.056818             0.061605  ...   \n",
       "\n",
       "     RELP - Inadequately described  RELP - Not stated  SEXP - Male  \\\n",
       "0                         0.005066           0.089304     0.495587   \n",
       "1                         0.005405           0.070051     0.488707   \n",
       "2                         0.005846           0.089031     0.483762   \n",
       "3                         0.002856           0.069741     0.486358   \n",
       "4                         0.004301           0.092769     0.505114   \n",
       "..                             ...                ...          ...   \n",
       "146                       0.002762           0.083012     0.493321   \n",
       "147                       0.003842           0.073615     0.485514   \n",
       "148                       0.004758           0.104341     0.494793   \n",
       "149                       0.006079           0.085782     0.489373   \n",
       "150                       0.004098           0.097011     0.498322   \n",
       "\n",
       "     SEXP - Female  MDCP - Married in a registered marriage  \\\n",
       "0         0.504413                                 0.302321   \n",
       "1         0.511293                                 0.393949   \n",
       "2         0.516238                                 0.323363   \n",
       "3         0.513642                                 0.385881   \n",
       "4         0.494886                                 0.359438   \n",
       "..             ...                                      ...   \n",
       "146       0.506679                                 0.358736   \n",
       "147       0.514486                                 0.366147   \n",
       "148       0.505207                                 0.350901   \n",
       "149       0.510627                                 0.304067   \n",
       "150       0.501678                                 0.362536   \n",
       "\n",
       "     MDCP - Married in a de facto marriage  MDCP - Not married  \\\n",
       "0                                 0.073786            0.364082   \n",
       "1                                 0.060754            0.305368   \n",
       "2                                 0.081842            0.301819   \n",
       "3                                 0.044820            0.335227   \n",
       "4                                 0.078180            0.273848   \n",
       "..                                     ...                 ...   \n",
       "146                               0.039473            0.313827   \n",
       "147                               0.067582            0.294500   \n",
       "148                               0.080181            0.284154   \n",
       "149                               0.093583            0.361030   \n",
       "150                               0.081549            0.253436   \n",
       "\n",
       "     MDCP - Not applicable  PreferencePercent  PartyNm  \n",
       "0                 0.259811              58.18      ALP  \n",
       "1                 0.239929              60.13      LNP  \n",
       "2                 0.292976              60.98      ALP  \n",
       "3                 0.234073              56.26      LNP  \n",
       "4                 0.288534              68.94      LNP  \n",
       "..                     ...                ...      ...  \n",
       "146               0.287964              55.47      ALP  \n",
       "147               0.271771              60.91      ALP  \n",
       "148               0.284764              63.15      LNP  \n",
       "149               0.241320              58.17      ALP  \n",
       "150               0.302479              64.58      LNP  \n",
       "\n",
       "[151 rows x 142 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_master = No_NA_master.replace({ 'PartyNm' : {'Liberal' : 'LNP', 'Liberal National Party of Queensland' : 'LNP',\n",
    "                                                      'The Nationals' : 'LNP', 'Labor' : 'ALP',\n",
    "                                                      'Australian Labor Party' : 'ALP',\n",
    "                                                     'Australian Labor Party (Northern Territory) Branch' : 'ALP',\n",
    "                                                      'Centre Alliance' : 'Other', 'Independent' : 'Other', \n",
    "                                                      'Katter\\'s Australian Party (KAP)' : 'Other',\n",
    "                                                      'The Greens (VIC)' : 'Other'}})\n",
    "\n",
    "adjusted_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b84352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjusted_master[adjusted_master['PartyNm'] == 'LNP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73baf17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjusted_master[adjusted_master['PartyNm'] == 'ALP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f420e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjusted_master[adjusted_master['PartyNm'] == 'Other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377231d",
   "metadata": {},
   "source": [
    "# 2) Strength of victory model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4eaac",
   "metadata": {},
   "source": [
    "Definitions by the AEC https://www.aec.gov.au/footer/glossary.htm#:~:text=Marginal%20seat,per%20cent%20of%20the%20vote.\n",
    "\n",
    "- Marginal seat 56% or less\n",
    "- Fairly safe seat 56-60%\n",
    "- Safe seat 60% or greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4410b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = No_NA_master[['LGA', 'PartyNm','PreferencePercent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97b1c555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Australian Labor Party',\n",
       " 'Independent',\n",
       " \"Katter's Australian Party (KAP)\",\n",
       " 'Labor',\n",
       " 'Liberal',\n",
       " 'Liberal National Party of Queensland',\n",
       " 'The Greens (VIC)',\n",
       " 'The Nationals'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Safe = strength[strength['PreferencePercent'] >= 60]\n",
    "set(Safe['PartyNm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b696fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LGA</th>\n",
       "      <th>PartyNm</th>\n",
       "      <th>PreferencePercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelaide</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>58.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banks</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barton</td>\n",
       "      <td>Labor</td>\n",
       "      <td>59.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bean</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bendigo</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>59.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bennelong</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>56.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bonner</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "      <td>57.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Brand</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>56.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Cowper</td>\n",
       "      <td>The Nationals</td>\n",
       "      <td>56.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Flynn</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "      <td>58.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Forde</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "      <td>58.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Fremantle</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>56.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Goldstein</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>57.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Herbert</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "      <td>58.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Hindmarsh</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>56.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Holt</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>58.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Hughes</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>59.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Isaacs</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>56.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Jagajaga</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>56.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Kingsford Smith</td>\n",
       "      <td>Labor</td>\n",
       "      <td>58.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Macarthur</td>\n",
       "      <td>Labor</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Macnamara</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>56.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Makin</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>59.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>McMahon</td>\n",
       "      <td>Labor</td>\n",
       "      <td>56.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Menzies</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>57.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Monash</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>57.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>North Sydney</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>59.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Oxley</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Page</td>\n",
       "      <td>The Nationals</td>\n",
       "      <td>59.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Pearce</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Petrie</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Rankin</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>56.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Ryan</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "      <td>56.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Sturt</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>56.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Warringah</td>\n",
       "      <td>Independent</td>\n",
       "      <td>57.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Wills</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>58.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 LGA                               PartyNm  PreferencePercent\n",
       "0           Adelaide                Australian Labor Party              58.18\n",
       "3              Banks                               Liberal              56.26\n",
       "5             Barton                                 Labor              59.41\n",
       "7               Bean                Australian Labor Party              57.52\n",
       "8            Bendigo                Australian Labor Party              59.04\n",
       "9          Bennelong                               Liberal              56.91\n",
       "13            Bonner  Liberal National Party of Queensland              57.41\n",
       "18             Brand                Australian Labor Party              56.66\n",
       "36            Cowper                         The Nationals              56.79\n",
       "52             Flynn  Liberal National Party of Queensland              58.66\n",
       "53             Forde  Liberal National Party of Queensland              58.60\n",
       "58         Fremantle                Australian Labor Party              56.92\n",
       "62         Goldstein                               Liberal              57.79\n",
       "70           Herbert  Liberal National Party of Queensland              58.36\n",
       "72         Hindmarsh                Australian Labor Party              56.54\n",
       "74              Holt                Australian Labor Party              58.70\n",
       "76            Hughes                               Liberal              59.85\n",
       "80            Isaacs                Australian Labor Party              56.43\n",
       "81          Jagajaga                Australian Labor Party              56.57\n",
       "83   Kingsford Smith                                 Labor              58.81\n",
       "95         Macarthur                                 Labor              58.40\n",
       "97         Macnamara                Australian Labor Party              56.25\n",
       "99             Makin                Australian Labor Party              59.72\n",
       "105          McMahon                                 Labor              56.64\n",
       "108          Menzies                               Liberal              57.53\n",
       "110           Monash                               Liberal              57.36\n",
       "117     North Sydney                               Liberal              59.27\n",
       "119            Oxley                Australian Labor Party              56.39\n",
       "120             Page                         The Nationals              59.45\n",
       "124           Pearce                               Liberal              57.52\n",
       "126           Petrie  Liberal National Party of Queensland              58.40\n",
       "127           Rankin                Australian Labor Party              56.44\n",
       "132             Ryan  Liberal National Party of Queensland              56.03\n",
       "138            Sturt                               Liberal              56.87\n",
       "143        Warringah                           Independent              57.24\n",
       "149            Wills                Australian Labor Party              58.17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strength[(strength['PreferencePercent'] < 60) & (strength['PreferencePercent'] > 56)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f470eb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LGA</th>\n",
       "      <th>PartyNm</th>\n",
       "      <th>PreferencePercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bass</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>50.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Blair</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>51.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boothby</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>51.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Braddon</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>53.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "      <td>54.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Burt</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>54.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Casey</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>54.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Chisholm</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>50.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Corangamite</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>51.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Cowan</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>50.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Deakin</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>54.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Dickson</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "      <td>54.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Dobell</td>\n",
       "      <td>Labor</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Dunkley</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>52.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Eden-Monaro</td>\n",
       "      <td>Labor</td>\n",
       "      <td>50.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Flinders</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>55.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Gilmore</td>\n",
       "      <td>Labor</td>\n",
       "      <td>52.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Greenway</td>\n",
       "      <td>Labor</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Griffith</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>52.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Hasluck</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>55.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Higgins</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Hotham</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>55.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Hunter</td>\n",
       "      <td>Labor</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Indi</td>\n",
       "      <td>Independent</td>\n",
       "      <td>51.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Kooyong</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>55.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>La Trobe</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>54.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Leichhardt</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "      <td>54.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Lilley</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>50.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Lindsay</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Lingiari</td>\n",
       "      <td>Australian Labor Party (Northern Territory) Br...</td>\n",
       "      <td>55.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Longman</td>\n",
       "      <td>Liberal National Party of Queensland</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Lyons</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>55.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Macquarie</td>\n",
       "      <td>Labor</td>\n",
       "      <td>50.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Mayo</td>\n",
       "      <td>Centre Alliance</td>\n",
       "      <td>55.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>McEwen</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>55.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Moreton</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>51.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Parramatta</td>\n",
       "      <td>Labor</td>\n",
       "      <td>53.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Paterson</td>\n",
       "      <td>Labor</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Perth</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>54.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Reid</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Richmond</td>\n",
       "      <td>Labor</td>\n",
       "      <td>54.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Robertson</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>54.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Shortland</td>\n",
       "      <td>Labor</td>\n",
       "      <td>54.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Solomon</td>\n",
       "      <td>Australian Labor Party (Northern Territory) Br...</td>\n",
       "      <td>53.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Stirling</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>55.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Swan</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>52.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Wentworth</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>51.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Werriwa</td>\n",
       "      <td>Labor</td>\n",
       "      <td>55.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LGA                                            PartyNm  \\\n",
       "6           Bass                                            Liberal   \n",
       "11         Blair                             Australian Labor Party   \n",
       "14       Boothby                                            Liberal   \n",
       "16       Braddon                                            Liberal   \n",
       "19      Brisbane               Liberal National Party of Queensland   \n",
       "21          Burt                             Australian Labor Party   \n",
       "27         Casey                                            Liberal   \n",
       "29      Chisholm                                            Liberal   \n",
       "33   Corangamite                             Australian Labor Party   \n",
       "35         Cowan                             Australian Labor Party   \n",
       "40        Deakin                                            Liberal   \n",
       "41       Dickson               Liberal National Party of Queensland   \n",
       "42        Dobell                                              Labor   \n",
       "43       Dunkley                             Australian Labor Party   \n",
       "45   Eden-Monaro                                              Labor   \n",
       "51      Flinders                                            Liberal   \n",
       "60       Gilmore                                              Labor   \n",
       "65      Greenway                                              Labor   \n",
       "67      Griffith                             Australian Labor Party   \n",
       "69       Hasluck                                            Liberal   \n",
       "71       Higgins                                            Liberal   \n",
       "75        Hotham                             Australian Labor Party   \n",
       "78        Hunter                                              Labor   \n",
       "79          Indi                                        Independent   \n",
       "85       Kooyong                                            Liberal   \n",
       "86      La Trobe                                            Liberal   \n",
       "88    Leichhardt               Liberal National Party of Queensland   \n",
       "89        Lilley                             Australian Labor Party   \n",
       "90       Lindsay                                            Liberal   \n",
       "91      Lingiari  Australian Labor Party (Northern Territory) Br...   \n",
       "92       Longman               Liberal National Party of Queensland   \n",
       "94         Lyons                             Australian Labor Party   \n",
       "98     Macquarie                                              Labor   \n",
       "103         Mayo                                    Centre Alliance   \n",
       "104       McEwen                             Australian Labor Party   \n",
       "113      Moreton                             Australian Labor Party   \n",
       "122   Parramatta                                              Labor   \n",
       "123     Paterson                                              Labor   \n",
       "125        Perth                             Australian Labor Party   \n",
       "128         Reid                                            Liberal   \n",
       "129     Richmond                                              Labor   \n",
       "131    Robertson                                            Liberal   \n",
       "134    Shortland                                              Labor   \n",
       "135      Solomon  Australian Labor Party (Northern Territory) Br...   \n",
       "137     Stirling                                            Liberal   \n",
       "139         Swan                                            Liberal   \n",
       "145    Wentworth                                            Liberal   \n",
       "146      Werriwa                                              Labor   \n",
       "\n",
       "     PreferencePercent  \n",
       "6                50.41  \n",
       "11               51.21  \n",
       "14               51.38  \n",
       "16               53.09  \n",
       "19               54.92  \n",
       "21               54.99  \n",
       "27               54.64  \n",
       "29               50.57  \n",
       "33               51.07  \n",
       "35               50.83  \n",
       "40               54.78  \n",
       "41               54.64  \n",
       "42               51.50  \n",
       "43               52.74  \n",
       "45               50.85  \n",
       "51               55.64  \n",
       "60               52.61  \n",
       "65               52.80  \n",
       "67               52.86  \n",
       "69               55.39  \n",
       "71               53.88  \n",
       "75               55.91  \n",
       "78               52.98  \n",
       "79               51.39  \n",
       "85               55.70  \n",
       "86               54.49  \n",
       "88               54.17  \n",
       "89               50.64  \n",
       "90               55.04  \n",
       "91               55.46  \n",
       "92               53.28  \n",
       "94               55.18  \n",
       "98               50.19  \n",
       "103              55.14  \n",
       "104              55.02  \n",
       "113              51.90  \n",
       "122              53.50  \n",
       "123              55.04  \n",
       "125              54.93  \n",
       "128              53.18  \n",
       "129              54.08  \n",
       "131              54.24  \n",
       "134              54.45  \n",
       "135              53.08  \n",
       "137              55.65  \n",
       "139              52.69  \n",
       "145              51.31  \n",
       "146              55.47  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strength[strength['PreferencePercent'] <= 56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99628f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>LGA</th>\n",
       "      <th>AGE5P - 0-4 years</th>\n",
       "      <th>AGE5P - 5-9 years</th>\n",
       "      <th>AGE5P - 10-14 years</th>\n",
       "      <th>AGE5P - 15-19 years</th>\n",
       "      <th>AGE5P - 20-24 years</th>\n",
       "      <th>AGE5P - 25-29 years</th>\n",
       "      <th>AGE5P - 30-34 years</th>\n",
       "      <th>AGE5P - 35-39 years</th>\n",
       "      <th>...</th>\n",
       "      <th>RELP - Not stated</th>\n",
       "      <th>SEXP - Male</th>\n",
       "      <th>SEXP - Female</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "      <th>MDCP - Married in a de facto marriage</th>\n",
       "      <th>MDCP - Not married</th>\n",
       "      <th>MDCP - Not applicable</th>\n",
       "      <th>PreferencePercent</th>\n",
       "      <th>PartyNm</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>0.050723</td>\n",
       "      <td>0.045462</td>\n",
       "      <td>0.057361</td>\n",
       "      <td>0.096777</td>\n",
       "      <td>0.092626</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089304</td>\n",
       "      <td>0.495587</td>\n",
       "      <td>0.504413</td>\n",
       "      <td>0.302321</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.364082</td>\n",
       "      <td>0.259811</td>\n",
       "      <td>58.18</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Fairly Safe ALP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Aston</td>\n",
       "      <td>0.058553</td>\n",
       "      <td>0.058845</td>\n",
       "      <td>0.058346</td>\n",
       "      <td>0.064261</td>\n",
       "      <td>0.066246</td>\n",
       "      <td>0.064391</td>\n",
       "      <td>0.067479</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070051</td>\n",
       "      <td>0.488707</td>\n",
       "      <td>0.511293</td>\n",
       "      <td>0.393949</td>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.305368</td>\n",
       "      <td>0.239929</td>\n",
       "      <td>60.13</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>0.062608</td>\n",
       "      <td>0.066055</td>\n",
       "      <td>0.061419</td>\n",
       "      <td>0.062910</td>\n",
       "      <td>0.065860</td>\n",
       "      <td>0.060216</td>\n",
       "      <td>0.059006</td>\n",
       "      <td>0.059537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089031</td>\n",
       "      <td>0.483762</td>\n",
       "      <td>0.516238</td>\n",
       "      <td>0.323363</td>\n",
       "      <td>0.081842</td>\n",
       "      <td>0.301819</td>\n",
       "      <td>0.292976</td>\n",
       "      <td>60.98</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Safe ALP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.059368</td>\n",
       "      <td>0.055980</td>\n",
       "      <td>0.060472</td>\n",
       "      <td>0.068598</td>\n",
       "      <td>0.074162</td>\n",
       "      <td>0.073559</td>\n",
       "      <td>0.066018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069741</td>\n",
       "      <td>0.486358</td>\n",
       "      <td>0.513642</td>\n",
       "      <td>0.385881</td>\n",
       "      <td>0.044820</td>\n",
       "      <td>0.335227</td>\n",
       "      <td>0.234073</td>\n",
       "      <td>56.26</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Fairly Safe LNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Barker</td>\n",
       "      <td>0.055697</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>0.062771</td>\n",
       "      <td>0.059757</td>\n",
       "      <td>0.050169</td>\n",
       "      <td>0.051558</td>\n",
       "      <td>0.054241</td>\n",
       "      <td>0.054597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092769</td>\n",
       "      <td>0.505114</td>\n",
       "      <td>0.494886</td>\n",
       "      <td>0.359438</td>\n",
       "      <td>0.078180</td>\n",
       "      <td>0.273848</td>\n",
       "      <td>0.288534</td>\n",
       "      <td>68.94</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>164</td>\n",
       "      <td>Werriwa</td>\n",
       "      <td>0.072655</td>\n",
       "      <td>0.078559</td>\n",
       "      <td>0.077535</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.072380</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>0.071789</td>\n",
       "      <td>0.070232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083012</td>\n",
       "      <td>0.493321</td>\n",
       "      <td>0.506679</td>\n",
       "      <td>0.358736</td>\n",
       "      <td>0.039473</td>\n",
       "      <td>0.313827</td>\n",
       "      <td>0.287964</td>\n",
       "      <td>55.47</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Marginal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>165</td>\n",
       "      <td>Whitlam</td>\n",
       "      <td>0.060113</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>0.064873</td>\n",
       "      <td>0.066358</td>\n",
       "      <td>0.055673</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073615</td>\n",
       "      <td>0.485514</td>\n",
       "      <td>0.514486</td>\n",
       "      <td>0.366147</td>\n",
       "      <td>0.067582</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.271771</td>\n",
       "      <td>60.91</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Safe ALP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>166</td>\n",
       "      <td>Wide Bay</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.060062</td>\n",
       "      <td>0.064301</td>\n",
       "      <td>0.056468</td>\n",
       "      <td>0.042020</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.049902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.494793</td>\n",
       "      <td>0.505207</td>\n",
       "      <td>0.350901</td>\n",
       "      <td>0.080181</td>\n",
       "      <td>0.284154</td>\n",
       "      <td>0.284764</td>\n",
       "      <td>63.15</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>167</td>\n",
       "      <td>Wills</td>\n",
       "      <td>0.062568</td>\n",
       "      <td>0.052921</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.043616</td>\n",
       "      <td>0.080062</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>0.108720</td>\n",
       "      <td>0.087122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085782</td>\n",
       "      <td>0.489373</td>\n",
       "      <td>0.510627</td>\n",
       "      <td>0.304067</td>\n",
       "      <td>0.093583</td>\n",
       "      <td>0.361030</td>\n",
       "      <td>0.241320</td>\n",
       "      <td>58.17</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Fairly Safe ALP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>168</td>\n",
       "      <td>Wright</td>\n",
       "      <td>0.060883</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>0.075367</td>\n",
       "      <td>0.070010</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.061605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097011</td>\n",
       "      <td>0.498322</td>\n",
       "      <td>0.501678</td>\n",
       "      <td>0.362536</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>0.253436</td>\n",
       "      <td>0.302479</td>\n",
       "      <td>64.58</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       LGA  AGE5P - 0-4 years  AGE5P - 5-9 years  \\\n",
       "0        0  Adelaide           0.053945           0.050723   \n",
       "1        1     Aston           0.058553           0.058845   \n",
       "2        2  Ballarat           0.062608           0.066055   \n",
       "3        3     Banks           0.060613           0.059368   \n",
       "4        4    Barker           0.055697           0.063267   \n",
       "..     ...       ...                ...                ...   \n",
       "146    164   Werriwa           0.072655           0.078559   \n",
       "147    165   Whitlam           0.060113           0.065839   \n",
       "148    166  Wide Bay           0.048858           0.060062   \n",
       "149    167     Wills           0.062568           0.052921   \n",
       "150    168    Wright           0.060883           0.074120   \n",
       "\n",
       "     AGE5P - 10-14 years  AGE5P - 15-19 years  AGE5P - 20-24 years  \\\n",
       "0               0.045462             0.057361             0.096777   \n",
       "1               0.058346             0.064261             0.066246   \n",
       "2               0.061419             0.062910             0.065860   \n",
       "3               0.055980             0.060472             0.068598   \n",
       "4               0.062771             0.059757             0.050169   \n",
       "..                   ...                  ...                  ...   \n",
       "146             0.077535             0.077236             0.072380   \n",
       "147             0.064873             0.066358             0.055673   \n",
       "148             0.064301             0.056468             0.042020   \n",
       "149             0.043203             0.043616             0.080062   \n",
       "150             0.075367             0.070010             0.055936   \n",
       "\n",
       "     AGE5P - 25-29 years  AGE5P - 30-34 years  AGE5P - 35-39 years  ...  \\\n",
       "0               0.092626             0.088211             0.070221  ...   \n",
       "1               0.064391             0.067479             0.066700  ...   \n",
       "2               0.060216             0.059006             0.059537  ...   \n",
       "3               0.074162             0.073559             0.066018  ...   \n",
       "4               0.051558             0.054241             0.054597  ...   \n",
       "..                   ...                  ...                  ...  ...   \n",
       "146             0.065587             0.071789             0.070232  ...   \n",
       "147             0.052804             0.056448             0.056678  ...   \n",
       "148             0.040191             0.044199             0.049902  ...   \n",
       "149             0.112030             0.108720             0.087122  ...   \n",
       "150             0.051039             0.056818             0.061605  ...   \n",
       "\n",
       "     RELP - Not stated  SEXP - Male  SEXP - Female  \\\n",
       "0             0.089304     0.495587       0.504413   \n",
       "1             0.070051     0.488707       0.511293   \n",
       "2             0.089031     0.483762       0.516238   \n",
       "3             0.069741     0.486358       0.513642   \n",
       "4             0.092769     0.505114       0.494886   \n",
       "..                 ...          ...            ...   \n",
       "146           0.083012     0.493321       0.506679   \n",
       "147           0.073615     0.485514       0.514486   \n",
       "148           0.104341     0.494793       0.505207   \n",
       "149           0.085782     0.489373       0.510627   \n",
       "150           0.097011     0.498322       0.501678   \n",
       "\n",
       "     MDCP - Married in a registered marriage  \\\n",
       "0                                   0.302321   \n",
       "1                                   0.393949   \n",
       "2                                   0.323363   \n",
       "3                                   0.385881   \n",
       "4                                   0.359438   \n",
       "..                                       ...   \n",
       "146                                 0.358736   \n",
       "147                                 0.366147   \n",
       "148                                 0.350901   \n",
       "149                                 0.304067   \n",
       "150                                 0.362536   \n",
       "\n",
       "     MDCP - Married in a de facto marriage  MDCP - Not married  \\\n",
       "0                                 0.073786            0.364082   \n",
       "1                                 0.060754            0.305368   \n",
       "2                                 0.081842            0.301819   \n",
       "3                                 0.044820            0.335227   \n",
       "4                                 0.078180            0.273848   \n",
       "..                                     ...                 ...   \n",
       "146                               0.039473            0.313827   \n",
       "147                               0.067582            0.294500   \n",
       "148                               0.080181            0.284154   \n",
       "149                               0.093583            0.361030   \n",
       "150                               0.081549            0.253436   \n",
       "\n",
       "     MDCP - Not applicable  PreferencePercent  PartyNm         Strength  \n",
       "0                 0.259811              58.18      ALP  Fairly Safe ALP  \n",
       "1                 0.239929              60.13      LNP         Safe LNP  \n",
       "2                 0.292976              60.98      ALP         Safe ALP  \n",
       "3                 0.234073              56.26      LNP  Fairly Safe LNP  \n",
       "4                 0.288534              68.94      LNP         Safe LNP  \n",
       "..                     ...                ...      ...              ...  \n",
       "146               0.287964              55.47      ALP         Marginal  \n",
       "147               0.271771              60.91      ALP         Safe ALP  \n",
       "148               0.284764              63.15      LNP         Safe LNP  \n",
       "149               0.241320              58.17      ALP  Fairly Safe ALP  \n",
       "150               0.302479              64.58      LNP         Safe LNP  \n",
       "\n",
       "[151 rows x 143 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions = [(adjusted_master['PreferencePercent'] <= 56),\n",
    "              ((adjusted_master['PreferencePercent'] > 56) & (adjusted_master['PreferencePercent'] < 60 )\n",
    "              & (adjusted_master['PartyNm'] == 'LNP')),\n",
    "              ((adjusted_master['PreferencePercent'] > 56) & (adjusted_master['PreferencePercent'] < 60 )\n",
    "              & (adjusted_master['PartyNm'] == 'ALP')),\n",
    "              (adjusted_master['PreferencePercent'] >= 60) & (adjusted_master['PartyNm'] == 'LNP'),\n",
    "              (adjusted_master['PreferencePercent'] > 56) & (adjusted_master['PartyNm'] == 'ALP'),\n",
    "             (adjusted_master['PreferencePercent'] > 56) & (adjusted_master['PartyNm'] == 'Other')]\n",
    "\n",
    "strengths = ['Marginal', 'Fairly Safe LNP', 'Fairly Safe ALP', 'Safe LNP', 'Safe ALP', 'Other']\n",
    "\n",
    "adjusted_master['Strength'] = np.select(conditions, strengths)\n",
    "adjusted_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22950bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fairly Safe ALP',\n",
       " 'Fairly Safe LNP',\n",
       " 'Marginal',\n",
       " 'Other',\n",
       " 'Safe ALP',\n",
       " 'Safe LNP'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(adjusted_master['Strength'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d6659",
   "metadata": {},
   "source": [
    "# 3) Correct Model (ALP, LNP, Other)\n",
    "\n",
    "Forgot non-ordinal categorical vars can't be regressed on a single output, need dummy vars & regress on those. \n",
    "\n",
    "created dummy variables:\n",
    "- is_ALP\n",
    "- is_LNP\n",
    "- is_Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c399a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>LGA</th>\n",
       "      <th>AGE5P - 0-4 years</th>\n",
       "      <th>AGE5P - 5-9 years</th>\n",
       "      <th>AGE5P - 10-14 years</th>\n",
       "      <th>AGE5P - 15-19 years</th>\n",
       "      <th>AGE5P - 20-24 years</th>\n",
       "      <th>AGE5P - 25-29 years</th>\n",
       "      <th>AGE5P - 30-34 years</th>\n",
       "      <th>AGE5P - 35-39 years</th>\n",
       "      <th>...</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "      <th>MDCP - Married in a de facto marriage</th>\n",
       "      <th>MDCP - Not married</th>\n",
       "      <th>MDCP - Not applicable</th>\n",
       "      <th>PreferencePercent</th>\n",
       "      <th>PartyNm</th>\n",
       "      <th>Strength</th>\n",
       "      <th>is_ALP</th>\n",
       "      <th>is_LNP</th>\n",
       "      <th>is_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>0.050723</td>\n",
       "      <td>0.045462</td>\n",
       "      <td>0.057361</td>\n",
       "      <td>0.096777</td>\n",
       "      <td>0.092626</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302321</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.364082</td>\n",
       "      <td>0.259811</td>\n",
       "      <td>58.18</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Fairly Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Aston</td>\n",
       "      <td>0.058553</td>\n",
       "      <td>0.058845</td>\n",
       "      <td>0.058346</td>\n",
       "      <td>0.064261</td>\n",
       "      <td>0.066246</td>\n",
       "      <td>0.064391</td>\n",
       "      <td>0.067479</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393949</td>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.305368</td>\n",
       "      <td>0.239929</td>\n",
       "      <td>60.13</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>0.062608</td>\n",
       "      <td>0.066055</td>\n",
       "      <td>0.061419</td>\n",
       "      <td>0.062910</td>\n",
       "      <td>0.065860</td>\n",
       "      <td>0.060216</td>\n",
       "      <td>0.059006</td>\n",
       "      <td>0.059537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323363</td>\n",
       "      <td>0.081842</td>\n",
       "      <td>0.301819</td>\n",
       "      <td>0.292976</td>\n",
       "      <td>60.98</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.059368</td>\n",
       "      <td>0.055980</td>\n",
       "      <td>0.060472</td>\n",
       "      <td>0.068598</td>\n",
       "      <td>0.074162</td>\n",
       "      <td>0.073559</td>\n",
       "      <td>0.066018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385881</td>\n",
       "      <td>0.044820</td>\n",
       "      <td>0.335227</td>\n",
       "      <td>0.234073</td>\n",
       "      <td>56.26</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Fairly Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Barker</td>\n",
       "      <td>0.055697</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>0.062771</td>\n",
       "      <td>0.059757</td>\n",
       "      <td>0.050169</td>\n",
       "      <td>0.051558</td>\n",
       "      <td>0.054241</td>\n",
       "      <td>0.054597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359438</td>\n",
       "      <td>0.078180</td>\n",
       "      <td>0.273848</td>\n",
       "      <td>0.288534</td>\n",
       "      <td>68.94</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>164</td>\n",
       "      <td>Werriwa</td>\n",
       "      <td>0.072655</td>\n",
       "      <td>0.078559</td>\n",
       "      <td>0.077535</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.072380</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>0.071789</td>\n",
       "      <td>0.070232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358736</td>\n",
       "      <td>0.039473</td>\n",
       "      <td>0.313827</td>\n",
       "      <td>0.287964</td>\n",
       "      <td>55.47</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Marginal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>165</td>\n",
       "      <td>Whitlam</td>\n",
       "      <td>0.060113</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>0.064873</td>\n",
       "      <td>0.066358</td>\n",
       "      <td>0.055673</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366147</td>\n",
       "      <td>0.067582</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.271771</td>\n",
       "      <td>60.91</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>166</td>\n",
       "      <td>Wide Bay</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.060062</td>\n",
       "      <td>0.064301</td>\n",
       "      <td>0.056468</td>\n",
       "      <td>0.042020</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.049902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350901</td>\n",
       "      <td>0.080181</td>\n",
       "      <td>0.284154</td>\n",
       "      <td>0.284764</td>\n",
       "      <td>63.15</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>167</td>\n",
       "      <td>Wills</td>\n",
       "      <td>0.062568</td>\n",
       "      <td>0.052921</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.043616</td>\n",
       "      <td>0.080062</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>0.108720</td>\n",
       "      <td>0.087122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304067</td>\n",
       "      <td>0.093583</td>\n",
       "      <td>0.361030</td>\n",
       "      <td>0.241320</td>\n",
       "      <td>58.17</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Fairly Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>168</td>\n",
       "      <td>Wright</td>\n",
       "      <td>0.060883</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>0.075367</td>\n",
       "      <td>0.070010</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.061605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362536</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>0.253436</td>\n",
       "      <td>0.302479</td>\n",
       "      <td>64.58</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       LGA  AGE5P - 0-4 years  AGE5P - 5-9 years  \\\n",
       "0        0  Adelaide           0.053945           0.050723   \n",
       "1        1     Aston           0.058553           0.058845   \n",
       "2        2  Ballarat           0.062608           0.066055   \n",
       "3        3     Banks           0.060613           0.059368   \n",
       "4        4    Barker           0.055697           0.063267   \n",
       "..     ...       ...                ...                ...   \n",
       "146    164   Werriwa           0.072655           0.078559   \n",
       "147    165   Whitlam           0.060113           0.065839   \n",
       "148    166  Wide Bay           0.048858           0.060062   \n",
       "149    167     Wills           0.062568           0.052921   \n",
       "150    168    Wright           0.060883           0.074120   \n",
       "\n",
       "     AGE5P - 10-14 years  AGE5P - 15-19 years  AGE5P - 20-24 years  \\\n",
       "0               0.045462             0.057361             0.096777   \n",
       "1               0.058346             0.064261             0.066246   \n",
       "2               0.061419             0.062910             0.065860   \n",
       "3               0.055980             0.060472             0.068598   \n",
       "4               0.062771             0.059757             0.050169   \n",
       "..                   ...                  ...                  ...   \n",
       "146             0.077535             0.077236             0.072380   \n",
       "147             0.064873             0.066358             0.055673   \n",
       "148             0.064301             0.056468             0.042020   \n",
       "149             0.043203             0.043616             0.080062   \n",
       "150             0.075367             0.070010             0.055936   \n",
       "\n",
       "     AGE5P - 25-29 years  AGE5P - 30-34 years  AGE5P - 35-39 years  ...  \\\n",
       "0               0.092626             0.088211             0.070221  ...   \n",
       "1               0.064391             0.067479             0.066700  ...   \n",
       "2               0.060216             0.059006             0.059537  ...   \n",
       "3               0.074162             0.073559             0.066018  ...   \n",
       "4               0.051558             0.054241             0.054597  ...   \n",
       "..                   ...                  ...                  ...  ...   \n",
       "146             0.065587             0.071789             0.070232  ...   \n",
       "147             0.052804             0.056448             0.056678  ...   \n",
       "148             0.040191             0.044199             0.049902  ...   \n",
       "149             0.112030             0.108720             0.087122  ...   \n",
       "150             0.051039             0.056818             0.061605  ...   \n",
       "\n",
       "     MDCP - Married in a registered marriage  \\\n",
       "0                                   0.302321   \n",
       "1                                   0.393949   \n",
       "2                                   0.323363   \n",
       "3                                   0.385881   \n",
       "4                                   0.359438   \n",
       "..                                       ...   \n",
       "146                                 0.358736   \n",
       "147                                 0.366147   \n",
       "148                                 0.350901   \n",
       "149                                 0.304067   \n",
       "150                                 0.362536   \n",
       "\n",
       "     MDCP - Married in a de facto marriage  MDCP - Not married  \\\n",
       "0                                 0.073786            0.364082   \n",
       "1                                 0.060754            0.305368   \n",
       "2                                 0.081842            0.301819   \n",
       "3                                 0.044820            0.335227   \n",
       "4                                 0.078180            0.273848   \n",
       "..                                     ...                 ...   \n",
       "146                               0.039473            0.313827   \n",
       "147                               0.067582            0.294500   \n",
       "148                               0.080181            0.284154   \n",
       "149                               0.093583            0.361030   \n",
       "150                               0.081549            0.253436   \n",
       "\n",
       "     MDCP - Not applicable  PreferencePercent  PartyNm         Strength  \\\n",
       "0                 0.259811              58.18      ALP  Fairly Safe ALP   \n",
       "1                 0.239929              60.13      LNP         Safe LNP   \n",
       "2                 0.292976              60.98      ALP         Safe ALP   \n",
       "3                 0.234073              56.26      LNP  Fairly Safe LNP   \n",
       "4                 0.288534              68.94      LNP         Safe LNP   \n",
       "..                     ...                ...      ...              ...   \n",
       "146               0.287964              55.47      ALP         Marginal   \n",
       "147               0.271771              60.91      ALP         Safe ALP   \n",
       "148               0.284764              63.15      LNP         Safe LNP   \n",
       "149               0.241320              58.17      ALP  Fairly Safe ALP   \n",
       "150               0.302479              64.58      LNP         Safe LNP   \n",
       "\n",
       "     is_ALP  is_LNP  is_Other  \n",
       "0         1       0         0  \n",
       "1         0       1         0  \n",
       "2         1       0         0  \n",
       "3         0       1         0  \n",
       "4         0       1         0  \n",
       "..      ...     ...       ...  \n",
       "146       1       0         0  \n",
       "147       1       0         0  \n",
       "148       0       1         0  \n",
       "149       1       0         0  \n",
       "150       0       1         0  \n",
       "\n",
       "[151 rows x 146 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_master['is_ALP'] = np.where(adjusted_master['PartyNm'] == 'ALP', 1, 0)\n",
    "adjusted_master['is_LNP'] = np.where(adjusted_master['PartyNm'] == 'LNP', 1, 0)\n",
    "adjusted_master['is_Other'] = np.where(adjusted_master['PartyNm'] == 'Other', 1, 0)\n",
    "adjusted_master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28dccc",
   "metadata": {},
   "source": [
    "# 3.1 ALP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5bea37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 146)\n",
      "(31, 146)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(adjusted_master, test_size=0.2, random_state=142)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5eb4fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other'], axis=1)\n",
    "y_train = train['is_ALP']\n",
    "X_test = test.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other'], axis=1)\n",
    "y_test = test['is_ALP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90a54c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set:  0.7416666666666667\n",
      "Accuracy score on testing set:  0.6129032258064516\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "print(\"Accuracy score on training set: \", accuracy_score(y_train, y_hat_train))\n",
    "print(\"Accuracy score on testing set: \", accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b850b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using 1 features:  0.5483870967741935\n",
      "Accuracy on test set using 2 features:  0.5483870967741935\n",
      "Accuracy on test set using 3 features:  0.6129032258064516\n",
      "Accuracy on test set using 4 features:  0.5483870967741935\n",
      "Accuracy on test set using 5 features:  0.5806451612903226\n",
      "Accuracy on test set using 6 features:  0.6129032258064516\n",
      "Accuracy on test set using 7 features:  0.6129032258064516\n",
      "Accuracy on test set using 8 features:  0.6129032258064516\n",
      "Accuracy on test set using 9 features:  0.5806451612903226\n",
      "Accuracy on test set using 10 features:  0.5483870967741935\n",
      "Accuracy on test set using 11 features:  0.5483870967741935\n",
      "Accuracy on test set using 12 features:  0.5483870967741935\n",
      "Accuracy on test set using 13 features:  0.5483870967741935\n",
      "Accuracy on test set using 14 features:  0.5483870967741935\n",
      "Accuracy on test set using 15 features:  0.6129032258064516\n",
      "Accuracy on test set using 16 features:  0.6129032258064516\n",
      "Accuracy on test set using 17 features:  0.6129032258064516\n",
      "Accuracy on test set using 18 features:  0.6129032258064516\n",
      "Accuracy on test set using 19 features:  0.5483870967741935\n",
      "Accuracy on test set using 20 features:  0.5483870967741935\n",
      "Accuracy on test set using 21 features:  0.5483870967741935\n",
      "Accuracy on test set using 22 features:  0.5483870967741935\n",
      "Accuracy on test set using 23 features:  0.5483870967741935\n",
      "Accuracy on test set using 24 features:  0.5483870967741935\n",
      "Accuracy on test set using 25 features:  0.5483870967741935\n",
      "Accuracy on test set using 26 features:  0.5483870967741935\n",
      "Accuracy on test set using 27 features:  0.5483870967741935\n",
      "Accuracy on test set using 28 features:  0.5483870967741935\n",
      "Accuracy on test set using 29 features:  0.5483870967741935\n",
      "Accuracy on test set using 30 features:  0.5483870967741935\n",
      "Accuracy on test set using 31 features:  0.5483870967741935\n",
      "Accuracy on test set using 32 features:  0.5483870967741935\n",
      "Accuracy on test set using 33 features:  0.5483870967741935\n",
      "Accuracy on test set using 34 features:  0.5483870967741935\n",
      "Accuracy on test set using 35 features:  0.5483870967741935\n",
      "Accuracy on test set using 36 features:  0.5483870967741935\n",
      "Accuracy on test set using 37 features:  0.5483870967741935\n",
      "Accuracy on test set using 38 features:  0.5483870967741935\n",
      "Accuracy on test set using 39 features:  0.5483870967741935\n",
      "Accuracy on test set using 40 features:  0.5483870967741935\n",
      "Accuracy on test set using 41 features:  0.5483870967741935\n",
      "Accuracy on test set using 42 features:  0.5483870967741935\n",
      "Accuracy on test set using 43 features:  0.5483870967741935\n",
      "Accuracy on test set using 44 features:  0.5483870967741935\n",
      "Accuracy on test set using 45 features:  0.6129032258064516\n",
      "Accuracy on test set using 46 features:  0.6129032258064516\n",
      "Accuracy on test set using 47 features:  0.6129032258064516\n",
      "Accuracy on test set using 48 features:  0.6129032258064516\n",
      "Accuracy on test set using 49 features:  0.6129032258064516\n",
      "Accuracy on test set using 50 features:  0.6129032258064516\n",
      "Accuracy on test set using 51 features:  0.6129032258064516\n",
      "Accuracy on test set using 52 features:  0.6129032258064516\n",
      "Accuracy on test set using 53 features:  0.6129032258064516\n",
      "Accuracy on test set using 54 features:  0.6129032258064516\n",
      "Accuracy on test set using 55 features:  0.6129032258064516\n",
      "Accuracy on test set using 56 features:  0.6129032258064516\n",
      "Accuracy on test set using 57 features:  0.6129032258064516\n",
      "Accuracy on test set using 58 features:  0.6129032258064516\n",
      "Accuracy on test set using 59 features:  0.6129032258064516\n",
      "Accuracy on test set using 60 features:  0.6129032258064516\n",
      "Accuracy on test set using 61 features:  0.6129032258064516\n",
      "Accuracy on test set using 62 features:  0.6129032258064516\n",
      "Accuracy on test set using 63 features:  0.6129032258064516\n",
      "Accuracy on test set using 64 features:  0.6129032258064516\n",
      "Accuracy on test set using 65 features:  0.6129032258064516\n",
      "Accuracy on test set using 66 features:  0.6129032258064516\n",
      "Accuracy on test set using 67 features:  0.6129032258064516\n",
      "Accuracy on test set using 68 features:  0.6129032258064516\n",
      "Accuracy on test set using 69 features:  0.6129032258064516\n",
      "Accuracy on test set using 70 features:  0.6129032258064516\n",
      "Accuracy on test set using 71 features:  0.6129032258064516\n",
      "Accuracy on test set using 72 features:  0.6129032258064516\n",
      "Accuracy on test set using 73 features:  0.6129032258064516\n",
      "Accuracy on test set using 74 features:  0.6129032258064516\n",
      "Accuracy on test set using 75 features:  0.6129032258064516\n",
      "Accuracy on test set using 76 features:  0.6129032258064516\n",
      "Accuracy on test set using 77 features:  0.6129032258064516\n",
      "Accuracy on test set using 78 features:  0.6129032258064516\n",
      "Accuracy on test set using 79 features:  0.6129032258064516\n",
      "Accuracy on test set using 80 features:  0.6129032258064516\n",
      "Accuracy on test set using 81 features:  0.6129032258064516\n",
      "Accuracy on test set using 82 features:  0.6129032258064516\n",
      "Accuracy on test set using 83 features:  0.6129032258064516\n",
      "Accuracy on test set using 84 features:  0.6129032258064516\n",
      "Accuracy on test set using 85 features:  0.6129032258064516\n",
      "Accuracy on test set using 86 features:  0.6129032258064516\n",
      "Accuracy on test set using 87 features:  0.6129032258064516\n",
      "Accuracy on test set using 88 features:  0.6129032258064516\n",
      "Accuracy on test set using 89 features:  0.6129032258064516\n",
      "Accuracy on test set using 90 features:  0.6129032258064516\n",
      "Accuracy on test set using 91 features:  0.6129032258064516\n",
      "Accuracy on test set using 92 features:  0.6129032258064516\n",
      "Accuracy on test set using 93 features:  0.6129032258064516\n",
      "Accuracy on test set using 94 features:  0.6129032258064516\n",
      "Accuracy on test set using 95 features:  0.6129032258064516\n",
      "Accuracy on test set using 96 features:  0.6129032258064516\n",
      "Accuracy on test set using 97 features:  0.6129032258064516\n",
      "Accuracy on test set using 98 features:  0.6129032258064516\n",
      "Accuracy on test set using 99 features:  0.6129032258064516\n",
      "Accuracy on test set using 100 features:  0.6129032258064516\n",
      "Accuracy on test set using 101 features:  0.6129032258064516\n",
      "Accuracy on test set using 102 features:  0.6129032258064516\n",
      "Accuracy on test set using 103 features:  0.6129032258064516\n",
      "Accuracy on test set using 104 features:  0.6129032258064516\n",
      "Accuracy on test set using 105 features:  0.6129032258064516\n",
      "Accuracy on test set using 106 features:  0.6129032258064516\n",
      "Accuracy on test set using 107 features:  0.6129032258064516\n",
      "Accuracy on test set using 108 features:  0.6129032258064516\n",
      "Accuracy on test set using 109 features:  0.6129032258064516\n",
      "Accuracy on test set using 110 features:  0.6129032258064516\n",
      "Accuracy on test set using 111 features:  0.6129032258064516\n",
      "Accuracy on test set using 112 features:  0.6129032258064516\n",
      "Accuracy on test set using 113 features:  0.6129032258064516\n",
      "Accuracy on test set using 114 features:  0.6129032258064516\n",
      "Accuracy on test set using 115 features:  0.6129032258064516\n",
      "Accuracy on test set using 116 features:  0.6129032258064516\n",
      "Accuracy on test set using 117 features:  0.6129032258064516\n",
      "Accuracy on test set using 118 features:  0.6129032258064516\n",
      "Accuracy on test set using 119 features:  0.6129032258064516\n",
      "Accuracy on test set using 120 features:  0.6129032258064516\n",
      "Accuracy on test set using 121 features:  0.6129032258064516\n",
      "Accuracy on test set using 122 features:  0.6129032258064516\n",
      "Accuracy on test set using 123 features:  0.6129032258064516\n",
      "Accuracy on test set using 124 features:  0.6129032258064516\n",
      "Accuracy on test set using 125 features:  0.6129032258064516\n",
      "Accuracy on test set using 126 features:  0.6129032258064516\n",
      "Accuracy on test set using 127 features:  0.6129032258064516\n",
      "Accuracy on test set using 128 features:  0.6129032258064516\n",
      "Accuracy on test set using 129 features:  0.6129032258064516\n",
      "Accuracy on test set using 130 features:  0.6129032258064516\n",
      "Accuracy on test set using 131 features:  0.6129032258064516\n",
      "Accuracy on test set using 132 features:  0.6129032258064516\n",
      "Accuracy on test set using 133 features:  0.6129032258064516\n",
      "Accuracy on test set using 134 features:  0.6129032258064516\n",
      "Accuracy on test set using 135 features:  0.6129032258064516\n",
      "Accuracy on test set using 136 features:  0.6129032258064516\n",
      "Accuracy on test set using 137 features:  0.6129032258064516\n",
      "Accuracy on test set using 138 features:  0.6129032258064516\n",
      "Accuracy on test set using 139 features:  0.6129032258064516\n",
      "3 columns provide the highest accuracy at: 61.29 %\n"
     ]
    }
   ],
   "source": [
    "acc_scores = []\n",
    "for i in range(1,140):\n",
    "    clf = LogisticRegression()\n",
    "    rfe = RFE(estimator=clf, n_features_to_select=i)\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfe.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy on test set using\", i, \"features: \", acc_score)\n",
    "    \n",
    "    acc_scores.append(acc_score)\n",
    "\n",
    "best = acc_scores[0]\n",
    "best_number_of_columns = 0;\n",
    "for i in range(len(acc_scores)):\n",
    "    if acc_scores[i] > best:\n",
    "        best = acc_scores[i]\n",
    "        best_number_of_columns = i\n",
    "        \n",
    "print(best_number_of_columns + 1, 'columns provide the highest accuracy at: %.2f' % (best*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b04aff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test set:  0.6129032258064516\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "rfe = RFE(estimator=lr_model, n_features_to_select=3, step=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "y_test_hat = rfe.predict(X_test)\n",
    "print(\"accuracy score on test set: \", accuracy_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc22423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected False, Rank: 26.000\n",
      "Column: 1, Selected False, Rank: 123.000\n",
      "Column: 2, Selected False, Rank: 56.000\n",
      "Column: 3, Selected False, Rank: 87.000\n",
      "Column: 4, Selected False, Rank: 15.000\n",
      "Column: 5, Selected False, Rank: 7.000\n",
      "Column: 6, Selected False, Rank: 6.000\n",
      "Column: 7, Selected False, Rank: 22.000\n",
      "Column: 8, Selected False, Rank: 82.000\n",
      "Column: 9, Selected False, Rank: 68.000\n",
      "Column: 10, Selected False, Rank: 39.000\n",
      "Column: 11, Selected False, Rank: 31.000\n",
      "Column: 12, Selected False, Rank: 25.000\n",
      "Column: 13, Selected False, Rank: 18.000\n",
      "Column: 14, Selected False, Rank: 21.000\n",
      "Column: 15, Selected False, Rank: 32.000\n",
      "Column: 16, Selected False, Rank: 53.000\n",
      "Column: 17, Selected False, Rank: 61.000\n",
      "Column: 18, Selected False, Rank: 79.000\n",
      "Column: 19, Selected False, Rank: 107.000\n",
      "Column: 20, Selected False, Rank: 134.000\n",
      "Column: 21, Selected False, Rank: 71.000\n",
      "Column: 22, Selected False, Rank: 112.000\n",
      "Column: 23, Selected False, Rank: 75.000\n",
      "Column: 24, Selected False, Rank: 106.000\n",
      "Column: 25, Selected False, Rank: 17.000\n",
      "Column: 26, Selected False, Rank: 109.000\n",
      "Column: 27, Selected False, Rank: 129.000\n",
      "Column: 28, Selected False, Rank: 38.000\n",
      "Column: 29, Selected False, Rank: 45.000\n",
      "Column: 30, Selected False, Rank: 51.000\n",
      "Column: 31, Selected False, Rank: 43.000\n",
      "Column: 32, Selected False, Rank: 54.000\n",
      "Column: 33, Selected False, Rank: 69.000\n",
      "Column: 34, Selected False, Rank: 125.000\n",
      "Column: 35, Selected False, Rank: 89.000\n",
      "Column: 36, Selected False, Rank: 114.000\n",
      "Column: 37, Selected False, Rank: 49.000\n",
      "Column: 38, Selected False, Rank: 37.000\n",
      "Column: 39, Selected False, Rank: 11.000\n",
      "Column: 40, Selected False, Rank: 118.000\n",
      "Column: 41, Selected False, Rank: 63.000\n",
      "Column: 42, Selected False, Rank: 59.000\n",
      "Column: 43, Selected False, Rank: 128.000\n",
      "Column: 44, Selected False, Rank: 57.000\n",
      "Column: 45, Selected False, Rank: 105.000\n",
      "Column: 46, Selected False, Rank: 19.000\n",
      "Column: 47, Selected False, Rank: 47.000\n",
      "Column: 48, Selected False, Rank: 65.000\n",
      "Column: 49, Selected False, Rank: 99.000\n",
      "Column: 50, Selected False, Rank: 95.000\n",
      "Column: 51, Selected False, Rank: 10.000\n",
      "Column: 52, Selected False, Rank: 101.000\n",
      "Column: 53, Selected False, Rank: 84.000\n",
      "Column: 54, Selected False, Rank: 67.000\n",
      "Column: 55, Selected False, Rank: 120.000\n",
      "Column: 56, Selected False, Rank: 115.000\n",
      "Column: 57, Selected False, Rank: 83.000\n",
      "Column: 58, Selected False, Rank: 93.000\n",
      "Column: 59, Selected False, Rank: 119.000\n",
      "Column: 60, Selected False, Rank: 48.000\n",
      "Column: 61, Selected False, Rank: 30.000\n",
      "Column: 62, Selected False, Rank: 72.000\n",
      "Column: 63, Selected False, Rank: 91.000\n",
      "Column: 64, Selected False, Rank: 132.000\n",
      "Column: 65, Selected False, Rank: 66.000\n",
      "Column: 66, Selected False, Rank: 23.000\n",
      "Column: 67, Selected False, Rank: 86.000\n",
      "Column: 68, Selected False, Rank: 116.000\n",
      "Column: 69, Selected False, Rank: 35.000\n",
      "Column: 70, Selected False, Rank: 133.000\n",
      "Column: 71, Selected False, Rank: 50.000\n",
      "Column: 72, Selected False, Rank: 29.000\n",
      "Column: 73, Selected False, Rank: 96.000\n",
      "Column: 74, Selected False, Rank: 88.000\n",
      "Column: 75, Selected False, Rank: 76.000\n",
      "Column: 76, Selected False, Rank: 74.000\n",
      "Column: 77, Selected False, Rank: 46.000\n",
      "Column: 78, Selected False, Rank: 80.000\n",
      "Column: 79, Selected False, Rank: 44.000\n",
      "Column: 80, Selected False, Rank: 14.000\n",
      "Column: 81, Selected False, Rank: 92.000\n",
      "Column: 82, Selected False, Rank: 94.000\n",
      "Column: 83, Selected False, Rank: 13.000\n",
      "Column: 84, Selected False, Rank: 90.000\n",
      "Column: 85, Selected False, Rank: 70.000\n",
      "Column: 86, Selected True, Rank: 1.000\n",
      "Column: 87, Selected False, Rank: 36.000\n",
      "Column: 88, Selected False, Rank: 40.000\n",
      "Column: 89, Selected False, Rank: 108.000\n",
      "Column: 90, Selected False, Rank: 4.000\n",
      "Column: 91, Selected False, Rank: 41.000\n",
      "Column: 92, Selected False, Rank: 12.000\n",
      "Column: 93, Selected False, Rank: 52.000\n",
      "Column: 94, Selected False, Rank: 98.000\n",
      "Column: 95, Selected False, Rank: 102.000\n",
      "Column: 96, Selected False, Rank: 55.000\n",
      "Column: 97, Selected False, Rank: 34.000\n",
      "Column: 98, Selected False, Rank: 97.000\n",
      "Column: 99, Selected False, Rank: 124.000\n",
      "Column: 100, Selected False, Rank: 127.000\n",
      "Column: 101, Selected False, Rank: 117.000\n",
      "Column: 102, Selected False, Rank: 130.000\n",
      "Column: 103, Selected False, Rank: 73.000\n",
      "Column: 104, Selected False, Rank: 126.000\n",
      "Column: 105, Selected False, Rank: 135.000\n",
      "Column: 106, Selected False, Rank: 20.000\n",
      "Column: 107, Selected False, Rank: 110.000\n",
      "Column: 108, Selected False, Rank: 136.000\n",
      "Column: 109, Selected False, Rank: 60.000\n",
      "Column: 110, Selected False, Rank: 85.000\n",
      "Column: 111, Selected False, Rank: 122.000\n",
      "Column: 112, Selected False, Rank: 131.000\n",
      "Column: 113, Selected False, Rank: 104.000\n",
      "Column: 114, Selected False, Rank: 121.000\n",
      "Column: 115, Selected False, Rank: 100.000\n",
      "Column: 116, Selected False, Rank: 113.000\n",
      "Column: 117, Selected False, Rank: 58.000\n",
      "Column: 118, Selected False, Rank: 28.000\n",
      "Column: 119, Selected False, Rank: 16.000\n",
      "Column: 120, Selected False, Rank: 42.000\n",
      "Column: 121, Selected False, Rank: 103.000\n",
      "Column: 122, Selected False, Rank: 24.000\n",
      "Column: 123, Selected False, Rank: 5.000\n",
      "Column: 124, Selected True, Rank: 1.000\n",
      "Column: 125, Selected False, Rank: 8.000\n",
      "Column: 126, Selected True, Rank: 1.000\n",
      "Column: 127, Selected False, Rank: 78.000\n",
      "Column: 128, Selected False, Rank: 33.000\n",
      "Column: 129, Selected False, Rank: 2.000\n",
      "Column: 130, Selected False, Rank: 111.000\n",
      "Column: 131, Selected False, Rank: 77.000\n",
      "Column: 132, Selected False, Rank: 64.000\n",
      "Column: 133, Selected False, Rank: 62.000\n",
      "Column: 134, Selected False, Rank: 9.000\n",
      "Column: 135, Selected False, Rank: 81.000\n",
      "Column: 136, Selected False, Rank: 3.000\n",
      "Column: 137, Selected False, Rank: 27.000\n",
      "[86, 124, 126]\n"
     ]
    }
   ],
   "source": [
    "relevant_cols = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    if rfe.support_[i] == True:\n",
    "        relevant_cols.append(i)\n",
    "        \n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a4badbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [86, 124, 126]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>66.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent\n",
       "3         0          0         56.26\n",
       "70        0          0         58.36\n",
       "92        0          0         53.28\n",
       "124       0          0         57.52\n",
       "56        1          0         62.21\n",
       "134       1          0         54.45\n",
       "107       0          1         71.83\n",
       "78        1          0         52.98\n",
       "128       0          0         53.18\n",
       "65        1          0         52.80\n",
       "42        1          0         51.50\n",
       "123       1          0         55.04\n",
       "111       0          0         65.36\n",
       "0         1          1         58.18\n",
       "136       1          0         64.13\n",
       "98        1          0         50.19\n",
       "90        0          0         55.04\n",
       "114       0          0         64.36\n",
       "108       0          0         57.53\n",
       "131       0          0         54.24\n",
       "101       0          0         72.49\n",
       "68        0          0         70.48\n",
       "95        1          0         58.40\n",
       "119       1          0         56.39\n",
       "64        1          1         66.30\n",
       "77        0          0         62.99\n",
       "71        0          0         53.88\n",
       "150       0          0         64.58\n",
       "7         1          0         57.52\n",
       "103       0          0         55.14\n",
       "53        0          0         58.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(test['is_ALP'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(X_test))\n",
    "result.insert(2, \"Pref Percent\", test['PreferencePercent'].values)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5afc33f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSTP - Never married</th>\n",
       "      <th>RELP - Christianity</th>\n",
       "      <th>RELP - Islam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.260373</td>\n",
       "      <td>0.555461</td>\n",
       "      <td>0.048503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.331367</td>\n",
       "      <td>0.584734</td>\n",
       "      <td>0.005375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.257813</td>\n",
       "      <td>0.561709</td>\n",
       "      <td>0.001849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.265419</td>\n",
       "      <td>0.502974</td>\n",
       "      <td>0.011615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.255628</td>\n",
       "      <td>0.492437</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.250313</td>\n",
       "      <td>0.619925</td>\n",
       "      <td>0.001979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.591751</td>\n",
       "      <td>0.260394</td>\n",
       "      <td>0.035241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.260744</td>\n",
       "      <td>0.632586</td>\n",
       "      <td>0.002672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.319403</td>\n",
       "      <td>0.457105</td>\n",
       "      <td>0.027582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.233375</td>\n",
       "      <td>0.527592</td>\n",
       "      <td>0.048839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.261646</td>\n",
       "      <td>0.624938</td>\n",
       "      <td>0.003161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.254980</td>\n",
       "      <td>0.641047</td>\n",
       "      <td>0.002615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.333886</td>\n",
       "      <td>0.508566</td>\n",
       "      <td>0.008611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.357867</td>\n",
       "      <td>0.430949</td>\n",
       "      <td>0.042542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.301677</td>\n",
       "      <td>0.456894</td>\n",
       "      <td>0.024514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.264001</td>\n",
       "      <td>0.584620</td>\n",
       "      <td>0.002772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.286019</td>\n",
       "      <td>0.632425</td>\n",
       "      <td>0.019091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.265708</td>\n",
       "      <td>0.664317</td>\n",
       "      <td>0.004786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.239032</td>\n",
       "      <td>0.512021</td>\n",
       "      <td>0.018006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.255435</td>\n",
       "      <td>0.598827</td>\n",
       "      <td>0.003382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.241290</td>\n",
       "      <td>0.679945</td>\n",
       "      <td>0.001313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.255309</td>\n",
       "      <td>0.671598</td>\n",
       "      <td>0.010641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.274782</td>\n",
       "      <td>0.609142</td>\n",
       "      <td>0.063042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.276051</td>\n",
       "      <td>0.504791</td>\n",
       "      <td>0.026523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.393886</td>\n",
       "      <td>0.398269</td>\n",
       "      <td>0.011578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.243119</td>\n",
       "      <td>0.687635</td>\n",
       "      <td>0.006187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.369329</td>\n",
       "      <td>0.424266</td>\n",
       "      <td>0.009518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.248191</td>\n",
       "      <td>0.588464</td>\n",
       "      <td>0.003963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.267665</td>\n",
       "      <td>0.513521</td>\n",
       "      <td>0.018860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.227454</td>\n",
       "      <td>0.464238</td>\n",
       "      <td>0.001088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.276851</td>\n",
       "      <td>0.550102</td>\n",
       "      <td>0.012473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MSTP - Never married  RELP - Christianity  RELP - Islam\n",
       "3                0.260373             0.555461      0.048503\n",
       "70               0.331367             0.584734      0.005375\n",
       "92               0.257813             0.561709      0.001849\n",
       "124              0.265419             0.502974      0.011615\n",
       "56               0.255628             0.492437      0.002484\n",
       "134              0.250313             0.619925      0.001979\n",
       "107              0.591751             0.260394      0.035241\n",
       "78               0.260744             0.632586      0.002672\n",
       "128              0.319403             0.457105      0.027582\n",
       "65               0.233375             0.527592      0.048839\n",
       "42               0.261646             0.624938      0.003161\n",
       "123              0.254980             0.641047      0.002615\n",
       "111              0.333886             0.508566      0.008611\n",
       "0                0.357867             0.430949      0.042542\n",
       "136              0.301677             0.456894      0.024514\n",
       "98               0.264001             0.584620      0.002772\n",
       "90               0.286019             0.632425      0.019091\n",
       "114              0.265708             0.664317      0.004786\n",
       "108              0.239032             0.512021      0.018006\n",
       "131              0.255435             0.598827      0.003382\n",
       "101              0.241290             0.679945      0.001313\n",
       "68               0.255309             0.671598      0.010641\n",
       "95               0.274782             0.609142      0.063042\n",
       "119              0.276051             0.504791      0.026523\n",
       "64               0.393886             0.398269      0.011578\n",
       "77               0.243119             0.687635      0.006187\n",
       "71               0.369329             0.424266      0.009518\n",
       "150              0.248191             0.588464      0.003963\n",
       "7                0.267665             0.513521      0.018860\n",
       "103              0.227454             0.464238      0.001088\n",
       "53               0.276851             0.550102      0.012473"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cols = X_test.iloc[:,relevant_cols]\n",
    "best_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed0983af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSTP - Never married', 'RELP - Christianity', 'RELP - Islam'], dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_name = best_cols.columns\n",
    "list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24e64dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSTP - Never married</th>\n",
       "      <th>RELP - Christianity</th>\n",
       "      <th>RELP - Islam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.67747</td>\n",
       "      <td>-1.245888</td>\n",
       "      <td>1.028113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.67747</td>\n",
       "      <td>-1.245888</td>\n",
       "      <td>1.028113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSTP - Never married  RELP - Christianity  RELP - Islam\n",
       "0               0.67747            -1.245888      1.028113\n",
       "1               0.67747            -1.245888      1.028113"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_DF = pd.DataFrame(list(map(np.ravel, rfe.estimator_.coef_)), columns=X_train.columns[rfe.support_], index=rfe.estimator_.classes_)\n",
    "coefficient_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf9c26",
   "metadata": {},
   "source": [
    "# 3.1 Interpretation (is_ALP) (61%)\n",
    "\n",
    "The main determinants of whether an electorate is votes ALP or not, is:\n",
    "- The proprtion of people who are Christian (significantly negatively affects it)\n",
    "- The proprtion of people who are Muslim (significantly positively affects it)\n",
    "- The proprtion of people who have never married (Slightly positively affects it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a224570",
   "metadata": {},
   "source": [
    "# Number of seats ALP Win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1a530ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [86, 124, 126]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "      <th>Electorate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.18</td>\n",
       "      <td>Adelaide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.13</td>\n",
       "      <td>Aston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60.98</td>\n",
       "      <td>Ballarat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.26</td>\n",
       "      <td>Banks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68.94</td>\n",
       "      <td>Barker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.47</td>\n",
       "      <td>Werriwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60.91</td>\n",
       "      <td>Whitlam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.15</td>\n",
       "      <td>Wide Bay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.17</td>\n",
       "      <td>Wills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.58</td>\n",
       "      <td>Wright</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent Electorate\n",
       "0         1          1         58.18   Adelaide\n",
       "1         0          0         60.13      Aston\n",
       "2         1          0         60.98   Ballarat\n",
       "3         0          0         56.26      Banks\n",
       "4         0          0         68.94     Barker\n",
       "..      ...        ...           ...        ...\n",
       "146       1          0         55.47    Werriwa\n",
       "147       1          0         60.91    Whitlam\n",
       "148       0          0         63.15   Wide Bay\n",
       "149       1          1         58.17      Wills\n",
       "150       0          0         64.58     Wright\n",
       "\n",
       "[151 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of seats the ALP win: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(adjusted_master['is_ALP'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(adjusted_master.drop(['index', 'LGA','PartyNm', 'PreferencePercent',\n",
    "                                                                'Strength', 'is_ALP', 'is_LNP', 'is_Other'], axis=1)))\n",
    "result.insert(2, \"Pref Percent\", adjusted_master['PreferencePercent'].values)\n",
    "result.insert(3, \"Electorate\", adjusted_master['LGA'].values)\n",
    "display(result)\n",
    "\n",
    "print('Number of seats the ALP win:',result['Predicted'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c3beaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Adelaide\n",
       "12       Blaxland\n",
       "19       Brisbane\n",
       "20          Bruce\n",
       "23        Calwell\n",
       "24       Canberra\n",
       "32         Cooper\n",
       "64      Grayndler\n",
       "97      Macnamara\n",
       "107     Melbourne\n",
       "122    Parramatta\n",
       "139          Swan\n",
       "140        Sydney\n",
       "144        Watson\n",
       "145     Wentworth\n",
       "149         Wills\n",
       "Name: Electorate, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALP_Wins = result[result['Predicted'] == 1]\n",
    "ALP_Wins['Electorate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97ab50",
   "metadata": {},
   "source": [
    "# 3.2 is_LNP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7379cc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 146)\n",
      "(31, 146)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(adjusted_master, test_size=0.2, random_state=142)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "26e04336",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other'], axis=1)\n",
    "y_train = train['is_LNP']\n",
    "X_test = test.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other'], axis=1)\n",
    "y_test = test['is_LNP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a28825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set:  0.725\n",
      "Accuracy score on testing set:  0.6451612903225806\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "print(\"Accuracy score on training set: \", accuracy_score(y_train, y_hat_train))\n",
    "print(\"Accuracy score on testing set: \", accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9598b5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using 1 features:  0.6129032258064516\n",
      "Accuracy on test set using 2 features:  0.6129032258064516\n",
      "Accuracy on test set using 3 features:  0.5806451612903226\n",
      "Accuracy on test set using 4 features:  0.5483870967741935\n",
      "Accuracy on test set using 5 features:  0.5483870967741935\n",
      "Accuracy on test set using 6 features:  0.5483870967741935\n",
      "Accuracy on test set using 7 features:  0.5806451612903226\n",
      "Accuracy on test set using 8 features:  0.5806451612903226\n",
      "Accuracy on test set using 9 features:  0.5806451612903226\n",
      "Accuracy on test set using 10 features:  0.5806451612903226\n",
      "Accuracy on test set using 11 features:  0.5806451612903226\n",
      "Accuracy on test set using 12 features:  0.5806451612903226\n",
      "Accuracy on test set using 13 features:  0.5806451612903226\n",
      "Accuracy on test set using 14 features:  0.5483870967741935\n",
      "Accuracy on test set using 15 features:  0.5483870967741935\n",
      "Accuracy on test set using 16 features:  0.5806451612903226\n",
      "Accuracy on test set using 17 features:  0.5483870967741935\n",
      "Accuracy on test set using 18 features:  0.5483870967741935\n",
      "Accuracy on test set using 19 features:  0.5483870967741935\n",
      "Accuracy on test set using 20 features:  0.5483870967741935\n",
      "Accuracy on test set using 21 features:  0.5483870967741935\n",
      "Accuracy on test set using 22 features:  0.5483870967741935\n",
      "Accuracy on test set using 23 features:  0.5483870967741935\n",
      "Accuracy on test set using 24 features:  0.5806451612903226\n",
      "Accuracy on test set using 25 features:  0.5806451612903226\n",
      "Accuracy on test set using 26 features:  0.5806451612903226\n",
      "Accuracy on test set using 27 features:  0.5806451612903226\n",
      "Accuracy on test set using 28 features:  0.5806451612903226\n",
      "Accuracy on test set using 29 features:  0.5806451612903226\n",
      "Accuracy on test set using 30 features:  0.5806451612903226\n",
      "Accuracy on test set using 31 features:  0.5806451612903226\n",
      "Accuracy on test set using 32 features:  0.5806451612903226\n",
      "Accuracy on test set using 33 features:  0.5806451612903226\n",
      "Accuracy on test set using 34 features:  0.5806451612903226\n",
      "Accuracy on test set using 35 features:  0.6129032258064516\n",
      "Accuracy on test set using 36 features:  0.6129032258064516\n",
      "Accuracy on test set using 37 features:  0.6129032258064516\n",
      "Accuracy on test set using 38 features:  0.6129032258064516\n",
      "Accuracy on test set using 39 features:  0.6129032258064516\n",
      "Accuracy on test set using 40 features:  0.6129032258064516\n",
      "Accuracy on test set using 41 features:  0.6129032258064516\n",
      "Accuracy on test set using 42 features:  0.6129032258064516\n",
      "Accuracy on test set using 43 features:  0.6129032258064516\n",
      "Accuracy on test set using 44 features:  0.6129032258064516\n",
      "Accuracy on test set using 45 features:  0.6129032258064516\n",
      "Accuracy on test set using 46 features:  0.6129032258064516\n",
      "Accuracy on test set using 47 features:  0.6451612903225806\n",
      "Accuracy on test set using 48 features:  0.6451612903225806\n",
      "Accuracy on test set using 49 features:  0.6451612903225806\n",
      "Accuracy on test set using 50 features:  0.6451612903225806\n",
      "Accuracy on test set using 51 features:  0.6451612903225806\n",
      "Accuracy on test set using 52 features:  0.6451612903225806\n",
      "Accuracy on test set using 53 features:  0.6451612903225806\n",
      "Accuracy on test set using 54 features:  0.6451612903225806\n",
      "Accuracy on test set using 55 features:  0.6451612903225806\n",
      "Accuracy on test set using 56 features:  0.6451612903225806\n",
      "Accuracy on test set using 57 features:  0.6451612903225806\n",
      "Accuracy on test set using 58 features:  0.6451612903225806\n",
      "Accuracy on test set using 59 features:  0.6451612903225806\n",
      "Accuracy on test set using 60 features:  0.6451612903225806\n",
      "Accuracy on test set using 61 features:  0.6451612903225806\n",
      "Accuracy on test set using 62 features:  0.6451612903225806\n",
      "Accuracy on test set using 63 features:  0.6451612903225806\n",
      "Accuracy on test set using 64 features:  0.6451612903225806\n",
      "Accuracy on test set using 65 features:  0.6451612903225806\n",
      "Accuracy on test set using 66 features:  0.6451612903225806\n",
      "Accuracy on test set using 67 features:  0.6451612903225806\n",
      "Accuracy on test set using 68 features:  0.6451612903225806\n",
      "Accuracy on test set using 69 features:  0.6451612903225806\n",
      "Accuracy on test set using 70 features:  0.6451612903225806\n",
      "Accuracy on test set using 71 features:  0.6451612903225806\n",
      "Accuracy on test set using 72 features:  0.6451612903225806\n",
      "Accuracy on test set using 73 features:  0.6451612903225806\n",
      "Accuracy on test set using 74 features:  0.6451612903225806\n",
      "Accuracy on test set using 75 features:  0.6451612903225806\n",
      "Accuracy on test set using 76 features:  0.6451612903225806\n",
      "Accuracy on test set using 77 features:  0.6451612903225806\n",
      "Accuracy on test set using 78 features:  0.6451612903225806\n",
      "Accuracy on test set using 79 features:  0.6451612903225806\n",
      "Accuracy on test set using 80 features:  0.6451612903225806\n",
      "Accuracy on test set using 81 features:  0.6451612903225806\n",
      "Accuracy on test set using 82 features:  0.6451612903225806\n",
      "Accuracy on test set using 83 features:  0.6451612903225806\n",
      "Accuracy on test set using 84 features:  0.6451612903225806\n",
      "Accuracy on test set using 85 features:  0.6451612903225806\n",
      "Accuracy on test set using 86 features:  0.6451612903225806\n",
      "Accuracy on test set using 87 features:  0.6451612903225806\n",
      "Accuracy on test set using 88 features:  0.6451612903225806\n",
      "Accuracy on test set using 89 features:  0.6451612903225806\n",
      "Accuracy on test set using 90 features:  0.6451612903225806\n",
      "Accuracy on test set using 91 features:  0.6451612903225806\n",
      "Accuracy on test set using 92 features:  0.6451612903225806\n",
      "Accuracy on test set using 93 features:  0.6451612903225806\n",
      "Accuracy on test set using 94 features:  0.6451612903225806\n",
      "Accuracy on test set using 95 features:  0.6451612903225806\n",
      "Accuracy on test set using 96 features:  0.6451612903225806\n",
      "Accuracy on test set using 97 features:  0.6451612903225806\n",
      "Accuracy on test set using 98 features:  0.6451612903225806\n",
      "Accuracy on test set using 99 features:  0.6451612903225806\n",
      "Accuracy on test set using 100 features:  0.6451612903225806\n",
      "Accuracy on test set using 101 features:  0.6451612903225806\n",
      "Accuracy on test set using 102 features:  0.6451612903225806\n",
      "Accuracy on test set using 103 features:  0.6451612903225806\n",
      "Accuracy on test set using 104 features:  0.6451612903225806\n",
      "Accuracy on test set using 105 features:  0.6451612903225806\n",
      "Accuracy on test set using 106 features:  0.6451612903225806\n",
      "Accuracy on test set using 107 features:  0.6451612903225806\n",
      "Accuracy on test set using 108 features:  0.6451612903225806\n",
      "Accuracy on test set using 109 features:  0.6451612903225806\n",
      "Accuracy on test set using 110 features:  0.6451612903225806\n",
      "Accuracy on test set using 111 features:  0.6451612903225806\n",
      "Accuracy on test set using 112 features:  0.6451612903225806\n",
      "Accuracy on test set using 113 features:  0.6451612903225806\n",
      "Accuracy on test set using 114 features:  0.6451612903225806\n",
      "Accuracy on test set using 115 features:  0.6451612903225806\n",
      "Accuracy on test set using 116 features:  0.6451612903225806\n",
      "Accuracy on test set using 117 features:  0.6451612903225806\n",
      "Accuracy on test set using 118 features:  0.6451612903225806\n",
      "Accuracy on test set using 119 features:  0.6451612903225806\n",
      "Accuracy on test set using 120 features:  0.6451612903225806\n",
      "Accuracy on test set using 121 features:  0.6451612903225806\n",
      "Accuracy on test set using 122 features:  0.6451612903225806\n",
      "Accuracy on test set using 123 features:  0.6451612903225806\n",
      "Accuracy on test set using 124 features:  0.6451612903225806\n",
      "Accuracy on test set using 125 features:  0.6451612903225806\n",
      "Accuracy on test set using 126 features:  0.6451612903225806\n",
      "Accuracy on test set using 127 features:  0.6451612903225806\n",
      "Accuracy on test set using 128 features:  0.6451612903225806\n",
      "Accuracy on test set using 129 features:  0.6451612903225806\n",
      "Accuracy on test set using 130 features:  0.6451612903225806\n",
      "Accuracy on test set using 131 features:  0.6451612903225806\n",
      "Accuracy on test set using 132 features:  0.6451612903225806\n",
      "Accuracy on test set using 133 features:  0.6451612903225806\n",
      "Accuracy on test set using 134 features:  0.6451612903225806\n",
      "Accuracy on test set using 135 features:  0.6451612903225806\n",
      "Accuracy on test set using 136 features:  0.6451612903225806\n",
      "Accuracy on test set using 137 features:  0.6451612903225806\n",
      "Accuracy on test set using 138 features:  0.6451612903225806\n",
      "Accuracy on test set using 139 features:  0.6451612903225806\n",
      "47 columns provide the highest accuracy at: 64.52 %\n"
     ]
    }
   ],
   "source": [
    "acc_scores = []\n",
    "for i in range(1,140):\n",
    "    clf = LogisticRegression()\n",
    "    rfe = RFE(estimator=clf, n_features_to_select=i)\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfe.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy on test set using\", i, \"features: \", acc_score)\n",
    "    \n",
    "    acc_scores.append(acc_score)\n",
    "\n",
    "best = acc_scores[0]\n",
    "best_number_of_columns = 0;\n",
    "for i in range(len(acc_scores)):\n",
    "    if acc_scores[i] > best:\n",
    "        best = acc_scores[i]\n",
    "        best_number_of_columns = i\n",
    "        \n",
    "print(best_number_of_columns + 1, 'columns provide the highest accuracy at: %.2f' % (best*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6760a9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test set:  0.6451612903225806\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "rfe = RFE(estimator=lr_model, n_features_to_select=47, step=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "y_test_hat = rfe.predict(X_test)\n",
    "print(\"accuracy score on test set: \", accuracy_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5185fed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected True, Rank: 1.000\n",
      "Column: 1, Selected False, Rank: 66.000\n",
      "Column: 2, Selected False, Rank: 11.000\n",
      "Column: 3, Selected False, Rank: 33.000\n",
      "Column: 4, Selected True, Rank: 1.000\n",
      "Column: 5, Selected True, Rank: 1.000\n",
      "Column: 6, Selected True, Rank: 1.000\n",
      "Column: 7, Selected True, Rank: 1.000\n",
      "Column: 8, Selected False, Rank: 31.000\n",
      "Column: 9, Selected False, Rank: 22.000\n",
      "Column: 10, Selected True, Rank: 1.000\n",
      "Column: 11, Selected True, Rank: 1.000\n",
      "Column: 12, Selected True, Rank: 1.000\n",
      "Column: 13, Selected True, Rank: 1.000\n",
      "Column: 14, Selected True, Rank: 1.000\n",
      "Column: 15, Selected True, Rank: 1.000\n",
      "Column: 16, Selected False, Rank: 6.000\n",
      "Column: 17, Selected False, Rank: 15.000\n",
      "Column: 18, Selected False, Rank: 32.000\n",
      "Column: 19, Selected False, Rank: 63.000\n",
      "Column: 20, Selected False, Rank: 89.000\n",
      "Column: 21, Selected False, Rank: 17.000\n",
      "Column: 22, Selected False, Rank: 80.000\n",
      "Column: 23, Selected False, Rank: 40.000\n",
      "Column: 24, Selected False, Rank: 49.000\n",
      "Column: 25, Selected True, Rank: 1.000\n",
      "Column: 26, Selected False, Rank: 42.000\n",
      "Column: 27, Selected False, Rank: 86.000\n",
      "Column: 28, Selected True, Rank: 1.000\n",
      "Column: 29, Selected False, Rank: 2.000\n",
      "Column: 30, Selected False, Rank: 12.000\n",
      "Column: 31, Selected True, Rank: 1.000\n",
      "Column: 32, Selected False, Rank: 8.000\n",
      "Column: 33, Selected False, Rank: 23.000\n",
      "Column: 34, Selected False, Rank: 87.000\n",
      "Column: 35, Selected False, Rank: 39.000\n",
      "Column: 36, Selected False, Rank: 60.000\n",
      "Column: 37, Selected False, Rank: 4.000\n",
      "Column: 38, Selected True, Rank: 1.000\n",
      "Column: 39, Selected True, Rank: 1.000\n",
      "Column: 40, Selected False, Rank: 90.000\n",
      "Column: 41, Selected False, Rank: 21.000\n",
      "Column: 42, Selected False, Rank: 16.000\n",
      "Column: 43, Selected False, Rank: 73.000\n",
      "Column: 44, Selected False, Rank: 9.000\n",
      "Column: 45, Selected False, Rank: 64.000\n",
      "Column: 46, Selected True, Rank: 1.000\n",
      "Column: 47, Selected True, Rank: 1.000\n",
      "Column: 48, Selected False, Rank: 27.000\n",
      "Column: 49, Selected False, Rank: 46.000\n",
      "Column: 50, Selected False, Rank: 48.000\n",
      "Column: 51, Selected True, Rank: 1.000\n",
      "Column: 52, Selected False, Rank: 56.000\n",
      "Column: 53, Selected False, Rank: 68.000\n",
      "Column: 54, Selected False, Rank: 20.000\n",
      "Column: 55, Selected False, Rank: 74.000\n",
      "Column: 56, Selected False, Rank: 54.000\n",
      "Column: 57, Selected False, Rank: 35.000\n",
      "Column: 58, Selected False, Rank: 47.000\n",
      "Column: 59, Selected False, Rank: 62.000\n",
      "Column: 60, Selected False, Rank: 5.000\n",
      "Column: 61, Selected True, Rank: 1.000\n",
      "Column: 62, Selected False, Rank: 29.000\n",
      "Column: 63, Selected False, Rank: 45.000\n",
      "Column: 64, Selected False, Rank: 75.000\n",
      "Column: 65, Selected False, Rank: 24.000\n",
      "Column: 66, Selected True, Rank: 1.000\n",
      "Column: 67, Selected False, Rank: 36.000\n",
      "Column: 68, Selected False, Rank: 82.000\n",
      "Column: 69, Selected True, Rank: 1.000\n",
      "Column: 70, Selected False, Rank: 76.000\n",
      "Column: 71, Selected True, Rank: 1.000\n",
      "Column: 72, Selected True, Rank: 1.000\n",
      "Column: 73, Selected False, Rank: 50.000\n",
      "Column: 74, Selected False, Rank: 41.000\n",
      "Column: 75, Selected False, Rank: 34.000\n",
      "Column: 76, Selected False, Rank: 30.000\n",
      "Column: 77, Selected True, Rank: 1.000\n",
      "Column: 78, Selected False, Rank: 52.000\n",
      "Column: 79, Selected True, Rank: 1.000\n",
      "Column: 80, Selected True, Rank: 1.000\n",
      "Column: 81, Selected False, Rank: 72.000\n",
      "Column: 82, Selected False, Rank: 51.000\n",
      "Column: 83, Selected True, Rank: 1.000\n",
      "Column: 84, Selected False, Rank: 44.000\n",
      "Column: 85, Selected False, Rank: 28.000\n",
      "Column: 86, Selected True, Rank: 1.000\n",
      "Column: 87, Selected True, Rank: 1.000\n",
      "Column: 88, Selected False, Rank: 7.000\n",
      "Column: 89, Selected False, Rank: 65.000\n",
      "Column: 90, Selected True, Rank: 1.000\n",
      "Column: 91, Selected True, Rank: 1.000\n",
      "Column: 92, Selected True, Rank: 1.000\n",
      "Column: 93, Selected False, Rank: 25.000\n",
      "Column: 94, Selected False, Rank: 55.000\n",
      "Column: 95, Selected False, Rank: 59.000\n",
      "Column: 96, Selected False, Rank: 10.000\n",
      "Column: 97, Selected True, Rank: 1.000\n",
      "Column: 98, Selected False, Rank: 53.000\n",
      "Column: 99, Selected False, Rank: 84.000\n",
      "Column: 100, Selected False, Rank: 83.000\n",
      "Column: 101, Selected False, Rank: 71.000\n",
      "Column: 102, Selected False, Rank: 85.000\n",
      "Column: 103, Selected False, Rank: 37.000\n",
      "Column: 104, Selected False, Rank: 79.000\n",
      "Column: 105, Selected False, Rank: 91.000\n",
      "Column: 106, Selected True, Rank: 1.000\n",
      "Column: 107, Selected False, Rank: 70.000\n",
      "Column: 108, Selected False, Rank: 92.000\n",
      "Column: 109, Selected False, Rank: 14.000\n",
      "Column: 110, Selected False, Rank: 43.000\n",
      "Column: 111, Selected False, Rank: 78.000\n",
      "Column: 112, Selected False, Rank: 88.000\n",
      "Column: 113, Selected False, Rank: 61.000\n",
      "Column: 114, Selected False, Rank: 77.000\n",
      "Column: 115, Selected False, Rank: 58.000\n",
      "Column: 116, Selected False, Rank: 69.000\n",
      "Column: 117, Selected False, Rank: 13.000\n",
      "Column: 118, Selected True, Rank: 1.000\n",
      "Column: 119, Selected True, Rank: 1.000\n",
      "Column: 120, Selected False, Rank: 3.000\n",
      "Column: 121, Selected False, Rank: 38.000\n",
      "Column: 122, Selected True, Rank: 1.000\n",
      "Column: 123, Selected True, Rank: 1.000\n",
      "Column: 124, Selected True, Rank: 1.000\n",
      "Column: 125, Selected True, Rank: 1.000\n",
      "Column: 126, Selected True, Rank: 1.000\n",
      "Column: 127, Selected False, Rank: 26.000\n",
      "Column: 128, Selected True, Rank: 1.000\n",
      "Column: 129, Selected True, Rank: 1.000\n",
      "Column: 130, Selected False, Rank: 67.000\n",
      "Column: 131, Selected False, Rank: 57.000\n",
      "Column: 132, Selected False, Rank: 18.000\n",
      "Column: 133, Selected False, Rank: 19.000\n",
      "Column: 134, Selected True, Rank: 1.000\n",
      "Column: 135, Selected False, Rank: 81.000\n",
      "Column: 136, Selected True, Rank: 1.000\n",
      "Column: 137, Selected True, Rank: 1.000\n",
      "[0, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 25, 28, 31, 38, 39, 46, 47, 51, 61, 66, 69, 71, 72, 77, 79, 80, 83, 86, 87, 90, 91, 92, 97, 106, 118, 119, 122, 123, 124, 125, 126, 128, 129, 134, 136, 137]\n"
     ]
    }
   ],
   "source": [
    "relevant_cols = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    if rfe.support_[i] == True:\n",
    "        relevant_cols.append(i)\n",
    "        \n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f014db89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [0, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 25, 28, 31, 38, 39, 46, 47, 51, 61, 66, 69, 71, 72, 77, 79, 80, 83, 86, 87, 90, 91, 92, 97, 106, 118, 119, 122, 123, 124, 125, 126, 128, 129, 134, 136, 137]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent\n",
       "3         1          1         56.26\n",
       "70        1          0         58.36\n",
       "92        1          1         53.28\n",
       "124       1          1         57.52\n",
       "56        0          1         62.21\n",
       "134       0          1         54.45\n",
       "107       0          0         71.83\n",
       "78        0          1         52.98\n",
       "128       1          0         53.18\n",
       "65        0          0         52.80\n",
       "42        0          1         51.50\n",
       "123       0          1         55.04\n",
       "111       1          0         65.36\n",
       "0         0          0         58.18\n",
       "136       0          0         64.13\n",
       "98        0          1         50.19\n",
       "90        1          1         55.04\n",
       "114       1          1         64.36\n",
       "108       1          1         57.53\n",
       "131       1          1         54.24\n",
       "101       1          1         72.49\n",
       "68        1          1         70.48\n",
       "95        0          0         58.40\n",
       "119       0          0         56.39\n",
       "64        0          0         66.30\n",
       "77        1          1         62.99\n",
       "71        1          0         53.88\n",
       "150       1          1         64.58\n",
       "7         0          0         57.52\n",
       "103       0          1         55.14\n",
       "53        1          1         58.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(test['is_LNP'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(X_test))\n",
    "result.insert(2, \"Pref Percent\", test['PreferencePercent'].values)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d90fa07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE5P - 0-4 years</th>\n",
       "      <th>AGE5P - 20-24 years</th>\n",
       "      <th>AGE5P - 25-29 years</th>\n",
       "      <th>AGE5P - 30-34 years</th>\n",
       "      <th>AGE5P - 35-39 years</th>\n",
       "      <th>AGE5P - 50-54 years</th>\n",
       "      <th>AGE5P - 55-59 years</th>\n",
       "      <th>AGE5P - 60-64 years</th>\n",
       "      <th>AGE5P - 65-69 years</th>\n",
       "      <th>AGE5P - 70-74 years</th>\n",
       "      <th>...</th>\n",
       "      <th>RLHP - Not applicable</th>\n",
       "      <th>RELP - Buddhism</th>\n",
       "      <th>RELP - Christianity</th>\n",
       "      <th>RELP - Hinduism</th>\n",
       "      <th>RELP - Islam</th>\n",
       "      <th>RELP - Other Religions</th>\n",
       "      <th>RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "      <th>MDCP - Not married</th>\n",
       "      <th>MDCP - Not applicable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.068598</td>\n",
       "      <td>0.074162</td>\n",
       "      <td>0.073559</td>\n",
       "      <td>0.066018</td>\n",
       "      <td>0.068046</td>\n",
       "      <td>0.065087</td>\n",
       "      <td>0.054882</td>\n",
       "      <td>0.046442</td>\n",
       "      <td>0.035024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017688</td>\n",
       "      <td>0.042913</td>\n",
       "      <td>0.555461</td>\n",
       "      <td>0.031552</td>\n",
       "      <td>0.048503</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.242016</td>\n",
       "      <td>0.385881</td>\n",
       "      <td>0.335227</td>\n",
       "      <td>0.234073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.070830</td>\n",
       "      <td>0.086721</td>\n",
       "      <td>0.078522</td>\n",
       "      <td>0.074623</td>\n",
       "      <td>0.064649</td>\n",
       "      <td>0.061693</td>\n",
       "      <td>0.058431</td>\n",
       "      <td>0.048320</td>\n",
       "      <td>0.042489</td>\n",
       "      <td>0.028672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053760</td>\n",
       "      <td>0.008369</td>\n",
       "      <td>0.584734</td>\n",
       "      <td>0.004756</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>0.290817</td>\n",
       "      <td>0.280871</td>\n",
       "      <td>0.293952</td>\n",
       "      <td>0.324590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.064828</td>\n",
       "      <td>0.062505</td>\n",
       "      <td>0.060643</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>0.058862</td>\n",
       "      <td>0.063701</td>\n",
       "      <td>0.061023</td>\n",
       "      <td>0.059491</td>\n",
       "      <td>0.059192</td>\n",
       "      <td>0.046097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027490</td>\n",
       "      <td>0.008668</td>\n",
       "      <td>0.561709</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>0.318683</td>\n",
       "      <td>0.339021</td>\n",
       "      <td>0.282468</td>\n",
       "      <td>0.292313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.079176</td>\n",
       "      <td>0.064734</td>\n",
       "      <td>0.076436</td>\n",
       "      <td>0.078890</td>\n",
       "      <td>0.070544</td>\n",
       "      <td>0.063458</td>\n",
       "      <td>0.051827</td>\n",
       "      <td>0.042782</td>\n",
       "      <td>0.039294</td>\n",
       "      <td>0.027305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035168</td>\n",
       "      <td>0.012099</td>\n",
       "      <td>0.502974</td>\n",
       "      <td>0.013953</td>\n",
       "      <td>0.011615</td>\n",
       "      <td>0.010976</td>\n",
       "      <td>0.342444</td>\n",
       "      <td>0.340102</td>\n",
       "      <td>0.244735</td>\n",
       "      <td>0.325028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.059398</td>\n",
       "      <td>0.048807</td>\n",
       "      <td>0.050483</td>\n",
       "      <td>0.059065</td>\n",
       "      <td>0.058893</td>\n",
       "      <td>0.067738</td>\n",
       "      <td>0.073947</td>\n",
       "      <td>0.068101</td>\n",
       "      <td>0.065627</td>\n",
       "      <td>0.047807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>0.492437</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>0.400382</td>\n",
       "      <td>0.351234</td>\n",
       "      <td>0.279460</td>\n",
       "      <td>0.278461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.058333</td>\n",
       "      <td>0.055385</td>\n",
       "      <td>0.050907</td>\n",
       "      <td>0.056568</td>\n",
       "      <td>0.055316</td>\n",
       "      <td>0.066985</td>\n",
       "      <td>0.068452</td>\n",
       "      <td>0.063455</td>\n",
       "      <td>0.062632</td>\n",
       "      <td>0.052789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024605</td>\n",
       "      <td>0.008112</td>\n",
       "      <td>0.619925</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.293455</td>\n",
       "      <td>0.366205</td>\n",
       "      <td>0.304889</td>\n",
       "      <td>0.253644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.037696</td>\n",
       "      <td>0.172723</td>\n",
       "      <td>0.182719</td>\n",
       "      <td>0.135079</td>\n",
       "      <td>0.078709</td>\n",
       "      <td>0.039943</td>\n",
       "      <td>0.035943</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>0.026816</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062582</td>\n",
       "      <td>0.060602</td>\n",
       "      <td>0.260394</td>\n",
       "      <td>0.026891</td>\n",
       "      <td>0.035241</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>0.472482</td>\n",
       "      <td>0.181893</td>\n",
       "      <td>0.453533</td>\n",
       "      <td>0.239190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.065358</td>\n",
       "      <td>0.058623</td>\n",
       "      <td>0.059331</td>\n",
       "      <td>0.061920</td>\n",
       "      <td>0.058527</td>\n",
       "      <td>0.064810</td>\n",
       "      <td>0.066347</td>\n",
       "      <td>0.061193</td>\n",
       "      <td>0.058119</td>\n",
       "      <td>0.045005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042456</td>\n",
       "      <td>0.006760</td>\n",
       "      <td>0.632586</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.260657</td>\n",
       "      <td>0.340778</td>\n",
       "      <td>0.281919</td>\n",
       "      <td>0.297467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.058885</td>\n",
       "      <td>0.087894</td>\n",
       "      <td>0.114632</td>\n",
       "      <td>0.106807</td>\n",
       "      <td>0.079441</td>\n",
       "      <td>0.059232</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.045155</td>\n",
       "      <td>0.037870</td>\n",
       "      <td>0.027664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035074</td>\n",
       "      <td>0.049320</td>\n",
       "      <td>0.457105</td>\n",
       "      <td>0.071167</td>\n",
       "      <td>0.027582</td>\n",
       "      <td>0.006292</td>\n",
       "      <td>0.285639</td>\n",
       "      <td>0.359918</td>\n",
       "      <td>0.336725</td>\n",
       "      <td>0.243467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.080754</td>\n",
       "      <td>0.060545</td>\n",
       "      <td>0.070088</td>\n",
       "      <td>0.088415</td>\n",
       "      <td>0.091308</td>\n",
       "      <td>0.058728</td>\n",
       "      <td>0.051643</td>\n",
       "      <td>0.044769</td>\n",
       "      <td>0.036943</td>\n",
       "      <td>0.024190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017887</td>\n",
       "      <td>0.020750</td>\n",
       "      <td>0.527592</td>\n",
       "      <td>0.121368</td>\n",
       "      <td>0.048839</td>\n",
       "      <td>0.049709</td>\n",
       "      <td>0.156307</td>\n",
       "      <td>0.400018</td>\n",
       "      <td>0.274307</td>\n",
       "      <td>0.281916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.061068</td>\n",
       "      <td>0.057914</td>\n",
       "      <td>0.053981</td>\n",
       "      <td>0.057499</td>\n",
       "      <td>0.057849</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.064249</td>\n",
       "      <td>0.059589</td>\n",
       "      <td>0.057369</td>\n",
       "      <td>0.045874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>0.624938</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.271944</td>\n",
       "      <td>0.328731</td>\n",
       "      <td>0.313633</td>\n",
       "      <td>0.281880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.066469</td>\n",
       "      <td>0.058206</td>\n",
       "      <td>0.060506</td>\n",
       "      <td>0.061059</td>\n",
       "      <td>0.058502</td>\n",
       "      <td>0.064503</td>\n",
       "      <td>0.064220</td>\n",
       "      <td>0.061155</td>\n",
       "      <td>0.061136</td>\n",
       "      <td>0.048215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028722</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.641047</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.257929</td>\n",
       "      <td>0.343199</td>\n",
       "      <td>0.286943</td>\n",
       "      <td>0.288633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.047779</td>\n",
       "      <td>0.079635</td>\n",
       "      <td>0.083584</td>\n",
       "      <td>0.075363</td>\n",
       "      <td>0.065382</td>\n",
       "      <td>0.064568</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.056628</td>\n",
       "      <td>0.057089</td>\n",
       "      <td>0.045427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043136</td>\n",
       "      <td>0.020328</td>\n",
       "      <td>0.508566</td>\n",
       "      <td>0.010957</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>0.008785</td>\n",
       "      <td>0.321620</td>\n",
       "      <td>0.292694</td>\n",
       "      <td>0.347325</td>\n",
       "      <td>0.266321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053945</td>\n",
       "      <td>0.096777</td>\n",
       "      <td>0.092626</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>0.059555</td>\n",
       "      <td>0.058079</td>\n",
       "      <td>0.050950</td>\n",
       "      <td>0.044418</td>\n",
       "      <td>0.030872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051793</td>\n",
       "      <td>0.040264</td>\n",
       "      <td>0.430949</td>\n",
       "      <td>0.034115</td>\n",
       "      <td>0.042542</td>\n",
       "      <td>0.017060</td>\n",
       "      <td>0.339837</td>\n",
       "      <td>0.302321</td>\n",
       "      <td>0.364082</td>\n",
       "      <td>0.259811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.077451</td>\n",
       "      <td>0.074264</td>\n",
       "      <td>0.074316</td>\n",
       "      <td>0.073817</td>\n",
       "      <td>0.061461</td>\n",
       "      <td>0.066454</td>\n",
       "      <td>0.060584</td>\n",
       "      <td>0.049302</td>\n",
       "      <td>0.041371</td>\n",
       "      <td>0.031424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021245</td>\n",
       "      <td>0.029130</td>\n",
       "      <td>0.456894</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>0.375639</td>\n",
       "      <td>0.291657</td>\n",
       "      <td>0.330960</td>\n",
       "      <td>0.295220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.058913</td>\n",
       "      <td>0.056665</td>\n",
       "      <td>0.052444</td>\n",
       "      <td>0.053738</td>\n",
       "      <td>0.057365</td>\n",
       "      <td>0.071490</td>\n",
       "      <td>0.072515</td>\n",
       "      <td>0.065976</td>\n",
       "      <td>0.060928</td>\n",
       "      <td>0.043996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.584620</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.310298</td>\n",
       "      <td>0.359523</td>\n",
       "      <td>0.290243</td>\n",
       "      <td>0.273720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.075389</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.077221</td>\n",
       "      <td>0.079455</td>\n",
       "      <td>0.072369</td>\n",
       "      <td>0.059632</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.050684</td>\n",
       "      <td>0.045327</td>\n",
       "      <td>0.029828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.632425</td>\n",
       "      <td>0.018406</td>\n",
       "      <td>0.019091</td>\n",
       "      <td>0.011278</td>\n",
       "      <td>0.222032</td>\n",
       "      <td>0.332070</td>\n",
       "      <td>0.303282</td>\n",
       "      <td>0.289176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.060304</td>\n",
       "      <td>0.057865</td>\n",
       "      <td>0.055206</td>\n",
       "      <td>0.054720</td>\n",
       "      <td>0.052054</td>\n",
       "      <td>0.065784</td>\n",
       "      <td>0.069870</td>\n",
       "      <td>0.065674</td>\n",
       "      <td>0.062198</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050088</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.664317</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.004786</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.212013</td>\n",
       "      <td>0.325706</td>\n",
       "      <td>0.280444</td>\n",
       "      <td>0.319889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.048672</td>\n",
       "      <td>0.062423</td>\n",
       "      <td>0.053251</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.056815</td>\n",
       "      <td>0.073088</td>\n",
       "      <td>0.069114</td>\n",
       "      <td>0.059437</td>\n",
       "      <td>0.055524</td>\n",
       "      <td>0.047199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027787</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.512021</td>\n",
       "      <td>0.012522</td>\n",
       "      <td>0.018006</td>\n",
       "      <td>0.009322</td>\n",
       "      <td>0.335852</td>\n",
       "      <td>0.431292</td>\n",
       "      <td>0.288801</td>\n",
       "      <td>0.236670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.056376</td>\n",
       "      <td>0.052379</td>\n",
       "      <td>0.051526</td>\n",
       "      <td>0.053357</td>\n",
       "      <td>0.056537</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.070262</td>\n",
       "      <td>0.063728</td>\n",
       "      <td>0.063029</td>\n",
       "      <td>0.052581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028863</td>\n",
       "      <td>0.011607</td>\n",
       "      <td>0.598827</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.294278</td>\n",
       "      <td>0.351900</td>\n",
       "      <td>0.314676</td>\n",
       "      <td>0.258640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.064005</td>\n",
       "      <td>0.051565</td>\n",
       "      <td>0.054532</td>\n",
       "      <td>0.054961</td>\n",
       "      <td>0.053795</td>\n",
       "      <td>0.068748</td>\n",
       "      <td>0.070557</td>\n",
       "      <td>0.066705</td>\n",
       "      <td>0.065151</td>\n",
       "      <td>0.049649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042222</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.679945</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>0.200118</td>\n",
       "      <td>0.340305</td>\n",
       "      <td>0.256910</td>\n",
       "      <td>0.325540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.065174</td>\n",
       "      <td>0.065147</td>\n",
       "      <td>0.063749</td>\n",
       "      <td>0.062601</td>\n",
       "      <td>0.057469</td>\n",
       "      <td>0.061627</td>\n",
       "      <td>0.062822</td>\n",
       "      <td>0.055688</td>\n",
       "      <td>0.052350</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042538</td>\n",
       "      <td>0.008324</td>\n",
       "      <td>0.671598</td>\n",
       "      <td>0.008129</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.205241</td>\n",
       "      <td>0.347481</td>\n",
       "      <td>0.276180</td>\n",
       "      <td>0.308948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.076859</td>\n",
       "      <td>0.071686</td>\n",
       "      <td>0.074341</td>\n",
       "      <td>0.078908</td>\n",
       "      <td>0.070113</td>\n",
       "      <td>0.062311</td>\n",
       "      <td>0.060545</td>\n",
       "      <td>0.053317</td>\n",
       "      <td>0.044509</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015018</td>\n",
       "      <td>0.022399</td>\n",
       "      <td>0.609142</td>\n",
       "      <td>0.031731</td>\n",
       "      <td>0.063042</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.184146</td>\n",
       "      <td>0.347075</td>\n",
       "      <td>0.302252</td>\n",
       "      <td>0.285155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.079105</td>\n",
       "      <td>0.070275</td>\n",
       "      <td>0.072878</td>\n",
       "      <td>0.079895</td>\n",
       "      <td>0.074038</td>\n",
       "      <td>0.062115</td>\n",
       "      <td>0.054392</td>\n",
       "      <td>0.044007</td>\n",
       "      <td>0.036636</td>\n",
       "      <td>0.025922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034164</td>\n",
       "      <td>0.050504</td>\n",
       "      <td>0.504791</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>0.026523</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>0.274726</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>0.286005</td>\n",
       "      <td>0.314088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.060788</td>\n",
       "      <td>0.067757</td>\n",
       "      <td>0.099278</td>\n",
       "      <td>0.102576</td>\n",
       "      <td>0.091848</td>\n",
       "      <td>0.065062</td>\n",
       "      <td>0.054114</td>\n",
       "      <td>0.047111</td>\n",
       "      <td>0.039460</td>\n",
       "      <td>0.028083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037737</td>\n",
       "      <td>0.032527</td>\n",
       "      <td>0.398269</td>\n",
       "      <td>0.022675</td>\n",
       "      <td>0.011578</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.415847</td>\n",
       "      <td>0.282001</td>\n",
       "      <td>0.333703</td>\n",
       "      <td>0.265895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.066998</td>\n",
       "      <td>0.057346</td>\n",
       "      <td>0.058003</td>\n",
       "      <td>0.061760</td>\n",
       "      <td>0.061083</td>\n",
       "      <td>0.068040</td>\n",
       "      <td>0.063944</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>0.054439</td>\n",
       "      <td>0.039370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030172</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.687635</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.213552</td>\n",
       "      <td>0.374747</td>\n",
       "      <td>0.262873</td>\n",
       "      <td>0.290926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.049081</td>\n",
       "      <td>0.087459</td>\n",
       "      <td>0.107743</td>\n",
       "      <td>0.095479</td>\n",
       "      <td>0.072454</td>\n",
       "      <td>0.059470</td>\n",
       "      <td>0.053957</td>\n",
       "      <td>0.047010</td>\n",
       "      <td>0.043170</td>\n",
       "      <td>0.033022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030042</td>\n",
       "      <td>0.025452</td>\n",
       "      <td>0.424266</td>\n",
       "      <td>0.034717</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.365632</td>\n",
       "      <td>0.327740</td>\n",
       "      <td>0.346263</td>\n",
       "      <td>0.239308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.060883</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.061605</td>\n",
       "      <td>0.074101</td>\n",
       "      <td>0.066310</td>\n",
       "      <td>0.058767</td>\n",
       "      <td>0.055117</td>\n",
       "      <td>0.038869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026199</td>\n",
       "      <td>0.009582</td>\n",
       "      <td>0.588464</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.003963</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.288079</td>\n",
       "      <td>0.362536</td>\n",
       "      <td>0.253436</td>\n",
       "      <td>0.302479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.066758</td>\n",
       "      <td>0.060364</td>\n",
       "      <td>0.065045</td>\n",
       "      <td>0.071262</td>\n",
       "      <td>0.068575</td>\n",
       "      <td>0.071056</td>\n",
       "      <td>0.066832</td>\n",
       "      <td>0.057558</td>\n",
       "      <td>0.051149</td>\n",
       "      <td>0.036425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028843</td>\n",
       "      <td>0.018889</td>\n",
       "      <td>0.513521</td>\n",
       "      <td>0.017458</td>\n",
       "      <td>0.018860</td>\n",
       "      <td>0.007910</td>\n",
       "      <td>0.335144</td>\n",
       "      <td>0.366788</td>\n",
       "      <td>0.283925</td>\n",
       "      <td>0.271511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.054343</td>\n",
       "      <td>0.047168</td>\n",
       "      <td>0.041547</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.053369</td>\n",
       "      <td>0.071385</td>\n",
       "      <td>0.074076</td>\n",
       "      <td>0.072045</td>\n",
       "      <td>0.073919</td>\n",
       "      <td>0.055941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030486</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>0.464238</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.423214</td>\n",
       "      <td>0.388684</td>\n",
       "      <td>0.255955</td>\n",
       "      <td>0.275573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.076394</td>\n",
       "      <td>0.067321</td>\n",
       "      <td>0.069272</td>\n",
       "      <td>0.071670</td>\n",
       "      <td>0.068783</td>\n",
       "      <td>0.059914</td>\n",
       "      <td>0.056592</td>\n",
       "      <td>0.049029</td>\n",
       "      <td>0.046872</td>\n",
       "      <td>0.032392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019656</td>\n",
       "      <td>0.013959</td>\n",
       "      <td>0.550102</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.012473</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.316483</td>\n",
       "      <td>0.323253</td>\n",
       "      <td>0.284138</td>\n",
       "      <td>0.301693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AGE5P - 0-4 years  AGE5P - 20-24 years  AGE5P - 25-29 years  \\\n",
       "3             0.060613             0.068598             0.074162   \n",
       "70            0.070830             0.086721             0.078522   \n",
       "92            0.064828             0.062505             0.060643   \n",
       "124           0.079176             0.064734             0.076436   \n",
       "56            0.059398             0.048807             0.050483   \n",
       "134           0.058333             0.055385             0.050907   \n",
       "107           0.037696             0.172723             0.182719   \n",
       "78            0.065358             0.058623             0.059331   \n",
       "128           0.058885             0.087894             0.114632   \n",
       "65            0.080754             0.060545             0.070088   \n",
       "42            0.061068             0.057914             0.053981   \n",
       "123           0.066469             0.058206             0.060506   \n",
       "111           0.047779             0.079635             0.083584   \n",
       "0             0.053945             0.096777             0.092626   \n",
       "136           0.077451             0.074264             0.074316   \n",
       "98            0.058913             0.056665             0.052444   \n",
       "90            0.075389             0.070922             0.077221   \n",
       "114           0.060304             0.057865             0.055206   \n",
       "108           0.048672             0.062423             0.053251   \n",
       "131           0.056376             0.052379             0.051526   \n",
       "101           0.064005             0.051565             0.054532   \n",
       "68            0.065174             0.065147             0.063749   \n",
       "95            0.076859             0.071686             0.074341   \n",
       "119           0.079105             0.070275             0.072878   \n",
       "64            0.060788             0.067757             0.099278   \n",
       "77            0.066998             0.057346             0.058003   \n",
       "71            0.049081             0.087459             0.107743   \n",
       "150           0.060883             0.055936             0.051039   \n",
       "7             0.066758             0.060364             0.065045   \n",
       "103           0.054343             0.047168             0.041547   \n",
       "53            0.076394             0.067321             0.069272   \n",
       "\n",
       "     AGE5P - 30-34 years  AGE5P - 35-39 years  AGE5P - 50-54 years  \\\n",
       "3               0.073559             0.066018             0.068046   \n",
       "70              0.074623             0.064649             0.061693   \n",
       "92              0.060699             0.058862             0.063701   \n",
       "124             0.078890             0.070544             0.063458   \n",
       "56              0.059065             0.058893             0.067738   \n",
       "134             0.056568             0.055316             0.066985   \n",
       "107             0.135079             0.078709             0.039943   \n",
       "78              0.061920             0.058527             0.064810   \n",
       "128             0.106807             0.079441             0.059232   \n",
       "65              0.088415             0.091308             0.058728   \n",
       "42              0.057499             0.057849             0.064800   \n",
       "123             0.061059             0.058502             0.064503   \n",
       "111             0.075363             0.065382             0.064568   \n",
       "0               0.088211             0.070221             0.059555   \n",
       "136             0.073817             0.061461             0.066454   \n",
       "98              0.053738             0.057365             0.071490   \n",
       "90              0.079455             0.072369             0.059632   \n",
       "114             0.054720             0.052054             0.065784   \n",
       "108             0.051724             0.056815             0.073088   \n",
       "131             0.053357             0.056537             0.069430   \n",
       "101             0.054961             0.053795             0.068748   \n",
       "68              0.062601             0.057469             0.061627   \n",
       "95              0.078908             0.070113             0.062311   \n",
       "119             0.079895             0.074038             0.062115   \n",
       "64              0.102576             0.091848             0.065062   \n",
       "77              0.061760             0.061083             0.068040   \n",
       "71              0.095479             0.072454             0.059470   \n",
       "150             0.056818             0.061605             0.074101   \n",
       "7               0.071262             0.068575             0.071056   \n",
       "103             0.047810             0.053369             0.071385   \n",
       "53              0.071670             0.068783             0.059914   \n",
       "\n",
       "     AGE5P - 55-59 years  AGE5P - 60-64 years  AGE5P - 65-69 years  \\\n",
       "3               0.065087             0.054882             0.046442   \n",
       "70              0.058431             0.048320             0.042489   \n",
       "92              0.061023             0.059491             0.059192   \n",
       "124             0.051827             0.042782             0.039294   \n",
       "56              0.073947             0.068101             0.065627   \n",
       "134             0.068452             0.063455             0.062632   \n",
       "107             0.035943             0.031038             0.026816   \n",
       "78              0.066347             0.061193             0.058119   \n",
       "128             0.054545             0.045155             0.037870   \n",
       "65              0.051643             0.044769             0.036943   \n",
       "42              0.064249             0.059589             0.057369   \n",
       "123             0.064220             0.061155             0.061136   \n",
       "111             0.061900             0.056628             0.057089   \n",
       "0               0.058079             0.050950             0.044418   \n",
       "136             0.060584             0.049302             0.041371   \n",
       "98              0.072515             0.065976             0.060928   \n",
       "90              0.057950             0.050684             0.045327   \n",
       "114             0.069870             0.065674             0.062198   \n",
       "108             0.069114             0.059437             0.055524   \n",
       "131             0.070262             0.063728             0.063029   \n",
       "101             0.070557             0.066705             0.065151   \n",
       "68              0.062822             0.055688             0.052350   \n",
       "95              0.060545             0.053317             0.044509   \n",
       "119             0.054392             0.044007             0.036636   \n",
       "64              0.054114             0.047111             0.039460   \n",
       "77              0.063944             0.058780             0.054439   \n",
       "71              0.053957             0.047010             0.043170   \n",
       "150             0.066310             0.058767             0.055117   \n",
       "7               0.066832             0.057558             0.051149   \n",
       "103             0.074076             0.072045             0.073919   \n",
       "53              0.056592             0.049029             0.046872   \n",
       "\n",
       "     AGE5P - 70-74 years  ...  RLHP - Not applicable  RELP - Buddhism  \\\n",
       "3               0.035024  ...               0.017688         0.042913   \n",
       "70              0.028672  ...               0.053760         0.008369   \n",
       "92              0.046097  ...               0.027490         0.008668   \n",
       "124             0.027305  ...               0.035168         0.012099   \n",
       "56              0.047807  ...               0.035667         0.007118   \n",
       "134             0.052789  ...               0.024605         0.008112   \n",
       "107             0.018287  ...               0.062582         0.060602   \n",
       "78              0.045005  ...               0.042456         0.006760   \n",
       "128             0.027664  ...               0.035074         0.049320   \n",
       "65              0.024190  ...               0.017887         0.020750   \n",
       "42              0.045874  ...               0.031749         0.008360   \n",
       "123             0.048215  ...               0.028722         0.006727   \n",
       "111             0.045427  ...               0.043136         0.020328   \n",
       "0               0.030872  ...               0.051793         0.040264   \n",
       "136             0.031424  ...               0.021245         0.029130   \n",
       "98              0.043996  ...               0.031667         0.010019   \n",
       "90              0.029828  ...               0.025000         0.010827   \n",
       "114             0.050018  ...               0.050088         0.005876   \n",
       "108             0.047199  ...               0.027787         0.033149   \n",
       "131             0.052581  ...               0.028863         0.011607   \n",
       "101             0.049649  ...               0.042222         0.004522   \n",
       "68              0.041420  ...               0.042538         0.008324   \n",
       "95              0.028535  ...               0.015018         0.022399   \n",
       "119             0.025922  ...               0.034164         0.050504   \n",
       "64              0.028083  ...               0.037737         0.032527   \n",
       "77              0.039370  ...               0.030172         0.007017   \n",
       "71              0.033022  ...               0.030042         0.025452   \n",
       "150             0.038869  ...               0.026199         0.009582   \n",
       "7               0.036425  ...               0.028843         0.018889   \n",
       "103             0.055941  ...               0.030486         0.009135   \n",
       "53              0.032392  ...               0.019656         0.013959   \n",
       "\n",
       "     RELP - Christianity  RELP - Hinduism  RELP - Islam  \\\n",
       "3               0.555461         0.031552      0.048503   \n",
       "70              0.584734         0.004756      0.005375   \n",
       "92              0.561709         0.003076      0.001849   \n",
       "124             0.502974         0.013953      0.011615   \n",
       "56              0.492437         0.002534      0.002484   \n",
       "134             0.619925         0.002810      0.001979   \n",
       "107             0.260394         0.026891      0.035241   \n",
       "78              0.632586         0.002985      0.002672   \n",
       "128             0.457105         0.071167      0.027582   \n",
       "65              0.527592         0.121368      0.048839   \n",
       "42              0.624938         0.003226      0.003161   \n",
       "123             0.641047         0.003155      0.002615   \n",
       "111             0.508566         0.010957      0.008611   \n",
       "0               0.430949         0.034115      0.042542   \n",
       "136             0.456894         0.013425      0.024514   \n",
       "98              0.584620         0.002256      0.002772   \n",
       "90              0.632425         0.018406      0.019091   \n",
       "114             0.664317         0.003982      0.004786   \n",
       "108             0.512021         0.012522      0.018006   \n",
       "131             0.598827         0.004374      0.003382   \n",
       "101             0.679945         0.002372      0.001313   \n",
       "68              0.671598         0.008129      0.010641   \n",
       "95              0.609142         0.031731      0.063042   \n",
       "119             0.504791         0.023658      0.026523   \n",
       "64              0.398269         0.022675      0.011578   \n",
       "77              0.687635         0.003140      0.006187   \n",
       "71              0.424266         0.034717      0.009518   \n",
       "150             0.588464         0.002710      0.003963   \n",
       "7               0.513521         0.017458      0.018860   \n",
       "103             0.464238         0.001730      0.001088   \n",
       "53              0.550102         0.006753      0.012473   \n",
       "\n",
       "     RELP - Other Religions  \\\n",
       "3                  0.005776   \n",
       "70                 0.004806   \n",
       "92                 0.005013   \n",
       "124                0.010976   \n",
       "56                 0.003877   \n",
       "134                0.002962   \n",
       "107                0.004693   \n",
       "78                 0.003029   \n",
       "128                0.006292   \n",
       "65                 0.049709   \n",
       "42                 0.003940   \n",
       "123                0.003682   \n",
       "111                0.008785   \n",
       "0                  0.017060   \n",
       "136                0.007319   \n",
       "98                 0.005048   \n",
       "90                 0.011278   \n",
       "114                0.003846   \n",
       "108                0.009322   \n",
       "131                0.003550   \n",
       "101                0.004288   \n",
       "68                 0.005307   \n",
       "95                 0.010184   \n",
       "119                0.010492   \n",
       "64                 0.004635   \n",
       "77                 0.003233   \n",
       "71                 0.005084   \n",
       "150                0.005318   \n",
       "7                  0.007910   \n",
       "103                0.004618   \n",
       "53                 0.007273   \n",
       "\n",
       "     RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation  \\\n",
       "3                                             0.242016                                 \n",
       "70                                            0.290817                                 \n",
       "92                                            0.318683                                 \n",
       "124                                           0.342444                                 \n",
       "56                                            0.400382                                 \n",
       "134                                           0.293455                                 \n",
       "107                                           0.472482                                 \n",
       "78                                            0.260657                                 \n",
       "128                                           0.285639                                 \n",
       "65                                            0.156307                                 \n",
       "42                                            0.271944                                 \n",
       "123                                           0.257929                                 \n",
       "111                                           0.321620                                 \n",
       "0                                             0.339837                                 \n",
       "136                                           0.375639                                 \n",
       "98                                            0.310298                                 \n",
       "90                                            0.222032                                 \n",
       "114                                           0.212013                                 \n",
       "108                                           0.335852                                 \n",
       "131                                           0.294278                                 \n",
       "101                                           0.200118                                 \n",
       "68                                            0.205241                                 \n",
       "95                                            0.184146                                 \n",
       "119                                           0.274726                                 \n",
       "64                                            0.415847                                 \n",
       "77                                            0.213552                                 \n",
       "71                                            0.365632                                 \n",
       "150                                           0.288079                                 \n",
       "7                                             0.335144                                 \n",
       "103                                           0.423214                                 \n",
       "53                                            0.316483                                 \n",
       "\n",
       "     MDCP - Married in a registered marriage  MDCP - Not married  \\\n",
       "3                                   0.385881            0.335227   \n",
       "70                                  0.280871            0.293952   \n",
       "92                                  0.339021            0.282468   \n",
       "124                                 0.340102            0.244735   \n",
       "56                                  0.351234            0.279460   \n",
       "134                                 0.366205            0.304889   \n",
       "107                                 0.181893            0.453533   \n",
       "78                                  0.340778            0.281919   \n",
       "128                                 0.359918            0.336725   \n",
       "65                                  0.400018            0.274307   \n",
       "42                                  0.328731            0.313633   \n",
       "123                                 0.343199            0.286943   \n",
       "111                                 0.292694            0.347325   \n",
       "0                                   0.302321            0.364082   \n",
       "136                                 0.291657            0.330960   \n",
       "98                                  0.359523            0.290243   \n",
       "90                                  0.332070            0.303282   \n",
       "114                                 0.325706            0.280444   \n",
       "108                                 0.431292            0.288801   \n",
       "131                                 0.351900            0.314676   \n",
       "101                                 0.340305            0.256910   \n",
       "68                                  0.347481            0.276180   \n",
       "95                                  0.347075            0.302252   \n",
       "119                                 0.329300            0.286005   \n",
       "64                                  0.282001            0.333703   \n",
       "77                                  0.374747            0.262873   \n",
       "71                                  0.327740            0.346263   \n",
       "150                                 0.362536            0.253436   \n",
       "7                                   0.366788            0.283925   \n",
       "103                                 0.388684            0.255955   \n",
       "53                                  0.323253            0.284138   \n",
       "\n",
       "     MDCP - Not applicable  \n",
       "3                 0.234073  \n",
       "70                0.324590  \n",
       "92                0.292313  \n",
       "124               0.325028  \n",
       "56                0.278461  \n",
       "134               0.253644  \n",
       "107               0.239190  \n",
       "78                0.297467  \n",
       "128               0.243467  \n",
       "65                0.281916  \n",
       "42                0.281880  \n",
       "123               0.288633  \n",
       "111               0.266321  \n",
       "0                 0.259811  \n",
       "136               0.295220  \n",
       "98                0.273720  \n",
       "90                0.289176  \n",
       "114               0.319889  \n",
       "108               0.236670  \n",
       "131               0.258640  \n",
       "101               0.325540  \n",
       "68                0.308948  \n",
       "95                0.285155  \n",
       "119               0.314088  \n",
       "64                0.265895  \n",
       "77                0.290926  \n",
       "71                0.239308  \n",
       "150               0.302479  \n",
       "7                 0.271511  \n",
       "103               0.275573  \n",
       "53                0.301693  \n",
       "\n",
       "[31 rows x 47 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cols = X_test.iloc[:,relevant_cols]\n",
    "best_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f345bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AGE5P - 0-4 years', 'AGE5P - 20-24 years', 'AGE5P - 25-29 years',\n",
       "       'AGE5P - 30-34 years', 'AGE5P - 35-39 years', 'AGE5P - 50-54 years',\n",
       "       'AGE5P - 55-59 years', 'AGE5P - 60-64 years', 'AGE5P - 65-69 years',\n",
       "       'AGE5P - 70-74 years', 'AGE5P - 75-79 years',\n",
       "       'HEAP - Certificate III & IV Level',\n",
       "       'HEAP - Secondary Education - Years 9 and below',\n",
       "       'HEAP - Not applicable', 'INCP - $2,000-$2,999 ($104,000-$155,999)',\n",
       "       'INCP - $3,000 or more ($156,000 or more)', 'INCP - Nil income',\n",
       "       'INCP - Not applicable', 'INDP - Agriculture, Forestry and Fishing',\n",
       "       'INDP - Mining', 'INDP - Public Administration and Safety',\n",
       "       'INDP - Transport, Postal and Warehousing',\n",
       "       'LFHRP - Employed, worked full-time',\n",
       "       'LFHRP - Employed, worked part-time', 'LFHRP - Not in the labour force',\n",
       "       'LFHRP - Not applicable', 'GNGP - National Government',\n",
       "       'GNGP - Private sector', 'MSTP - Never married', 'MSTP - Widowed',\n",
       "       'MSTP - Married', 'MSTP - Not applicable',\n",
       "       'RLHP - Husband, Wife or Partner in a registered marriage',\n",
       "       'RLHP - Natural or adopted child under 15',\n",
       "       'RLHP - Non-dependent natural, or adopted child',\n",
       "       'RLHP - Group household member', 'RLHP - Lone person',\n",
       "       'RLHP - Not applicable', 'RELP - Buddhism', 'RELP - Christianity',\n",
       "       'RELP - Hinduism', 'RELP - Islam', 'RELP - Other Religions',\n",
       "       'RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation',\n",
       "       'MDCP - Married in a registered marriage', 'MDCP - Not married',\n",
       "       'MDCP - Not applicable'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_name = best_cols.columns\n",
    "list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ce0db06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE5P - 0-4 years</th>\n",
       "      <th>AGE5P - 20-24 years</th>\n",
       "      <th>AGE5P - 25-29 years</th>\n",
       "      <th>AGE5P - 30-34 years</th>\n",
       "      <th>AGE5P - 35-39 years</th>\n",
       "      <th>AGE5P - 50-54 years</th>\n",
       "      <th>AGE5P - 55-59 years</th>\n",
       "      <th>AGE5P - 60-64 years</th>\n",
       "      <th>AGE5P - 65-69 years</th>\n",
       "      <th>AGE5P - 70-74 years</th>\n",
       "      <th>...</th>\n",
       "      <th>RLHP - Not applicable</th>\n",
       "      <th>RELP - Buddhism</th>\n",
       "      <th>RELP - Christianity</th>\n",
       "      <th>RELP - Hinduism</th>\n",
       "      <th>RELP - Islam</th>\n",
       "      <th>RELP - Other Religions</th>\n",
       "      <th>RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "      <th>MDCP - Not married</th>\n",
       "      <th>MDCP - Not applicable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.19152</td>\n",
       "      <td>-0.250676</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.401863</td>\n",
       "      <td>-0.228084</td>\n",
       "      <td>0.119506</td>\n",
       "      <td>0.156255</td>\n",
       "      <td>0.192222</td>\n",
       "      <td>0.246282</td>\n",
       "      <td>0.227102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203385</td>\n",
       "      <td>-0.395329</td>\n",
       "      <td>1.152395</td>\n",
       "      <td>-0.303826</td>\n",
       "      <td>-0.865314</td>\n",
       "      <td>-0.152802</td>\n",
       "      <td>0.493993</td>\n",
       "      <td>0.414232</td>\n",
       "      <td>-0.561112</td>\n",
       "      <td>0.14731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.19152</td>\n",
       "      <td>-0.250676</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.401863</td>\n",
       "      <td>-0.228084</td>\n",
       "      <td>0.119506</td>\n",
       "      <td>0.156255</td>\n",
       "      <td>0.192222</td>\n",
       "      <td>0.246282</td>\n",
       "      <td>0.227102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203385</td>\n",
       "      <td>-0.395329</td>\n",
       "      <td>1.152395</td>\n",
       "      <td>-0.303826</td>\n",
       "      <td>-0.865314</td>\n",
       "      <td>-0.152802</td>\n",
       "      <td>0.493993</td>\n",
       "      <td>0.414232</td>\n",
       "      <td>-0.561112</td>\n",
       "      <td>0.14731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGE5P - 0-4 years  AGE5P - 20-24 years  AGE5P - 25-29 years  \\\n",
       "0           -0.19152            -0.250676               -0.393   \n",
       "1           -0.19152            -0.250676               -0.393   \n",
       "\n",
       "   AGE5P - 30-34 years  AGE5P - 35-39 years  AGE5P - 50-54 years  \\\n",
       "0            -0.401863            -0.228084             0.119506   \n",
       "1            -0.401863            -0.228084             0.119506   \n",
       "\n",
       "   AGE5P - 55-59 years  AGE5P - 60-64 years  AGE5P - 65-69 years  \\\n",
       "0             0.156255             0.192222             0.246282   \n",
       "1             0.156255             0.192222             0.246282   \n",
       "\n",
       "   AGE5P - 70-74 years  ...  RLHP - Not applicable  RELP - Buddhism  \\\n",
       "0             0.227102  ...               0.203385        -0.395329   \n",
       "1             0.227102  ...               0.203385        -0.395329   \n",
       "\n",
       "   RELP - Christianity  RELP - Hinduism  RELP - Islam  RELP - Other Religions  \\\n",
       "0             1.152395        -0.303826     -0.865314               -0.152802   \n",
       "1             1.152395        -0.303826     -0.865314               -0.152802   \n",
       "\n",
       "   RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation  \\\n",
       "0                                           0.493993                                 \n",
       "1                                           0.493993                                 \n",
       "\n",
       "   MDCP - Married in a registered marriage  MDCP - Not married  \\\n",
       "0                                 0.414232           -0.561112   \n",
       "1                                 0.414232           -0.561112   \n",
       "\n",
       "   MDCP - Not applicable  \n",
       "0                0.14731  \n",
       "1                0.14731  \n",
       "\n",
       "[2 rows x 47 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_DF = pd.DataFrame(list(map(np.ravel, rfe.estimator_.coef_)), columns=X_train.columns[rfe.support_], index=rfe.estimator_.classes_)\n",
    "coefficient_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c2a0600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AGE5P - 0-4 years                                                                 -0.191520\n",
       "AGE5P - 20-24 years                                                               -0.250676\n",
       "AGE5P - 25-29 years                                                               -0.393000\n",
       "AGE5P - 30-34 years                                                               -0.401863\n",
       "AGE5P - 35-39 years                                                               -0.228084\n",
       "AGE5P - 50-54 years                                                                0.119506\n",
       "AGE5P - 55-59 years                                                                0.156255\n",
       "AGE5P - 60-64 years                                                                0.192222\n",
       "AGE5P - 65-69 years                                                                0.246282\n",
       "AGE5P - 70-74 years                                                                0.227102\n",
       "AGE5P - 75-79 years                                                                0.157763\n",
       "HEAP - Certificate III & IV Level                                                  0.276873\n",
       "HEAP - Secondary Education - Years 9 and below                                    -0.142124\n",
       "HEAP - Not applicable                                                             -0.121207\n",
       "INCP - $2,000-$2,999 ($104,000-$155,999)                                           0.144686\n",
       "INCP - $3,000 or more ($156,000 or more)                                           0.300809\n",
       "INCP - Nil income                                                                 -0.206046\n",
       "INCP - Not applicable                                                             -0.120509\n",
       "INDP - Agriculture, Forestry and Fishing                                           0.296786\n",
       "INDP - Mining                                                                      0.186368\n",
       "INDP - Public Administration and Safety                                           -0.255109\n",
       "INDP - Transport, Postal and Warehousing                                          -0.137790\n",
       "LFHRP - Employed, worked full-time                                                -0.111157\n",
       "LFHRP - Employed, worked part-time                                                 0.173542\n",
       "LFHRP - Not in the labour force                                                    0.129490\n",
       "LFHRP - Not applicable                                                            -0.120885\n",
       "GNGP - National Government                                                        -0.275532\n",
       "GNGP - Private sector                                                              0.327695\n",
       "MSTP - Never married                                                              -0.618718\n",
       "MSTP - Widowed                                                                     0.132538\n",
       "MSTP - Married                                                                     0.519304\n",
       "MSTP - Not applicable                                                             -0.121227\n",
       "RLHP - Husband, Wife or Partner in a registered marriage                           0.392106\n",
       "RLHP - Natural or adopted child under 15                                          -0.136775\n",
       "RLHP - Non-dependent natural, or adopted child                                    -0.221380\n",
       "RLHP - Group household member                                                     -0.184728\n",
       "RLHP - Lone person                                                                 0.206686\n",
       "RLHP - Not applicable                                                              0.203385\n",
       "RELP - Buddhism                                                                   -0.395329\n",
       "RELP - Christianity                                                                1.152395\n",
       "RELP - Hinduism                                                                   -0.303826\n",
       "RELP - Islam                                                                      -0.865314\n",
       "RELP - Other Religions                                                            -0.152802\n",
       "RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation    0.493993\n",
       "MDCP - Married in a registered marriage                                            0.414232\n",
       "MDCP - Not married                                                                -0.561112\n",
       "MDCP - Not applicable                                                              0.147310\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_DF.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e47d0",
   "metadata": {},
   "source": [
    "# 3.2 Interpretation (is_LNP) (65%)\n",
    "\n",
    "## Significant Positive Influence\n",
    "- Christian (1.152395) \n",
    "- Married (0.519304)\n",
    "- Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation (0.493993)\n",
    "- registered marriage (0.414232)\n",
    "- Husband, Wife or Partner in a registered marriage (0.392106)\n",
    "\n",
    "## Significant Negative Influence\n",
    "- Islam (-0.865314)\n",
    "- Never married (-0.618718)\n",
    "- Not married (-0.561112)\n",
    "- 30-34 years (-0.401863)\n",
    "- Buddhism (-0.395329)\n",
    "- 25-29 years (-0.393000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3b2aa",
   "metadata": {},
   "source": [
    "# Number of Seats LNP Win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a43bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [0, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 25, 28, 31, 38, 39, 46, 47, 51, 61, 66, 69, 71, 72, 77, 79, 80, 83, 86, 87, 90, 91, 92, 97, 106, 118, 119, 122, 123, 124, 125, 126, 128, 129, 134, 136, 137]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "      <th>Electorate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.18</td>\n",
       "      <td>Adelaide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60.13</td>\n",
       "      <td>Aston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.98</td>\n",
       "      <td>Ballarat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56.26</td>\n",
       "      <td>Banks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68.94</td>\n",
       "      <td>Barker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.47</td>\n",
       "      <td>Werriwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.91</td>\n",
       "      <td>Whitlam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63.15</td>\n",
       "      <td>Wide Bay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.17</td>\n",
       "      <td>Wills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64.58</td>\n",
       "      <td>Wright</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent Electorate\n",
       "0         0          0         58.18   Adelaide\n",
       "1         1          1         60.13      Aston\n",
       "2         0          1         60.98   Ballarat\n",
       "3         1          1         56.26      Banks\n",
       "4         1          1         68.94     Barker\n",
       "..      ...        ...           ...        ...\n",
       "146       0          0         55.47    Werriwa\n",
       "147       0          1         60.91    Whitlam\n",
       "148       1          1         63.15   Wide Bay\n",
       "149       0          0         58.17      Wills\n",
       "150       1          1         64.58     Wright\n",
       "\n",
       "[151 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of seats the LNP win: 98\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(adjusted_master['is_LNP'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(adjusted_master.drop(['index', 'LGA','PartyNm', 'PreferencePercent',\n",
    "                                                                'Strength', 'is_ALP', 'is_LNP', 'is_Other'], axis=1)))\n",
    "result.insert(2, \"Pref Percent\", adjusted_master['PreferencePercent'].values)\n",
    "result.insert(3, \"Electorate\", adjusted_master['LGA'].values)\n",
    "display(result)\n",
    "\n",
    "print('Number of seats the LNP win:',result['Predicted'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4bc798fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          Aston\n",
       "2       Ballarat\n",
       "3          Banks\n",
       "4         Barker\n",
       "6           Bass\n",
       "         ...    \n",
       "142       Wannon\n",
       "143    Warringah\n",
       "147      Whitlam\n",
       "148     Wide Bay\n",
       "150       Wright\n",
       "Name: Electorate, Length: 98, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LNP_Wins = result[result['Predicted'] == 1]\n",
    "LNP_Wins['Electorate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776dad8",
   "metadata": {},
   "source": [
    "# 3.3 is_Other Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "794b5edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 146)\n",
      "(31, 146)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(adjusted_master, test_size=0.2, random_state=142)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f67792fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other'], axis=1)\n",
    "y_train = train['is_Other']\n",
    "X_test = test.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other'], axis=1)\n",
    "y_test = test['is_Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "81a51f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set:  0.9666666666666667\n",
      "Accuracy score on testing set:  0.9354838709677419\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "print(\"Accuracy score on training set: \", accuracy_score(y_train, y_hat_train))\n",
    "print(\"Accuracy score on testing set: \", accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26a9166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using 1 features:  0.9354838709677419\n",
      "Accuracy on test set using 2 features:  0.9354838709677419\n",
      "Accuracy on test set using 3 features:  0.9354838709677419\n",
      "Accuracy on test set using 4 features:  0.9354838709677419\n",
      "Accuracy on test set using 5 features:  0.9354838709677419\n",
      "Accuracy on test set using 6 features:  0.9354838709677419\n",
      "Accuracy on test set using 7 features:  0.9354838709677419\n",
      "Accuracy on test set using 8 features:  0.9354838709677419\n",
      "Accuracy on test set using 9 features:  0.9354838709677419\n",
      "Accuracy on test set using 10 features:  0.9354838709677419\n",
      "Accuracy on test set using 11 features:  0.9354838709677419\n",
      "Accuracy on test set using 12 features:  0.9354838709677419\n",
      "Accuracy on test set using 13 features:  0.9354838709677419\n",
      "Accuracy on test set using 14 features:  0.9354838709677419\n",
      "Accuracy on test set using 15 features:  0.9354838709677419\n",
      "Accuracy on test set using 16 features:  0.9354838709677419\n",
      "Accuracy on test set using 17 features:  0.9354838709677419\n",
      "Accuracy on test set using 18 features:  0.9354838709677419\n",
      "Accuracy on test set using 19 features:  0.9354838709677419\n",
      "Accuracy on test set using 20 features:  0.9354838709677419\n",
      "Accuracy on test set using 21 features:  0.9354838709677419\n",
      "Accuracy on test set using 22 features:  0.9354838709677419\n",
      "Accuracy on test set using 23 features:  0.9354838709677419\n",
      "Accuracy on test set using 24 features:  0.9354838709677419\n",
      "Accuracy on test set using 25 features:  0.9354838709677419\n",
      "Accuracy on test set using 26 features:  0.9354838709677419\n",
      "Accuracy on test set using 27 features:  0.9354838709677419\n",
      "Accuracy on test set using 28 features:  0.9354838709677419\n",
      "Accuracy on test set using 29 features:  0.9354838709677419\n",
      "Accuracy on test set using 30 features:  0.9354838709677419\n",
      "Accuracy on test set using 31 features:  0.9354838709677419\n",
      "Accuracy on test set using 32 features:  0.9354838709677419\n",
      "Accuracy on test set using 33 features:  0.9354838709677419\n",
      "Accuracy on test set using 34 features:  0.9354838709677419\n",
      "Accuracy on test set using 35 features:  0.9354838709677419\n",
      "Accuracy on test set using 36 features:  0.9354838709677419\n",
      "Accuracy on test set using 37 features:  0.9354838709677419\n",
      "Accuracy on test set using 38 features:  0.9354838709677419\n",
      "Accuracy on test set using 39 features:  0.9354838709677419\n",
      "Accuracy on test set using 40 features:  0.9354838709677419\n",
      "Accuracy on test set using 41 features:  0.9354838709677419\n",
      "Accuracy on test set using 42 features:  0.9354838709677419\n",
      "Accuracy on test set using 43 features:  0.9354838709677419\n",
      "Accuracy on test set using 44 features:  0.9354838709677419\n",
      "Accuracy on test set using 45 features:  0.9354838709677419\n",
      "Accuracy on test set using 46 features:  0.9354838709677419\n",
      "Accuracy on test set using 47 features:  0.9354838709677419\n",
      "Accuracy on test set using 48 features:  0.9354838709677419\n",
      "Accuracy on test set using 49 features:  0.9354838709677419\n",
      "Accuracy on test set using 50 features:  0.9354838709677419\n",
      "Accuracy on test set using 51 features:  0.9354838709677419\n",
      "Accuracy on test set using 52 features:  0.9354838709677419\n",
      "Accuracy on test set using 53 features:  0.9354838709677419\n",
      "Accuracy on test set using 54 features:  0.9354838709677419\n",
      "Accuracy on test set using 55 features:  0.9354838709677419\n",
      "Accuracy on test set using 56 features:  0.9354838709677419\n",
      "Accuracy on test set using 57 features:  0.9354838709677419\n",
      "Accuracy on test set using 58 features:  0.9354838709677419\n",
      "Accuracy on test set using 59 features:  0.9354838709677419\n",
      "Accuracy on test set using 60 features:  0.9354838709677419\n",
      "Accuracy on test set using 61 features:  0.9354838709677419\n",
      "Accuracy on test set using 62 features:  0.9354838709677419\n",
      "Accuracy on test set using 63 features:  0.9354838709677419\n",
      "Accuracy on test set using 64 features:  0.9354838709677419\n",
      "Accuracy on test set using 65 features:  0.9354838709677419\n",
      "Accuracy on test set using 66 features:  0.9354838709677419\n",
      "Accuracy on test set using 67 features:  0.9354838709677419\n",
      "Accuracy on test set using 68 features:  0.9354838709677419\n",
      "Accuracy on test set using 69 features:  0.9354838709677419\n",
      "Accuracy on test set using 70 features:  0.9354838709677419\n",
      "Accuracy on test set using 71 features:  0.9354838709677419\n",
      "Accuracy on test set using 72 features:  0.9354838709677419\n",
      "Accuracy on test set using 73 features:  0.9354838709677419\n",
      "Accuracy on test set using 74 features:  0.9354838709677419\n",
      "Accuracy on test set using 75 features:  0.9354838709677419\n",
      "Accuracy on test set using 76 features:  0.9354838709677419\n",
      "Accuracy on test set using 77 features:  0.9354838709677419\n",
      "Accuracy on test set using 78 features:  0.9354838709677419\n",
      "Accuracy on test set using 79 features:  0.9354838709677419\n",
      "Accuracy on test set using 80 features:  0.9354838709677419\n",
      "Accuracy on test set using 81 features:  0.9354838709677419\n",
      "Accuracy on test set using 82 features:  0.9354838709677419\n",
      "Accuracy on test set using 83 features:  0.9354838709677419\n",
      "Accuracy on test set using 84 features:  0.9354838709677419\n",
      "Accuracy on test set using 85 features:  0.9354838709677419\n",
      "Accuracy on test set using 86 features:  0.9354838709677419\n",
      "Accuracy on test set using 87 features:  0.9354838709677419\n",
      "Accuracy on test set using 88 features:  0.9354838709677419\n",
      "Accuracy on test set using 89 features:  0.9354838709677419\n",
      "Accuracy on test set using 90 features:  0.9354838709677419\n",
      "Accuracy on test set using 91 features:  0.9354838709677419\n",
      "Accuracy on test set using 92 features:  0.9354838709677419\n",
      "Accuracy on test set using 93 features:  0.9354838709677419\n",
      "Accuracy on test set using 94 features:  0.9354838709677419\n",
      "Accuracy on test set using 95 features:  0.9354838709677419\n",
      "Accuracy on test set using 96 features:  0.9354838709677419\n",
      "Accuracy on test set using 97 features:  0.9354838709677419\n",
      "Accuracy on test set using 98 features:  0.9354838709677419\n",
      "Accuracy on test set using 99 features:  0.9354838709677419\n",
      "Accuracy on test set using 100 features:  0.9354838709677419\n",
      "Accuracy on test set using 101 features:  0.9354838709677419\n",
      "Accuracy on test set using 102 features:  0.9354838709677419\n",
      "Accuracy on test set using 103 features:  0.9354838709677419\n",
      "Accuracy on test set using 104 features:  0.9354838709677419\n",
      "Accuracy on test set using 105 features:  0.9354838709677419\n",
      "Accuracy on test set using 106 features:  0.9354838709677419\n",
      "Accuracy on test set using 107 features:  0.9354838709677419\n",
      "Accuracy on test set using 108 features:  0.9354838709677419\n",
      "Accuracy on test set using 109 features:  0.9354838709677419\n",
      "Accuracy on test set using 110 features:  0.9354838709677419\n",
      "Accuracy on test set using 111 features:  0.9354838709677419\n",
      "Accuracy on test set using 112 features:  0.9354838709677419\n",
      "Accuracy on test set using 113 features:  0.9354838709677419\n",
      "Accuracy on test set using 114 features:  0.9354838709677419\n",
      "Accuracy on test set using 115 features:  0.9354838709677419\n",
      "Accuracy on test set using 116 features:  0.9354838709677419\n",
      "Accuracy on test set using 117 features:  0.9354838709677419\n",
      "Accuracy on test set using 118 features:  0.9354838709677419\n",
      "Accuracy on test set using 119 features:  0.9354838709677419\n",
      "Accuracy on test set using 120 features:  0.9354838709677419\n",
      "Accuracy on test set using 121 features:  0.9354838709677419\n",
      "Accuracy on test set using 122 features:  0.9354838709677419\n",
      "Accuracy on test set using 123 features:  0.9354838709677419\n",
      "Accuracy on test set using 124 features:  0.9354838709677419\n",
      "Accuracy on test set using 125 features:  0.9354838709677419\n",
      "Accuracy on test set using 126 features:  0.9354838709677419\n",
      "Accuracy on test set using 127 features:  0.9354838709677419\n",
      "Accuracy on test set using 128 features:  0.9354838709677419\n",
      "Accuracy on test set using 129 features:  0.9354838709677419\n",
      "Accuracy on test set using 130 features:  0.9354838709677419\n",
      "Accuracy on test set using 131 features:  0.9354838709677419\n",
      "Accuracy on test set using 132 features:  0.9354838709677419\n",
      "Accuracy on test set using 133 features:  0.9354838709677419\n",
      "Accuracy on test set using 134 features:  0.9354838709677419\n",
      "Accuracy on test set using 135 features:  0.9354838709677419\n",
      "Accuracy on test set using 136 features:  0.9354838709677419\n",
      "Accuracy on test set using 137 features:  0.9354838709677419\n",
      "Accuracy on test set using 138 features:  0.9354838709677419\n",
      "Accuracy on test set using 139 features:  0.9354838709677419\n",
      "1 columns provide the highest accuracy at: 93.55 %\n"
     ]
    }
   ],
   "source": [
    "acc_scores = []\n",
    "for i in range(1,140):\n",
    "    clf = LogisticRegression()\n",
    "    rfe = RFE(estimator=clf, n_features_to_select=i)\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfe.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy on test set using\", i, \"features: \", acc_score)\n",
    "    \n",
    "    acc_scores.append(acc_score)\n",
    "\n",
    "best = acc_scores[0]\n",
    "best_number_of_columns = 0;\n",
    "for i in range(len(acc_scores)):\n",
    "    if acc_scores[i] > best:\n",
    "        best = acc_scores[i]\n",
    "        best_number_of_columns = i\n",
    "        \n",
    "print(best_number_of_columns + 1, 'columns provide the highest accuracy at: %.2f' % (best*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a384d3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test set:  0.9354838709677419\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "rfe = RFE(estimator=lr_model, n_features_to_select=1, step=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "y_test_hat = rfe.predict(X_test)\n",
    "print(\"accuracy score on test set: \", accuracy_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "674f836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected False, Rank: 47.000\n",
      "Column: 1, Selected False, Rank: 93.000\n",
      "Column: 2, Selected False, Rank: 74.000\n",
      "Column: 3, Selected False, Rank: 66.000\n",
      "Column: 4, Selected False, Rank: 18.000\n",
      "Column: 5, Selected False, Rank: 21.000\n",
      "Column: 6, Selected False, Rank: 23.000\n",
      "Column: 7, Selected False, Rank: 42.000\n",
      "Column: 8, Selected False, Rank: 108.000\n",
      "Column: 9, Selected False, Rank: 80.000\n",
      "Column: 10, Selected False, Rank: 53.000\n",
      "Column: 11, Selected False, Rank: 33.000\n",
      "Column: 12, Selected False, Rank: 35.000\n",
      "Column: 13, Selected False, Rank: 34.000\n",
      "Column: 14, Selected False, Rank: 36.000\n",
      "Column: 15, Selected False, Rank: 63.000\n",
      "Column: 16, Selected False, Rank: 81.000\n",
      "Column: 17, Selected False, Rank: 85.000\n",
      "Column: 18, Selected False, Rank: 109.000\n",
      "Column: 19, Selected False, Rank: 130.000\n",
      "Column: 20, Selected False, Rank: 133.000\n",
      "Column: 21, Selected False, Rank: 55.000\n",
      "Column: 22, Selected False, Rank: 76.000\n",
      "Column: 23, Selected False, Rank: 41.000\n",
      "Column: 24, Selected False, Rank: 54.000\n",
      "Column: 25, Selected False, Rank: 110.000\n",
      "Column: 26, Selected False, Rank: 13.000\n",
      "Column: 27, Selected False, Rank: 127.000\n",
      "Column: 28, Selected False, Rank: 116.000\n",
      "Column: 29, Selected False, Rank: 49.000\n",
      "Column: 30, Selected False, Rank: 19.000\n",
      "Column: 31, Selected False, Rank: 100.000\n",
      "Column: 32, Selected False, Rank: 82.000\n",
      "Column: 33, Selected False, Rank: 97.000\n",
      "Column: 34, Selected False, Rank: 95.000\n",
      "Column: 35, Selected False, Rank: 111.000\n",
      "Column: 36, Selected False, Rank: 70.000\n",
      "Column: 37, Selected False, Rank: 56.000\n",
      "Column: 38, Selected False, Rank: 94.000\n",
      "Column: 39, Selected False, Rank: 14.000\n",
      "Column: 40, Selected False, Rank: 73.000\n",
      "Column: 41, Selected False, Rank: 64.000\n",
      "Column: 42, Selected False, Rank: 58.000\n",
      "Column: 43, Selected False, Rank: 69.000\n",
      "Column: 44, Selected False, Rank: 103.000\n",
      "Column: 45, Selected False, Rank: 105.000\n",
      "Column: 46, Selected False, Rank: 6.000\n",
      "Column: 47, Selected False, Rank: 99.000\n",
      "Column: 48, Selected False, Rank: 25.000\n",
      "Column: 49, Selected False, Rank: 68.000\n",
      "Column: 50, Selected False, Rank: 90.000\n",
      "Column: 51, Selected False, Rank: 12.000\n",
      "Column: 52, Selected False, Rank: 112.000\n",
      "Column: 53, Selected False, Rank: 24.000\n",
      "Column: 54, Selected False, Rank: 87.000\n",
      "Column: 55, Selected False, Rank: 135.000\n",
      "Column: 56, Selected False, Rank: 59.000\n",
      "Column: 57, Selected False, Rank: 114.000\n",
      "Column: 58, Selected False, Rank: 86.000\n",
      "Column: 59, Selected False, Rank: 77.000\n",
      "Column: 60, Selected False, Rank: 46.000\n",
      "Column: 61, Selected False, Rank: 134.000\n",
      "Column: 62, Selected False, Rank: 50.000\n",
      "Column: 63, Selected False, Rank: 102.000\n",
      "Column: 64, Selected False, Rank: 89.000\n",
      "Column: 65, Selected False, Rank: 45.000\n",
      "Column: 66, Selected False, Rank: 44.000\n",
      "Column: 67, Selected False, Rank: 124.000\n",
      "Column: 68, Selected False, Rank: 60.000\n",
      "Column: 69, Selected False, Rank: 37.000\n",
      "Column: 70, Selected False, Rank: 72.000\n",
      "Column: 71, Selected False, Rank: 96.000\n",
      "Column: 72, Selected False, Rank: 71.000\n",
      "Column: 73, Selected False, Rank: 79.000\n",
      "Column: 74, Selected False, Rank: 119.000\n",
      "Column: 75, Selected False, Rank: 62.000\n",
      "Column: 76, Selected False, Rank: 61.000\n",
      "Column: 77, Selected False, Rank: 40.000\n",
      "Column: 78, Selected False, Rank: 31.000\n",
      "Column: 79, Selected False, Rank: 98.000\n",
      "Column: 80, Selected False, Rank: 48.000\n",
      "Column: 81, Selected False, Rank: 27.000\n",
      "Column: 82, Selected False, Rank: 78.000\n",
      "Column: 83, Selected False, Rank: 113.000\n",
      "Column: 84, Selected False, Rank: 101.000\n",
      "Column: 85, Selected False, Rank: 43.000\n",
      "Column: 86, Selected False, Rank: 38.000\n",
      "Column: 87, Selected False, Rank: 51.000\n",
      "Column: 88, Selected False, Rank: 22.000\n",
      "Column: 89, Selected False, Rank: 128.000\n",
      "Column: 90, Selected False, Rank: 7.000\n",
      "Column: 91, Selected False, Rank: 92.000\n",
      "Column: 92, Selected False, Rank: 3.000\n",
      "Column: 93, Selected False, Rank: 9.000\n",
      "Column: 94, Selected False, Rank: 118.000\n",
      "Column: 95, Selected False, Rank: 121.000\n",
      "Column: 96, Selected False, Rank: 107.000\n",
      "Column: 97, Selected False, Rank: 39.000\n",
      "Column: 98, Selected False, Rank: 106.000\n",
      "Column: 99, Selected False, Rank: 122.000\n",
      "Column: 100, Selected False, Rank: 132.000\n",
      "Column: 101, Selected False, Rank: 138.000\n",
      "Column: 102, Selected False, Rank: 131.000\n",
      "Column: 103, Selected False, Rank: 28.000\n",
      "Column: 104, Selected False, Rank: 115.000\n",
      "Column: 105, Selected False, Rank: 137.000\n",
      "Column: 106, Selected False, Rank: 16.000\n",
      "Column: 107, Selected False, Rank: 91.000\n",
      "Column: 108, Selected False, Rank: 136.000\n",
      "Column: 109, Selected False, Rank: 67.000\n",
      "Column: 110, Selected False, Rank: 75.000\n",
      "Column: 111, Selected False, Rank: 129.000\n",
      "Column: 112, Selected False, Rank: 125.000\n",
      "Column: 113, Selected False, Rank: 117.000\n",
      "Column: 114, Selected False, Rank: 126.000\n",
      "Column: 115, Selected False, Rank: 104.000\n",
      "Column: 116, Selected False, Rank: 123.000\n",
      "Column: 117, Selected False, Rank: 65.000\n",
      "Column: 118, Selected False, Rank: 88.000\n",
      "Column: 119, Selected False, Rank: 5.000\n",
      "Column: 120, Selected False, Rank: 29.000\n",
      "Column: 121, Selected False, Rank: 32.000\n",
      "Column: 122, Selected False, Rank: 30.000\n",
      "Column: 123, Selected False, Rank: 10.000\n",
      "Column: 124, Selected False, Rank: 26.000\n",
      "Column: 125, Selected False, Rank: 15.000\n",
      "Column: 126, Selected False, Rank: 2.000\n",
      "Column: 127, Selected False, Rank: 57.000\n",
      "Column: 128, Selected False, Rank: 52.000\n",
      "Column: 129, Selected True, Rank: 1.000\n",
      "Column: 130, Selected False, Rank: 120.000\n",
      "Column: 131, Selected False, Rank: 20.000\n",
      "Column: 132, Selected False, Rank: 83.000\n",
      "Column: 133, Selected False, Rank: 84.000\n",
      "Column: 134, Selected False, Rank: 4.000\n",
      "Column: 135, Selected False, Rank: 11.000\n",
      "Column: 136, Selected False, Rank: 17.000\n",
      "Column: 137, Selected False, Rank: 8.000\n",
      "[129]\n"
     ]
    }
   ],
   "source": [
    "relevant_cols = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    if rfe.support_[i] == True:\n",
    "        relevant_cols.append(i)\n",
    "        \n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a08e714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [129]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent\n",
       "3         0          0         56.26\n",
       "70        0          0         58.36\n",
       "92        0          0         53.28\n",
       "124       0          0         57.52\n",
       "56        0          0         62.21\n",
       "134       0          0         54.45\n",
       "107       1          0         71.83\n",
       "78        0          0         52.98\n",
       "128       0          0         53.18\n",
       "65        0          0         52.80\n",
       "42        0          0         51.50\n",
       "123       0          0         55.04\n",
       "111       0          0         65.36\n",
       "0         0          0         58.18\n",
       "136       0          0         64.13\n",
       "98        0          0         50.19\n",
       "90        0          0         55.04\n",
       "114       0          0         64.36\n",
       "108       0          0         57.53\n",
       "131       0          0         54.24\n",
       "101       0          0         72.49\n",
       "68        0          0         70.48\n",
       "95        0          0         58.40\n",
       "119       0          0         56.39\n",
       "64        0          0         66.30\n",
       "77        0          0         62.99\n",
       "71        0          0         53.88\n",
       "150       0          0         64.58\n",
       "7         0          0         57.52\n",
       "103       1          0         55.14\n",
       "53        0          0         58.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(test['is_Other'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(X_test))\n",
    "result.insert(2, \"Pref Percent\", test['PreferencePercent'].values)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42c7b5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.290817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.318683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.342444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.400382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.293455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.472482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.260657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.285639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.156307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.271944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.257929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.321620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.339837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.375639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.310298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.222032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.212013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.335852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.294278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.200118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.205241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.184146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.274726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.415847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.213552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.365632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.288079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.335144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.423214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.316483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation\n",
       "3                                             0.242016                              \n",
       "70                                            0.290817                              \n",
       "92                                            0.318683                              \n",
       "124                                           0.342444                              \n",
       "56                                            0.400382                              \n",
       "134                                           0.293455                              \n",
       "107                                           0.472482                              \n",
       "78                                            0.260657                              \n",
       "128                                           0.285639                              \n",
       "65                                            0.156307                              \n",
       "42                                            0.271944                              \n",
       "123                                           0.257929                              \n",
       "111                                           0.321620                              \n",
       "0                                             0.339837                              \n",
       "136                                           0.375639                              \n",
       "98                                            0.310298                              \n",
       "90                                            0.222032                              \n",
       "114                                           0.212013                              \n",
       "108                                           0.335852                              \n",
       "131                                           0.294278                              \n",
       "101                                           0.200118                              \n",
       "68                                            0.205241                              \n",
       "95                                            0.184146                              \n",
       "119                                           0.274726                              \n",
       "64                                            0.415847                              \n",
       "77                                            0.213552                              \n",
       "71                                            0.365632                              \n",
       "150                                           0.288079                              \n",
       "7                                             0.335144                              \n",
       "103                                           0.423214                              \n",
       "53                                            0.316483                              "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cols = X_test.iloc[:,relevant_cols]\n",
    "best_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3ebd0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation\n",
       "0                                             0.1457                              \n",
       "1                                             0.1457                              "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_DF = pd.DataFrame(list(map(np.ravel, rfe.estimator_.coef_)), columns=X_train.columns[rfe.support_], index=rfe.estimator_.classes_)\n",
    "coefficient_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f83b91",
   "metadata": {},
   "source": [
    "# 3.3 Interpretation is_Other (Caution) (94%)\n",
    "\n",
    "Though The model says it a high accuracy at 93%, this is a misguided interpretation as for all outputs it is outputing 0, i.e. not other. Hence this model can't predict whether an electorate is not ALP/LNP. The highest coefficient is (0.1457) it is very weak which explains why the model can't accurately predict a seat that is 'Other'. I would contend that the reasons why this Logistic regression model can't predict 'Other' electorates is due to there not being enough of them in order to correctly train the model, the total dataset has only 7 data points, also within the 'Other' category I have a group of party's like the Greens with Katters' Australian Party, Greens' have left-wing policies, KAP have Right-wing, I would doubt the composition of these electorates are similar.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079364de",
   "metadata": {},
   "source": [
    "# Number of seats Other wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4f7ce893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [129]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "      <th>Electorate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.18</td>\n",
       "      <td>Adelaide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60.13</td>\n",
       "      <td>Aston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.98</td>\n",
       "      <td>Ballarat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56.26</td>\n",
       "      <td>Banks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>68.94</td>\n",
       "      <td>Barker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.47</td>\n",
       "      <td>Werriwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.91</td>\n",
       "      <td>Whitlam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>63.15</td>\n",
       "      <td>Wide Bay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.17</td>\n",
       "      <td>Wills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64.58</td>\n",
       "      <td>Wright</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent Electorate\n",
       "0         0          0         58.18   Adelaide\n",
       "1         1          0         60.13      Aston\n",
       "2         0          0         60.98   Ballarat\n",
       "3         1          0         56.26      Banks\n",
       "4         1          0         68.94     Barker\n",
       "..      ...        ...           ...        ...\n",
       "146       0          0         55.47    Werriwa\n",
       "147       0          0         60.91    Whitlam\n",
       "148       1          0         63.15   Wide Bay\n",
       "149       0          0         58.17      Wills\n",
       "150       1          0         64.58     Wright\n",
       "\n",
       "[151 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of seats the Other win: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(adjusted_master['is_LNP'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(adjusted_master.drop(['index', 'LGA','PartyNm', 'PreferencePercent',\n",
    "                                                                'Strength', 'is_ALP', 'is_LNP', 'is_Other'], axis=1)))\n",
    "result.insert(2, \"Pref Percent\", adjusted_master['PreferencePercent'].values)\n",
    "result.insert(3, \"Electorate\", adjusted_master['LGA'].values)\n",
    "display(result)\n",
    "\n",
    "print('Number of seats the Other win:',result['Predicted'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c97c167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Electorate, dtype: object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Other_Wins = result[result['Predicted'] == 1]\n",
    "Other_Wins['Electorate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee87d00",
   "metadata": {},
   "source": [
    "# 4 More Specific model (Safe seats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06a479dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>LGA</th>\n",
       "      <th>AGE5P - 0-4 years</th>\n",
       "      <th>AGE5P - 5-9 years</th>\n",
       "      <th>AGE5P - 10-14 years</th>\n",
       "      <th>AGE5P - 15-19 years</th>\n",
       "      <th>AGE5P - 20-24 years</th>\n",
       "      <th>AGE5P - 25-29 years</th>\n",
       "      <th>AGE5P - 30-34 years</th>\n",
       "      <th>AGE5P - 35-39 years</th>\n",
       "      <th>...</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "      <th>MDCP - Married in a de facto marriage</th>\n",
       "      <th>MDCP - Not married</th>\n",
       "      <th>MDCP - Not applicable</th>\n",
       "      <th>PreferencePercent</th>\n",
       "      <th>PartyNm</th>\n",
       "      <th>Strength</th>\n",
       "      <th>is_ALP</th>\n",
       "      <th>is_LNP</th>\n",
       "      <th>is_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>0.050723</td>\n",
       "      <td>0.045462</td>\n",
       "      <td>0.057361</td>\n",
       "      <td>0.096777</td>\n",
       "      <td>0.092626</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302321</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.364082</td>\n",
       "      <td>0.259811</td>\n",
       "      <td>58.18</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Fairly Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Aston</td>\n",
       "      <td>0.058553</td>\n",
       "      <td>0.058845</td>\n",
       "      <td>0.058346</td>\n",
       "      <td>0.064261</td>\n",
       "      <td>0.066246</td>\n",
       "      <td>0.064391</td>\n",
       "      <td>0.067479</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393949</td>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.305368</td>\n",
       "      <td>0.239929</td>\n",
       "      <td>60.13</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>0.062608</td>\n",
       "      <td>0.066055</td>\n",
       "      <td>0.061419</td>\n",
       "      <td>0.062910</td>\n",
       "      <td>0.065860</td>\n",
       "      <td>0.060216</td>\n",
       "      <td>0.059006</td>\n",
       "      <td>0.059537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323363</td>\n",
       "      <td>0.081842</td>\n",
       "      <td>0.301819</td>\n",
       "      <td>0.292976</td>\n",
       "      <td>60.98</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.059368</td>\n",
       "      <td>0.055980</td>\n",
       "      <td>0.060472</td>\n",
       "      <td>0.068598</td>\n",
       "      <td>0.074162</td>\n",
       "      <td>0.073559</td>\n",
       "      <td>0.066018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385881</td>\n",
       "      <td>0.044820</td>\n",
       "      <td>0.335227</td>\n",
       "      <td>0.234073</td>\n",
       "      <td>56.26</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Fairly Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Barker</td>\n",
       "      <td>0.055697</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>0.062771</td>\n",
       "      <td>0.059757</td>\n",
       "      <td>0.050169</td>\n",
       "      <td>0.051558</td>\n",
       "      <td>0.054241</td>\n",
       "      <td>0.054597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359438</td>\n",
       "      <td>0.078180</td>\n",
       "      <td>0.273848</td>\n",
       "      <td>0.288534</td>\n",
       "      <td>68.94</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>164</td>\n",
       "      <td>Werriwa</td>\n",
       "      <td>0.072655</td>\n",
       "      <td>0.078559</td>\n",
       "      <td>0.077535</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.072380</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>0.071789</td>\n",
       "      <td>0.070232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358736</td>\n",
       "      <td>0.039473</td>\n",
       "      <td>0.313827</td>\n",
       "      <td>0.287964</td>\n",
       "      <td>55.47</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Marginal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>165</td>\n",
       "      <td>Whitlam</td>\n",
       "      <td>0.060113</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>0.064873</td>\n",
       "      <td>0.066358</td>\n",
       "      <td>0.055673</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366147</td>\n",
       "      <td>0.067582</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.271771</td>\n",
       "      <td>60.91</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>166</td>\n",
       "      <td>Wide Bay</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.060062</td>\n",
       "      <td>0.064301</td>\n",
       "      <td>0.056468</td>\n",
       "      <td>0.042020</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.049902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350901</td>\n",
       "      <td>0.080181</td>\n",
       "      <td>0.284154</td>\n",
       "      <td>0.284764</td>\n",
       "      <td>63.15</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>167</td>\n",
       "      <td>Wills</td>\n",
       "      <td>0.062568</td>\n",
       "      <td>0.052921</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.043616</td>\n",
       "      <td>0.080062</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>0.108720</td>\n",
       "      <td>0.087122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304067</td>\n",
       "      <td>0.093583</td>\n",
       "      <td>0.361030</td>\n",
       "      <td>0.241320</td>\n",
       "      <td>58.17</td>\n",
       "      <td>ALP</td>\n",
       "      <td>Fairly Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>168</td>\n",
       "      <td>Wright</td>\n",
       "      <td>0.060883</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>0.075367</td>\n",
       "      <td>0.070010</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.061605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362536</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>0.253436</td>\n",
       "      <td>0.302479</td>\n",
       "      <td>64.58</td>\n",
       "      <td>LNP</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       LGA  AGE5P - 0-4 years  AGE5P - 5-9 years  \\\n",
       "0        0  Adelaide           0.053945           0.050723   \n",
       "1        1     Aston           0.058553           0.058845   \n",
       "2        2  Ballarat           0.062608           0.066055   \n",
       "3        3     Banks           0.060613           0.059368   \n",
       "4        4    Barker           0.055697           0.063267   \n",
       "..     ...       ...                ...                ...   \n",
       "146    164   Werriwa           0.072655           0.078559   \n",
       "147    165   Whitlam           0.060113           0.065839   \n",
       "148    166  Wide Bay           0.048858           0.060062   \n",
       "149    167     Wills           0.062568           0.052921   \n",
       "150    168    Wright           0.060883           0.074120   \n",
       "\n",
       "     AGE5P - 10-14 years  AGE5P - 15-19 years  AGE5P - 20-24 years  \\\n",
       "0               0.045462             0.057361             0.096777   \n",
       "1               0.058346             0.064261             0.066246   \n",
       "2               0.061419             0.062910             0.065860   \n",
       "3               0.055980             0.060472             0.068598   \n",
       "4               0.062771             0.059757             0.050169   \n",
       "..                   ...                  ...                  ...   \n",
       "146             0.077535             0.077236             0.072380   \n",
       "147             0.064873             0.066358             0.055673   \n",
       "148             0.064301             0.056468             0.042020   \n",
       "149             0.043203             0.043616             0.080062   \n",
       "150             0.075367             0.070010             0.055936   \n",
       "\n",
       "     AGE5P - 25-29 years  AGE5P - 30-34 years  AGE5P - 35-39 years  ...  \\\n",
       "0               0.092626             0.088211             0.070221  ...   \n",
       "1               0.064391             0.067479             0.066700  ...   \n",
       "2               0.060216             0.059006             0.059537  ...   \n",
       "3               0.074162             0.073559             0.066018  ...   \n",
       "4               0.051558             0.054241             0.054597  ...   \n",
       "..                   ...                  ...                  ...  ...   \n",
       "146             0.065587             0.071789             0.070232  ...   \n",
       "147             0.052804             0.056448             0.056678  ...   \n",
       "148             0.040191             0.044199             0.049902  ...   \n",
       "149             0.112030             0.108720             0.087122  ...   \n",
       "150             0.051039             0.056818             0.061605  ...   \n",
       "\n",
       "     MDCP - Married in a registered marriage  \\\n",
       "0                                   0.302321   \n",
       "1                                   0.393949   \n",
       "2                                   0.323363   \n",
       "3                                   0.385881   \n",
       "4                                   0.359438   \n",
       "..                                       ...   \n",
       "146                                 0.358736   \n",
       "147                                 0.366147   \n",
       "148                                 0.350901   \n",
       "149                                 0.304067   \n",
       "150                                 0.362536   \n",
       "\n",
       "     MDCP - Married in a de facto marriage  MDCP - Not married  \\\n",
       "0                                 0.073786            0.364082   \n",
       "1                                 0.060754            0.305368   \n",
       "2                                 0.081842            0.301819   \n",
       "3                                 0.044820            0.335227   \n",
       "4                                 0.078180            0.273848   \n",
       "..                                     ...                 ...   \n",
       "146                               0.039473            0.313827   \n",
       "147                               0.067582            0.294500   \n",
       "148                               0.080181            0.284154   \n",
       "149                               0.093583            0.361030   \n",
       "150                               0.081549            0.253436   \n",
       "\n",
       "     MDCP - Not applicable  PreferencePercent  PartyNm         Strength  \\\n",
       "0                 0.259811              58.18      ALP  Fairly Safe ALP   \n",
       "1                 0.239929              60.13      LNP         Safe LNP   \n",
       "2                 0.292976              60.98      ALP         Safe ALP   \n",
       "3                 0.234073              56.26      LNP  Fairly Safe LNP   \n",
       "4                 0.288534              68.94      LNP         Safe LNP   \n",
       "..                     ...                ...      ...              ...   \n",
       "146               0.287964              55.47      ALP         Marginal   \n",
       "147               0.271771              60.91      ALP         Safe ALP   \n",
       "148               0.284764              63.15      LNP         Safe LNP   \n",
       "149               0.241320              58.17      ALP  Fairly Safe ALP   \n",
       "150               0.302479              64.58      LNP         Safe LNP   \n",
       "\n",
       "     is_ALP  is_LNP  is_Other  \n",
       "0         1       0         0  \n",
       "1         0       1         0  \n",
       "2         1       0         0  \n",
       "3         0       1         0  \n",
       "4         0       1         0  \n",
       "..      ...     ...       ...  \n",
       "146       1       0         0  \n",
       "147       1       0         0  \n",
       "148       0       1         0  \n",
       "149       1       0         0  \n",
       "150       0       1         0  \n",
       "\n",
       "[151 rows x 146 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5eefcc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>LGA</th>\n",
       "      <th>AGE5P - 0-4 years</th>\n",
       "      <th>AGE5P - 5-9 years</th>\n",
       "      <th>AGE5P - 10-14 years</th>\n",
       "      <th>AGE5P - 15-19 years</th>\n",
       "      <th>AGE5P - 20-24 years</th>\n",
       "      <th>AGE5P - 25-29 years</th>\n",
       "      <th>AGE5P - 30-34 years</th>\n",
       "      <th>AGE5P - 35-39 years</th>\n",
       "      <th>...</th>\n",
       "      <th>Strength</th>\n",
       "      <th>is_ALP</th>\n",
       "      <th>is_LNP</th>\n",
       "      <th>is_Other</th>\n",
       "      <th>is_Marginal</th>\n",
       "      <th>is_Fairly_Safe_ALP</th>\n",
       "      <th>is_Fairly_Safe_LNP</th>\n",
       "      <th>is_Safe_ALP</th>\n",
       "      <th>is_Safe_LNP</th>\n",
       "      <th>is_Strength_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>0.050723</td>\n",
       "      <td>0.045462</td>\n",
       "      <td>0.057361</td>\n",
       "      <td>0.096777</td>\n",
       "      <td>0.092626</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>...</td>\n",
       "      <td>Fairly Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Aston</td>\n",
       "      <td>0.058553</td>\n",
       "      <td>0.058845</td>\n",
       "      <td>0.058346</td>\n",
       "      <td>0.064261</td>\n",
       "      <td>0.066246</td>\n",
       "      <td>0.064391</td>\n",
       "      <td>0.067479</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>...</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>0.062608</td>\n",
       "      <td>0.066055</td>\n",
       "      <td>0.061419</td>\n",
       "      <td>0.062910</td>\n",
       "      <td>0.065860</td>\n",
       "      <td>0.060216</td>\n",
       "      <td>0.059006</td>\n",
       "      <td>0.059537</td>\n",
       "      <td>...</td>\n",
       "      <td>Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.059368</td>\n",
       "      <td>0.055980</td>\n",
       "      <td>0.060472</td>\n",
       "      <td>0.068598</td>\n",
       "      <td>0.074162</td>\n",
       "      <td>0.073559</td>\n",
       "      <td>0.066018</td>\n",
       "      <td>...</td>\n",
       "      <td>Fairly Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Barker</td>\n",
       "      <td>0.055697</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>0.062771</td>\n",
       "      <td>0.059757</td>\n",
       "      <td>0.050169</td>\n",
       "      <td>0.051558</td>\n",
       "      <td>0.054241</td>\n",
       "      <td>0.054597</td>\n",
       "      <td>...</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>164</td>\n",
       "      <td>Werriwa</td>\n",
       "      <td>0.072655</td>\n",
       "      <td>0.078559</td>\n",
       "      <td>0.077535</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.072380</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>0.071789</td>\n",
       "      <td>0.070232</td>\n",
       "      <td>...</td>\n",
       "      <td>Marginal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>165</td>\n",
       "      <td>Whitlam</td>\n",
       "      <td>0.060113</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>0.064873</td>\n",
       "      <td>0.066358</td>\n",
       "      <td>0.055673</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>...</td>\n",
       "      <td>Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>166</td>\n",
       "      <td>Wide Bay</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.060062</td>\n",
       "      <td>0.064301</td>\n",
       "      <td>0.056468</td>\n",
       "      <td>0.042020</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.049902</td>\n",
       "      <td>...</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>167</td>\n",
       "      <td>Wills</td>\n",
       "      <td>0.062568</td>\n",
       "      <td>0.052921</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.043616</td>\n",
       "      <td>0.080062</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>0.108720</td>\n",
       "      <td>0.087122</td>\n",
       "      <td>...</td>\n",
       "      <td>Fairly Safe ALP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>168</td>\n",
       "      <td>Wright</td>\n",
       "      <td>0.060883</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>0.075367</td>\n",
       "      <td>0.070010</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.061605</td>\n",
       "      <td>...</td>\n",
       "      <td>Safe LNP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       LGA  AGE5P - 0-4 years  AGE5P - 5-9 years  \\\n",
       "0        0  Adelaide           0.053945           0.050723   \n",
       "1        1     Aston           0.058553           0.058845   \n",
       "2        2  Ballarat           0.062608           0.066055   \n",
       "3        3     Banks           0.060613           0.059368   \n",
       "4        4    Barker           0.055697           0.063267   \n",
       "..     ...       ...                ...                ...   \n",
       "146    164   Werriwa           0.072655           0.078559   \n",
       "147    165   Whitlam           0.060113           0.065839   \n",
       "148    166  Wide Bay           0.048858           0.060062   \n",
       "149    167     Wills           0.062568           0.052921   \n",
       "150    168    Wright           0.060883           0.074120   \n",
       "\n",
       "     AGE5P - 10-14 years  AGE5P - 15-19 years  AGE5P - 20-24 years  \\\n",
       "0               0.045462             0.057361             0.096777   \n",
       "1               0.058346             0.064261             0.066246   \n",
       "2               0.061419             0.062910             0.065860   \n",
       "3               0.055980             0.060472             0.068598   \n",
       "4               0.062771             0.059757             0.050169   \n",
       "..                   ...                  ...                  ...   \n",
       "146             0.077535             0.077236             0.072380   \n",
       "147             0.064873             0.066358             0.055673   \n",
       "148             0.064301             0.056468             0.042020   \n",
       "149             0.043203             0.043616             0.080062   \n",
       "150             0.075367             0.070010             0.055936   \n",
       "\n",
       "     AGE5P - 25-29 years  AGE5P - 30-34 years  AGE5P - 35-39 years  ...  \\\n",
       "0               0.092626             0.088211             0.070221  ...   \n",
       "1               0.064391             0.067479             0.066700  ...   \n",
       "2               0.060216             0.059006             0.059537  ...   \n",
       "3               0.074162             0.073559             0.066018  ...   \n",
       "4               0.051558             0.054241             0.054597  ...   \n",
       "..                   ...                  ...                  ...  ...   \n",
       "146             0.065587             0.071789             0.070232  ...   \n",
       "147             0.052804             0.056448             0.056678  ...   \n",
       "148             0.040191             0.044199             0.049902  ...   \n",
       "149             0.112030             0.108720             0.087122  ...   \n",
       "150             0.051039             0.056818             0.061605  ...   \n",
       "\n",
       "            Strength  is_ALP  is_LNP  is_Other  is_Marginal  \\\n",
       "0    Fairly Safe ALP       1       0         0            0   \n",
       "1           Safe LNP       0       1         0            0   \n",
       "2           Safe ALP       1       0         0            0   \n",
       "3    Fairly Safe LNP       0       1         0            0   \n",
       "4           Safe LNP       0       1         0            0   \n",
       "..               ...     ...     ...       ...          ...   \n",
       "146         Marginal       1       0         0            1   \n",
       "147         Safe ALP       1       0         0            0   \n",
       "148         Safe LNP       0       1         0            0   \n",
       "149  Fairly Safe ALP       1       0         0            0   \n",
       "150         Safe LNP       0       1         0            0   \n",
       "\n",
       "     is_Fairly_Safe_ALP  is_Fairly_Safe_LNP  is_Safe_ALP  is_Safe_LNP  \\\n",
       "0                     1                   0            0            0   \n",
       "1                     0                   0            0            1   \n",
       "2                     0                   0            1            0   \n",
       "3                     0                   1            0            0   \n",
       "4                     0                   0            0            1   \n",
       "..                  ...                 ...          ...          ...   \n",
       "146                   0                   0            0            0   \n",
       "147                   0                   0            1            0   \n",
       "148                   0                   0            0            1   \n",
       "149                   1                   0            0            0   \n",
       "150                   0                   0            0            1   \n",
       "\n",
       "     is_Strength_Other  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "..                 ...  \n",
       "146                  0  \n",
       "147                  0  \n",
       "148                  0  \n",
       "149                  0  \n",
       "150                  0  \n",
       "\n",
       "[151 rows x 152 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_master['is_Marginal'] = np.where(adjusted_master['Strength'] == 'Marginal', 1, 0)\n",
    "adjusted_master['is_Fairly_Safe_ALP'] = np.where(adjusted_master['Strength'] == 'Fairly Safe ALP', 1, 0)\n",
    "adjusted_master['is_Fairly_Safe_LNP'] = np.where(adjusted_master['Strength'] == 'Fairly Safe LNP', 1, 0)\n",
    "adjusted_master['is_Safe_ALP'] = np.where(adjusted_master['Strength'] == 'Safe ALP', 1, 0)\n",
    "adjusted_master['is_Safe_LNP'] = np.where(adjusted_master['Strength'] == 'Safe LNP', 1, 0)\n",
    "adjusted_master['is_Strength_Other'] = np.where(adjusted_master['Strength'] == 'Other', 1, 0)\n",
    "adjusted_master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2c460",
   "metadata": {},
   "source": [
    "# 4.1 Marginal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3e7776c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 152)\n",
      "(31, 152)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(adjusted_master, test_size=0.2, random_state=142)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8ba7da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_train = train['is_Marginal']\n",
    "X_test = test.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_test = test['is_Marginal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dcc51132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set:  0.7\n",
      "Accuracy score on testing set:  0.6129032258064516\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "print(\"Accuracy score on training set: \", accuracy_score(y_train, y_hat_train))\n",
    "print(\"Accuracy score on testing set: \", accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aa52b4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using 1 features:  0.6129032258064516\n",
      "Accuracy on test set using 2 features:  0.6129032258064516\n",
      "Accuracy on test set using 3 features:  0.6129032258064516\n",
      "Accuracy on test set using 4 features:  0.6129032258064516\n",
      "Accuracy on test set using 5 features:  0.6129032258064516\n",
      "Accuracy on test set using 6 features:  0.6129032258064516\n",
      "Accuracy on test set using 7 features:  0.6129032258064516\n",
      "Accuracy on test set using 8 features:  0.6129032258064516\n",
      "Accuracy on test set using 9 features:  0.6129032258064516\n",
      "Accuracy on test set using 10 features:  0.6129032258064516\n",
      "Accuracy on test set using 11 features:  0.6129032258064516\n",
      "Accuracy on test set using 12 features:  0.6129032258064516\n",
      "Accuracy on test set using 13 features:  0.6129032258064516\n",
      "Accuracy on test set using 14 features:  0.6129032258064516\n",
      "Accuracy on test set using 15 features:  0.6129032258064516\n",
      "Accuracy on test set using 16 features:  0.6129032258064516\n",
      "Accuracy on test set using 17 features:  0.6129032258064516\n",
      "Accuracy on test set using 18 features:  0.6129032258064516\n",
      "Accuracy on test set using 19 features:  0.6129032258064516\n",
      "Accuracy on test set using 20 features:  0.6129032258064516\n",
      "Accuracy on test set using 21 features:  0.6129032258064516\n",
      "Accuracy on test set using 22 features:  0.6129032258064516\n",
      "Accuracy on test set using 23 features:  0.6129032258064516\n",
      "Accuracy on test set using 24 features:  0.6129032258064516\n",
      "Accuracy on test set using 25 features:  0.6129032258064516\n",
      "Accuracy on test set using 26 features:  0.6129032258064516\n",
      "Accuracy on test set using 27 features:  0.6129032258064516\n",
      "Accuracy on test set using 28 features:  0.6129032258064516\n",
      "Accuracy on test set using 29 features:  0.6129032258064516\n",
      "Accuracy on test set using 30 features:  0.6129032258064516\n",
      "Accuracy on test set using 31 features:  0.6129032258064516\n",
      "Accuracy on test set using 32 features:  0.6129032258064516\n",
      "Accuracy on test set using 33 features:  0.6129032258064516\n",
      "Accuracy on test set using 34 features:  0.6129032258064516\n",
      "Accuracy on test set using 35 features:  0.6129032258064516\n",
      "Accuracy on test set using 36 features:  0.6129032258064516\n",
      "Accuracy on test set using 37 features:  0.6129032258064516\n",
      "Accuracy on test set using 38 features:  0.6129032258064516\n",
      "Accuracy on test set using 39 features:  0.6129032258064516\n",
      "Accuracy on test set using 40 features:  0.6129032258064516\n",
      "Accuracy on test set using 41 features:  0.6129032258064516\n",
      "Accuracy on test set using 42 features:  0.6129032258064516\n",
      "Accuracy on test set using 43 features:  0.6129032258064516\n",
      "Accuracy on test set using 44 features:  0.6129032258064516\n",
      "Accuracy on test set using 45 features:  0.6129032258064516\n",
      "Accuracy on test set using 46 features:  0.6129032258064516\n",
      "Accuracy on test set using 47 features:  0.6129032258064516\n",
      "Accuracy on test set using 48 features:  0.6129032258064516\n",
      "Accuracy on test set using 49 features:  0.6129032258064516\n",
      "Accuracy on test set using 50 features:  0.6129032258064516\n",
      "Accuracy on test set using 51 features:  0.6129032258064516\n",
      "Accuracy on test set using 52 features:  0.6129032258064516\n",
      "Accuracy on test set using 53 features:  0.6129032258064516\n",
      "Accuracy on test set using 54 features:  0.6129032258064516\n",
      "Accuracy on test set using 55 features:  0.6129032258064516\n",
      "Accuracy on test set using 56 features:  0.6129032258064516\n",
      "Accuracy on test set using 57 features:  0.6129032258064516\n",
      "Accuracy on test set using 58 features:  0.6129032258064516\n",
      "Accuracy on test set using 59 features:  0.6129032258064516\n",
      "Accuracy on test set using 60 features:  0.6129032258064516\n",
      "Accuracy on test set using 61 features:  0.6129032258064516\n",
      "Accuracy on test set using 62 features:  0.6129032258064516\n",
      "Accuracy on test set using 63 features:  0.6129032258064516\n",
      "Accuracy on test set using 64 features:  0.6129032258064516\n",
      "Accuracy on test set using 65 features:  0.6129032258064516\n",
      "Accuracy on test set using 66 features:  0.6129032258064516\n",
      "Accuracy on test set using 67 features:  0.6129032258064516\n",
      "Accuracy on test set using 68 features:  0.6129032258064516\n",
      "Accuracy on test set using 69 features:  0.6129032258064516\n",
      "Accuracy on test set using 70 features:  0.6129032258064516\n",
      "Accuracy on test set using 71 features:  0.6129032258064516\n",
      "Accuracy on test set using 72 features:  0.6129032258064516\n",
      "Accuracy on test set using 73 features:  0.6129032258064516\n",
      "Accuracy on test set using 74 features:  0.6129032258064516\n",
      "Accuracy on test set using 75 features:  0.6129032258064516\n",
      "Accuracy on test set using 76 features:  0.6129032258064516\n",
      "Accuracy on test set using 77 features:  0.6129032258064516\n",
      "Accuracy on test set using 78 features:  0.6129032258064516\n",
      "Accuracy on test set using 79 features:  0.6129032258064516\n",
      "Accuracy on test set using 80 features:  0.6129032258064516\n",
      "Accuracy on test set using 81 features:  0.6129032258064516\n",
      "Accuracy on test set using 82 features:  0.6129032258064516\n",
      "Accuracy on test set using 83 features:  0.6129032258064516\n",
      "Accuracy on test set using 84 features:  0.6129032258064516\n",
      "Accuracy on test set using 85 features:  0.6129032258064516\n",
      "Accuracy on test set using 86 features:  0.6129032258064516\n",
      "Accuracy on test set using 87 features:  0.6129032258064516\n",
      "Accuracy on test set using 88 features:  0.6129032258064516\n",
      "Accuracy on test set using 89 features:  0.6129032258064516\n",
      "Accuracy on test set using 90 features:  0.6129032258064516\n",
      "Accuracy on test set using 91 features:  0.6129032258064516\n",
      "Accuracy on test set using 92 features:  0.6129032258064516\n",
      "Accuracy on test set using 93 features:  0.6129032258064516\n",
      "Accuracy on test set using 94 features:  0.6129032258064516\n",
      "Accuracy on test set using 95 features:  0.6129032258064516\n",
      "Accuracy on test set using 96 features:  0.6129032258064516\n",
      "Accuracy on test set using 97 features:  0.6129032258064516\n",
      "Accuracy on test set using 98 features:  0.6129032258064516\n",
      "Accuracy on test set using 99 features:  0.6129032258064516\n",
      "Accuracy on test set using 100 features:  0.6129032258064516\n",
      "Accuracy on test set using 101 features:  0.6129032258064516\n",
      "Accuracy on test set using 102 features:  0.6129032258064516\n",
      "Accuracy on test set using 103 features:  0.6129032258064516\n",
      "Accuracy on test set using 104 features:  0.6129032258064516\n",
      "Accuracy on test set using 105 features:  0.6129032258064516\n",
      "Accuracy on test set using 106 features:  0.6129032258064516\n",
      "Accuracy on test set using 107 features:  0.6129032258064516\n",
      "Accuracy on test set using 108 features:  0.6129032258064516\n",
      "Accuracy on test set using 109 features:  0.6129032258064516\n",
      "Accuracy on test set using 110 features:  0.6129032258064516\n",
      "Accuracy on test set using 111 features:  0.6129032258064516\n",
      "Accuracy on test set using 112 features:  0.6129032258064516\n",
      "Accuracy on test set using 113 features:  0.6129032258064516\n",
      "Accuracy on test set using 114 features:  0.6129032258064516\n",
      "Accuracy on test set using 115 features:  0.6129032258064516\n",
      "Accuracy on test set using 116 features:  0.6129032258064516\n",
      "Accuracy on test set using 117 features:  0.6129032258064516\n",
      "Accuracy on test set using 118 features:  0.6129032258064516\n",
      "Accuracy on test set using 119 features:  0.6129032258064516\n",
      "Accuracy on test set using 120 features:  0.6129032258064516\n",
      "Accuracy on test set using 121 features:  0.6129032258064516\n",
      "Accuracy on test set using 122 features:  0.6129032258064516\n",
      "Accuracy on test set using 123 features:  0.6129032258064516\n",
      "Accuracy on test set using 124 features:  0.6129032258064516\n",
      "Accuracy on test set using 125 features:  0.6129032258064516\n",
      "Accuracy on test set using 126 features:  0.6129032258064516\n",
      "Accuracy on test set using 127 features:  0.6129032258064516\n",
      "Accuracy on test set using 128 features:  0.6129032258064516\n",
      "Accuracy on test set using 129 features:  0.6129032258064516\n",
      "Accuracy on test set using 130 features:  0.6129032258064516\n",
      "Accuracy on test set using 131 features:  0.6129032258064516\n",
      "Accuracy on test set using 132 features:  0.6129032258064516\n",
      "Accuracy on test set using 133 features:  0.6129032258064516\n",
      "Accuracy on test set using 134 features:  0.6129032258064516\n",
      "Accuracy on test set using 135 features:  0.6129032258064516\n",
      "Accuracy on test set using 136 features:  0.6129032258064516\n",
      "Accuracy on test set using 137 features:  0.6129032258064516\n",
      "Accuracy on test set using 138 features:  0.6129032258064516\n",
      "Accuracy on test set using 139 features:  0.6129032258064516\n",
      "1 columns provide the highest accuracy at: 61.29 %\n"
     ]
    }
   ],
   "source": [
    "acc_scores = []\n",
    "for i in range(1,140):\n",
    "    clf = LogisticRegression()\n",
    "    rfe = RFE(estimator=clf, n_features_to_select=i)\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfe.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy on test set using\", i, \"features: \", acc_score)\n",
    "    \n",
    "    acc_scores.append(acc_score)\n",
    "\n",
    "best = acc_scores[0]\n",
    "best_number_of_columns = 0;\n",
    "for i in range(len(acc_scores)):\n",
    "    if acc_scores[i] > best:\n",
    "        best = acc_scores[i]\n",
    "        best_number_of_columns = i\n",
    "        \n",
    "print(best_number_of_columns + 1, 'columns provide the highest accuracy at: %.2f' % (best*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1b7731e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test set:  0.6129032258064516\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "rfe = RFE(estimator=lr_model, n_features_to_select=10, step=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "y_test_hat = rfe.predict(X_test)\n",
    "print(\"accuracy score on test set: \", accuracy_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "74c12667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected False, Rank: 94.000\n",
      "Column: 1, Selected False, Rank: 67.000\n",
      "Column: 2, Selected False, Rank: 81.000\n",
      "Column: 3, Selected False, Rank: 95.000\n",
      "Column: 4, Selected False, Rank: 45.000\n",
      "Column: 5, Selected False, Rank: 16.000\n",
      "Column: 6, Selected False, Rank: 13.000\n",
      "Column: 7, Selected False, Rank: 71.000\n",
      "Column: 8, Selected False, Rank: 87.000\n",
      "Column: 9, Selected False, Rank: 86.000\n",
      "Column: 10, Selected False, Rank: 77.000\n",
      "Column: 11, Selected False, Rank: 68.000\n",
      "Column: 12, Selected False, Rank: 84.000\n",
      "Column: 13, Selected False, Rank: 62.000\n",
      "Column: 14, Selected False, Rank: 50.000\n",
      "Column: 15, Selected False, Rank: 41.000\n",
      "Column: 16, Selected False, Rank: 46.000\n",
      "Column: 17, Selected False, Rank: 70.000\n",
      "Column: 18, Selected False, Rank: 90.000\n",
      "Column: 19, Selected False, Rank: 123.000\n",
      "Column: 20, Selected False, Rank: 128.000\n",
      "Column: 21, Selected False, Rank: 44.000\n",
      "Column: 22, Selected False, Rank: 42.000\n",
      "Column: 23, Selected False, Rank: 11.000\n",
      "Column: 24, Selected False, Rank: 17.000\n",
      "Column: 25, Selected False, Rank: 19.000\n",
      "Column: 26, Selected False, Rank: 22.000\n",
      "Column: 27, Selected False, Rank: 119.000\n",
      "Column: 28, Selected False, Rank: 2.000\n",
      "Column: 29, Selected False, Rank: 37.000\n",
      "Column: 30, Selected False, Rank: 35.000\n",
      "Column: 31, Selected False, Rank: 52.000\n",
      "Column: 32, Selected False, Rank: 21.000\n",
      "Column: 33, Selected False, Rank: 23.000\n",
      "Column: 34, Selected False, Rank: 31.000\n",
      "Column: 35, Selected False, Rank: 61.000\n",
      "Column: 36, Selected False, Rank: 108.000\n",
      "Column: 37, Selected False, Rank: 82.000\n",
      "Column: 38, Selected False, Rank: 54.000\n",
      "Column: 39, Selected False, Rank: 30.000\n",
      "Column: 40, Selected False, Rank: 9.000\n",
      "Column: 41, Selected False, Rank: 40.000\n",
      "Column: 42, Selected False, Rank: 99.000\n",
      "Column: 43, Selected False, Rank: 125.000\n",
      "Column: 44, Selected False, Rank: 43.000\n",
      "Column: 45, Selected False, Rank: 93.000\n",
      "Column: 46, Selected False, Rank: 47.000\n",
      "Column: 47, Selected False, Rank: 56.000\n",
      "Column: 48, Selected False, Rank: 36.000\n",
      "Column: 49, Selected False, Rank: 39.000\n",
      "Column: 50, Selected False, Rank: 60.000\n",
      "Column: 51, Selected False, Rank: 6.000\n",
      "Column: 52, Selected False, Rank: 72.000\n",
      "Column: 53, Selected False, Rank: 8.000\n",
      "Column: 54, Selected False, Rank: 78.000\n",
      "Column: 55, Selected False, Rank: 104.000\n",
      "Column: 56, Selected False, Rank: 28.000\n",
      "Column: 57, Selected False, Rank: 25.000\n",
      "Column: 58, Selected False, Rank: 73.000\n",
      "Column: 59, Selected False, Rank: 58.000\n",
      "Column: 60, Selected False, Rank: 83.000\n",
      "Column: 61, Selected False, Rank: 38.000\n",
      "Column: 62, Selected True, Rank: 1.000\n",
      "Column: 63, Selected False, Rank: 110.000\n",
      "Column: 64, Selected False, Rank: 65.000\n",
      "Column: 65, Selected False, Rank: 66.000\n",
      "Column: 66, Selected False, Rank: 12.000\n",
      "Column: 67, Selected False, Rank: 76.000\n",
      "Column: 68, Selected False, Rank: 80.000\n",
      "Column: 69, Selected False, Rank: 107.000\n",
      "Column: 70, Selected False, Rank: 89.000\n",
      "Column: 71, Selected False, Rank: 7.000\n",
      "Column: 72, Selected False, Rank: 18.000\n",
      "Column: 73, Selected False, Rank: 85.000\n",
      "Column: 74, Selected False, Rank: 113.000\n",
      "Column: 75, Selected False, Rank: 92.000\n",
      "Column: 76, Selected False, Rank: 118.000\n",
      "Column: 77, Selected True, Rank: 1.000\n",
      "Column: 78, Selected False, Rank: 27.000\n",
      "Column: 79, Selected False, Rank: 55.000\n",
      "Column: 80, Selected False, Rank: 79.000\n",
      "Column: 81, Selected False, Rank: 5.000\n",
      "Column: 82, Selected False, Rank: 101.000\n",
      "Column: 83, Selected False, Rank: 14.000\n",
      "Column: 84, Selected False, Rank: 111.000\n",
      "Column: 85, Selected True, Rank: 1.000\n",
      "Column: 86, Selected True, Rank: 1.000\n",
      "Column: 87, Selected False, Rank: 29.000\n",
      "Column: 88, Selected False, Rank: 34.000\n",
      "Column: 89, Selected False, Rank: 102.000\n",
      "Column: 90, Selected True, Rank: 1.000\n",
      "Column: 91, Selected False, Rank: 53.000\n",
      "Column: 92, Selected True, Rank: 1.000\n",
      "Column: 93, Selected False, Rank: 3.000\n",
      "Column: 94, Selected False, Rank: 117.000\n",
      "Column: 95, Selected False, Rank: 120.000\n",
      "Column: 96, Selected False, Rank: 91.000\n",
      "Column: 97, Selected False, Rank: 24.000\n",
      "Column: 98, Selected False, Rank: 88.000\n",
      "Column: 99, Selected False, Rank: 121.000\n",
      "Column: 100, Selected False, Rank: 98.000\n",
      "Column: 101, Selected False, Rank: 103.000\n",
      "Column: 102, Selected False, Rank: 124.000\n",
      "Column: 103, Selected False, Rank: 57.000\n",
      "Column: 104, Selected False, Rank: 105.000\n",
      "Column: 105, Selected False, Rank: 129.000\n",
      "Column: 106, Selected False, Rank: 20.000\n",
      "Column: 107, Selected False, Rank: 100.000\n",
      "Column: 108, Selected False, Rank: 127.000\n",
      "Column: 109, Selected False, Rank: 74.000\n",
      "Column: 110, Selected False, Rank: 122.000\n",
      "Column: 111, Selected False, Rank: 116.000\n",
      "Column: 112, Selected False, Rank: 126.000\n",
      "Column: 113, Selected False, Rank: 106.000\n",
      "Column: 114, Selected False, Rank: 112.000\n",
      "Column: 115, Selected False, Rank: 97.000\n",
      "Column: 116, Selected False, Rank: 96.000\n",
      "Column: 117, Selected False, Rank: 51.000\n",
      "Column: 118, Selected False, Rank: 26.000\n",
      "Column: 119, Selected False, Rank: 64.000\n",
      "Column: 120, Selected False, Rank: 69.000\n",
      "Column: 121, Selected False, Rank: 32.000\n",
      "Column: 122, Selected False, Rank: 109.000\n",
      "Column: 123, Selected False, Rank: 59.000\n",
      "Column: 124, Selected True, Rank: 1.000\n",
      "Column: 125, Selected False, Rank: 15.000\n",
      "Column: 126, Selected True, Rank: 1.000\n",
      "Column: 127, Selected False, Rank: 49.000\n",
      "Column: 128, Selected False, Rank: 48.000\n",
      "Column: 129, Selected True, Rank: 1.000\n",
      "Column: 130, Selected False, Rank: 75.000\n",
      "Column: 131, Selected False, Rank: 10.000\n",
      "Column: 132, Selected False, Rank: 114.000\n",
      "Column: 133, Selected False, Rank: 115.000\n",
      "Column: 134, Selected True, Rank: 1.000\n",
      "Column: 135, Selected False, Rank: 4.000\n",
      "Column: 136, Selected False, Rank: 33.000\n",
      "Column: 137, Selected False, Rank: 63.000\n",
      "[62, 77, 85, 86, 90, 92, 124, 126, 129, 134]\n"
     ]
    }
   ],
   "source": [
    "relevant_cols = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    if rfe.support_[i] == True:\n",
    "        relevant_cols.append(i)\n",
    "        \n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d5da457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [62, 77, 85, 86, 90, 92, 124, 126, 129, 134]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent\n",
       "3         0          0         56.26\n",
       "70        0          0         58.36\n",
       "92        1          0         53.28\n",
       "124       0          0         57.52\n",
       "56        0          0         62.21\n",
       "134       1          0         54.45\n",
       "107       0          0         71.83\n",
       "78        1          0         52.98\n",
       "128       1          0         53.18\n",
       "65        1          0         52.80\n",
       "42        1          0         51.50\n",
       "123       1          0         55.04\n",
       "111       0          0         65.36\n",
       "0         0          0         58.18\n",
       "136       0          0         64.13\n",
       "98        1          0         50.19\n",
       "90        1          0         55.04\n",
       "114       0          0         64.36\n",
       "108       0          0         57.53\n",
       "131       1          0         54.24\n",
       "101       0          0         72.49\n",
       "68        0          0         70.48\n",
       "95        0          0         58.40\n",
       "119       0          0         56.39\n",
       "64        0          0         66.30\n",
       "77        0          0         62.99\n",
       "71        1          0         53.88\n",
       "150       0          0         64.58\n",
       "7         0          0         57.52\n",
       "103       1          0         55.14\n",
       "53        0          0         58.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(test['is_Marginal'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(X_test))\n",
    "result.insert(2, \"Pref Percent\", test['PreferencePercent'].values)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24d336cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDP - Not applicable</th>\n",
       "      <th>LFHRP - Not in the labour force</th>\n",
       "      <th>GNGP - Not applicable</th>\n",
       "      <th>MSTP - Never married</th>\n",
       "      <th>MSTP - Married</th>\n",
       "      <th>RLHP - Husband, Wife or Partner in a registered marriage</th>\n",
       "      <th>RELP - Christianity</th>\n",
       "      <th>RELP - Islam</th>\n",
       "      <th>RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.533226</td>\n",
       "      <td>0.289307</td>\n",
       "      <td>0.533239</td>\n",
       "      <td>0.260373</td>\n",
       "      <td>0.429529</td>\n",
       "      <td>0.390934</td>\n",
       "      <td>0.555461</td>\n",
       "      <td>0.048503</td>\n",
       "      <td>0.242016</td>\n",
       "      <td>0.385881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.542880</td>\n",
       "      <td>0.227649</td>\n",
       "      <td>0.542897</td>\n",
       "      <td>0.331367</td>\n",
       "      <td>0.325917</td>\n",
       "      <td>0.284470</td>\n",
       "      <td>0.584734</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.290817</td>\n",
       "      <td>0.280871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.589752</td>\n",
       "      <td>0.291475</td>\n",
       "      <td>0.589748</td>\n",
       "      <td>0.257813</td>\n",
       "      <td>0.381666</td>\n",
       "      <td>0.344130</td>\n",
       "      <td>0.561709</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.318683</td>\n",
       "      <td>0.339021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.535209</td>\n",
       "      <td>0.207218</td>\n",
       "      <td>0.535174</td>\n",
       "      <td>0.265419</td>\n",
       "      <td>0.391204</td>\n",
       "      <td>0.343378</td>\n",
       "      <td>0.502974</td>\n",
       "      <td>0.011615</td>\n",
       "      <td>0.342444</td>\n",
       "      <td>0.340102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.559708</td>\n",
       "      <td>0.308677</td>\n",
       "      <td>0.559928</td>\n",
       "      <td>0.255628</td>\n",
       "      <td>0.398361</td>\n",
       "      <td>0.353628</td>\n",
       "      <td>0.492437</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.400382</td>\n",
       "      <td>0.351234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.570357</td>\n",
       "      <td>0.318004</td>\n",
       "      <td>0.570325</td>\n",
       "      <td>0.250313</td>\n",
       "      <td>0.402176</td>\n",
       "      <td>0.370344</td>\n",
       "      <td>0.619925</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.293455</td>\n",
       "      <td>0.366205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.487229</td>\n",
       "      <td>0.260181</td>\n",
       "      <td>0.487512</td>\n",
       "      <td>0.591751</td>\n",
       "      <td>0.232039</td>\n",
       "      <td>0.183481</td>\n",
       "      <td>0.260394</td>\n",
       "      <td>0.035241</td>\n",
       "      <td>0.472482</td>\n",
       "      <td>0.181893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.583925</td>\n",
       "      <td>0.305707</td>\n",
       "      <td>0.583959</td>\n",
       "      <td>0.260744</td>\n",
       "      <td>0.387295</td>\n",
       "      <td>0.344979</td>\n",
       "      <td>0.632586</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.260657</td>\n",
       "      <td>0.340778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.504883</td>\n",
       "      <td>0.267467</td>\n",
       "      <td>0.504869</td>\n",
       "      <td>0.319403</td>\n",
       "      <td>0.422374</td>\n",
       "      <td>0.363494</td>\n",
       "      <td>0.457105</td>\n",
       "      <td>0.027582</td>\n",
       "      <td>0.285639</td>\n",
       "      <td>0.359918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.517665</td>\n",
       "      <td>0.225184</td>\n",
       "      <td>0.517650</td>\n",
       "      <td>0.233375</td>\n",
       "      <td>0.438427</td>\n",
       "      <td>0.404093</td>\n",
       "      <td>0.527592</td>\n",
       "      <td>0.048839</td>\n",
       "      <td>0.156307</td>\n",
       "      <td>0.400018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.578397</td>\n",
       "      <td>0.300408</td>\n",
       "      <td>0.578400</td>\n",
       "      <td>0.261646</td>\n",
       "      <td>0.365818</td>\n",
       "      <td>0.333543</td>\n",
       "      <td>0.624938</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.271944</td>\n",
       "      <td>0.328731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.585717</td>\n",
       "      <td>0.300010</td>\n",
       "      <td>0.585721</td>\n",
       "      <td>0.254980</td>\n",
       "      <td>0.385849</td>\n",
       "      <td>0.347542</td>\n",
       "      <td>0.641047</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.257929</td>\n",
       "      <td>0.343199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.529888</td>\n",
       "      <td>0.268097</td>\n",
       "      <td>0.529809</td>\n",
       "      <td>0.333886</td>\n",
       "      <td>0.345265</td>\n",
       "      <td>0.296613</td>\n",
       "      <td>0.508566</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>0.321620</td>\n",
       "      <td>0.292694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.541928</td>\n",
       "      <td>0.298165</td>\n",
       "      <td>0.541961</td>\n",
       "      <td>0.357867</td>\n",
       "      <td>0.351498</td>\n",
       "      <td>0.304915</td>\n",
       "      <td>0.430949</td>\n",
       "      <td>0.042542</td>\n",
       "      <td>0.339837</td>\n",
       "      <td>0.302321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.618929</td>\n",
       "      <td>0.301413</td>\n",
       "      <td>0.616273</td>\n",
       "      <td>0.301677</td>\n",
       "      <td>0.325648</td>\n",
       "      <td>0.297934</td>\n",
       "      <td>0.456894</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.375639</td>\n",
       "      <td>0.291657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.518041</td>\n",
       "      <td>0.262153</td>\n",
       "      <td>0.518074</td>\n",
       "      <td>0.264001</td>\n",
       "      <td>0.398419</td>\n",
       "      <td>0.363866</td>\n",
       "      <td>0.584620</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.310298</td>\n",
       "      <td>0.359523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.523120</td>\n",
       "      <td>0.235585</td>\n",
       "      <td>0.523170</td>\n",
       "      <td>0.286019</td>\n",
       "      <td>0.366740</td>\n",
       "      <td>0.337503</td>\n",
       "      <td>0.632425</td>\n",
       "      <td>0.019091</td>\n",
       "      <td>0.222032</td>\n",
       "      <td>0.332070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.582145</td>\n",
       "      <td>0.292108</td>\n",
       "      <td>0.582161</td>\n",
       "      <td>0.265708</td>\n",
       "      <td>0.382280</td>\n",
       "      <td>0.329148</td>\n",
       "      <td>0.664317</td>\n",
       "      <td>0.004786</td>\n",
       "      <td>0.212013</td>\n",
       "      <td>0.325706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.529046</td>\n",
       "      <td>0.296752</td>\n",
       "      <td>0.529303</td>\n",
       "      <td>0.239032</td>\n",
       "      <td>0.478775</td>\n",
       "      <td>0.435158</td>\n",
       "      <td>0.512021</td>\n",
       "      <td>0.018006</td>\n",
       "      <td>0.335852</td>\n",
       "      <td>0.431292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.560608</td>\n",
       "      <td>0.308422</td>\n",
       "      <td>0.560608</td>\n",
       "      <td>0.255435</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.356312</td>\n",
       "      <td>0.598827</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.294278</td>\n",
       "      <td>0.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.571550</td>\n",
       "      <td>0.275261</td>\n",
       "      <td>0.571539</td>\n",
       "      <td>0.241290</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.343557</td>\n",
       "      <td>0.679945</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.200118</td>\n",
       "      <td>0.340305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.259107</td>\n",
       "      <td>0.553165</td>\n",
       "      <td>0.255309</td>\n",
       "      <td>0.397141</td>\n",
       "      <td>0.350155</td>\n",
       "      <td>0.671598</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.205241</td>\n",
       "      <td>0.347481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.544683</td>\n",
       "      <td>0.245754</td>\n",
       "      <td>0.544760</td>\n",
       "      <td>0.274782</td>\n",
       "      <td>0.379364</td>\n",
       "      <td>0.353049</td>\n",
       "      <td>0.609142</td>\n",
       "      <td>0.063042</td>\n",
       "      <td>0.184146</td>\n",
       "      <td>0.347075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.559690</td>\n",
       "      <td>0.240377</td>\n",
       "      <td>0.559459</td>\n",
       "      <td>0.276051</td>\n",
       "      <td>0.373822</td>\n",
       "      <td>0.334009</td>\n",
       "      <td>0.504791</td>\n",
       "      <td>0.026523</td>\n",
       "      <td>0.274726</td>\n",
       "      <td>0.329300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.448564</td>\n",
       "      <td>0.199592</td>\n",
       "      <td>0.448558</td>\n",
       "      <td>0.393886</td>\n",
       "      <td>0.326897</td>\n",
       "      <td>0.285071</td>\n",
       "      <td>0.398269</td>\n",
       "      <td>0.011578</td>\n",
       "      <td>0.415847</td>\n",
       "      <td>0.282001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.517483</td>\n",
       "      <td>0.239159</td>\n",
       "      <td>0.517462</td>\n",
       "      <td>0.243119</td>\n",
       "      <td>0.415495</td>\n",
       "      <td>0.378933</td>\n",
       "      <td>0.687635</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>0.213552</td>\n",
       "      <td>0.374747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.474949</td>\n",
       "      <td>0.242123</td>\n",
       "      <td>0.474799</td>\n",
       "      <td>0.369329</td>\n",
       "      <td>0.373453</td>\n",
       "      <td>0.330085</td>\n",
       "      <td>0.424266</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.365632</td>\n",
       "      <td>0.327740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.548397</td>\n",
       "      <td>0.250237</td>\n",
       "      <td>0.548425</td>\n",
       "      <td>0.248191</td>\n",
       "      <td>0.410307</td>\n",
       "      <td>0.368218</td>\n",
       "      <td>0.588464</td>\n",
       "      <td>0.003963</td>\n",
       "      <td>0.288079</td>\n",
       "      <td>0.362536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.487342</td>\n",
       "      <td>0.228712</td>\n",
       "      <td>0.484516</td>\n",
       "      <td>0.267665</td>\n",
       "      <td>0.410962</td>\n",
       "      <td>0.372793</td>\n",
       "      <td>0.513521</td>\n",
       "      <td>0.018860</td>\n",
       "      <td>0.335144</td>\n",
       "      <td>0.366788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.550468</td>\n",
       "      <td>0.297215</td>\n",
       "      <td>0.550419</td>\n",
       "      <td>0.227454</td>\n",
       "      <td>0.443481</td>\n",
       "      <td>0.391707</td>\n",
       "      <td>0.464238</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.423214</td>\n",
       "      <td>0.388684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.549980</td>\n",
       "      <td>0.237681</td>\n",
       "      <td>0.549957</td>\n",
       "      <td>0.276851</td>\n",
       "      <td>0.361511</td>\n",
       "      <td>0.328277</td>\n",
       "      <td>0.550102</td>\n",
       "      <td>0.012473</td>\n",
       "      <td>0.316483</td>\n",
       "      <td>0.323253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     INDP - Not applicable  LFHRP - Not in the labour force  \\\n",
       "3                 0.533226                         0.289307   \n",
       "70                0.542880                         0.227649   \n",
       "92                0.589752                         0.291475   \n",
       "124               0.535209                         0.207218   \n",
       "56                0.559708                         0.308677   \n",
       "134               0.570357                         0.318004   \n",
       "107               0.487229                         0.260181   \n",
       "78                0.583925                         0.305707   \n",
       "128               0.504883                         0.267467   \n",
       "65                0.517665                         0.225184   \n",
       "42                0.578397                         0.300408   \n",
       "123               0.585717                         0.300010   \n",
       "111               0.529888                         0.268097   \n",
       "0                 0.541928                         0.298165   \n",
       "136               0.618929                         0.301413   \n",
       "98                0.518041                         0.262153   \n",
       "90                0.523120                         0.235585   \n",
       "114               0.582145                         0.292108   \n",
       "108               0.529046                         0.296752   \n",
       "131               0.560608                         0.308422   \n",
       "101               0.571550                         0.275261   \n",
       "68                0.553191                         0.259107   \n",
       "95                0.544683                         0.245754   \n",
       "119               0.559690                         0.240377   \n",
       "64                0.448564                         0.199592   \n",
       "77                0.517483                         0.239159   \n",
       "71                0.474949                         0.242123   \n",
       "150               0.548397                         0.250237   \n",
       "7                 0.487342                         0.228712   \n",
       "103               0.550468                         0.297215   \n",
       "53                0.549980                         0.237681   \n",
       "\n",
       "     GNGP - Not applicable  MSTP - Never married  MSTP - Married  \\\n",
       "3                 0.533239              0.260373        0.429529   \n",
       "70                0.542897              0.331367        0.325917   \n",
       "92                0.589748              0.257813        0.381666   \n",
       "124               0.535174              0.265419        0.391204   \n",
       "56                0.559928              0.255628        0.398361   \n",
       "134               0.570325              0.250313        0.402176   \n",
       "107               0.487512              0.591751        0.232039   \n",
       "78                0.583959              0.260744        0.387295   \n",
       "128               0.504869              0.319403        0.422374   \n",
       "65                0.517650              0.233375        0.438427   \n",
       "42                0.578400              0.261646        0.365818   \n",
       "123               0.585721              0.254980        0.385849   \n",
       "111               0.529809              0.333886        0.345265   \n",
       "0                 0.541961              0.357867        0.351498   \n",
       "136               0.616273              0.301677        0.325648   \n",
       "98                0.518074              0.264001        0.398419   \n",
       "90                0.523170              0.286019        0.366740   \n",
       "114               0.582161              0.265708        0.382280   \n",
       "108               0.529303              0.239032        0.478775   \n",
       "131               0.560608              0.255435        0.391441   \n",
       "101               0.571539              0.241290        0.403993   \n",
       "68                0.553165              0.255309        0.397141   \n",
       "95                0.544760              0.274782        0.379364   \n",
       "119               0.559459              0.276051        0.373822   \n",
       "64                0.448558              0.393886        0.326897   \n",
       "77                0.517462              0.243119        0.415495   \n",
       "71                0.474799              0.369329        0.373453   \n",
       "150               0.548425              0.248191        0.410307   \n",
       "7                 0.484516              0.267665        0.410962   \n",
       "103               0.550419              0.227454        0.443481   \n",
       "53                0.549957              0.276851        0.361511   \n",
       "\n",
       "     RLHP - Husband, Wife or Partner in a registered marriage  \\\n",
       "3                                             0.390934          \n",
       "70                                            0.284470          \n",
       "92                                            0.344130          \n",
       "124                                           0.343378          \n",
       "56                                            0.353628          \n",
       "134                                           0.370344          \n",
       "107                                           0.183481          \n",
       "78                                            0.344979          \n",
       "128                                           0.363494          \n",
       "65                                            0.404093          \n",
       "42                                            0.333543          \n",
       "123                                           0.347542          \n",
       "111                                           0.296613          \n",
       "0                                             0.304915          \n",
       "136                                           0.297934          \n",
       "98                                            0.363866          \n",
       "90                                            0.337503          \n",
       "114                                           0.329148          \n",
       "108                                           0.435158          \n",
       "131                                           0.356312          \n",
       "101                                           0.343557          \n",
       "68                                            0.350155          \n",
       "95                                            0.353049          \n",
       "119                                           0.334009          \n",
       "64                                            0.285071          \n",
       "77                                            0.378933          \n",
       "71                                            0.330085          \n",
       "150                                           0.368218          \n",
       "7                                             0.372793          \n",
       "103                                           0.391707          \n",
       "53                                            0.328277          \n",
       "\n",
       "     RELP - Christianity  RELP - Islam  \\\n",
       "3               0.555461      0.048503   \n",
       "70              0.584734      0.005375   \n",
       "92              0.561709      0.001849   \n",
       "124             0.502974      0.011615   \n",
       "56              0.492437      0.002484   \n",
       "134             0.619925      0.001979   \n",
       "107             0.260394      0.035241   \n",
       "78              0.632586      0.002672   \n",
       "128             0.457105      0.027582   \n",
       "65              0.527592      0.048839   \n",
       "42              0.624938      0.003161   \n",
       "123             0.641047      0.002615   \n",
       "111             0.508566      0.008611   \n",
       "0               0.430949      0.042542   \n",
       "136             0.456894      0.024514   \n",
       "98              0.584620      0.002772   \n",
       "90              0.632425      0.019091   \n",
       "114             0.664317      0.004786   \n",
       "108             0.512021      0.018006   \n",
       "131             0.598827      0.003382   \n",
       "101             0.679945      0.001313   \n",
       "68              0.671598      0.010641   \n",
       "95              0.609142      0.063042   \n",
       "119             0.504791      0.026523   \n",
       "64              0.398269      0.011578   \n",
       "77              0.687635      0.006187   \n",
       "71              0.424266      0.009518   \n",
       "150             0.588464      0.003963   \n",
       "7               0.513521      0.018860   \n",
       "103             0.464238      0.001088   \n",
       "53              0.550102      0.012473   \n",
       "\n",
       "     RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation  \\\n",
       "3                                             0.242016                                 \n",
       "70                                            0.290817                                 \n",
       "92                                            0.318683                                 \n",
       "124                                           0.342444                                 \n",
       "56                                            0.400382                                 \n",
       "134                                           0.293455                                 \n",
       "107                                           0.472482                                 \n",
       "78                                            0.260657                                 \n",
       "128                                           0.285639                                 \n",
       "65                                            0.156307                                 \n",
       "42                                            0.271944                                 \n",
       "123                                           0.257929                                 \n",
       "111                                           0.321620                                 \n",
       "0                                             0.339837                                 \n",
       "136                                           0.375639                                 \n",
       "98                                            0.310298                                 \n",
       "90                                            0.222032                                 \n",
       "114                                           0.212013                                 \n",
       "108                                           0.335852                                 \n",
       "131                                           0.294278                                 \n",
       "101                                           0.200118                                 \n",
       "68                                            0.205241                                 \n",
       "95                                            0.184146                                 \n",
       "119                                           0.274726                                 \n",
       "64                                            0.415847                                 \n",
       "77                                            0.213552                                 \n",
       "71                                            0.365632                                 \n",
       "150                                           0.288079                                 \n",
       "7                                             0.335144                                 \n",
       "103                                           0.423214                                 \n",
       "53                                            0.316483                                 \n",
       "\n",
       "     MDCP - Married in a registered marriage  \n",
       "3                                   0.385881  \n",
       "70                                  0.280871  \n",
       "92                                  0.339021  \n",
       "124                                 0.340102  \n",
       "56                                  0.351234  \n",
       "134                                 0.366205  \n",
       "107                                 0.181893  \n",
       "78                                  0.340778  \n",
       "128                                 0.359918  \n",
       "65                                  0.400018  \n",
       "42                                  0.328731  \n",
       "123                                 0.343199  \n",
       "111                                 0.292694  \n",
       "0                                   0.302321  \n",
       "136                                 0.291657  \n",
       "98                                  0.359523  \n",
       "90                                  0.332070  \n",
       "114                                 0.325706  \n",
       "108                                 0.431292  \n",
       "131                                 0.351900  \n",
       "101                                 0.340305  \n",
       "68                                  0.347481  \n",
       "95                                  0.347075  \n",
       "119                                 0.329300  \n",
       "64                                  0.282001  \n",
       "77                                  0.374747  \n",
       "71                                  0.327740  \n",
       "150                                 0.362536  \n",
       "7                                   0.366788  \n",
       "103                                 0.388684  \n",
       "53                                  0.323253  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cols = X_test.iloc[:,relevant_cols]\n",
    "best_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "efefcdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDP - Not applicable</th>\n",
       "      <th>LFHRP - Not in the labour force</th>\n",
       "      <th>GNGP - Not applicable</th>\n",
       "      <th>MSTP - Never married</th>\n",
       "      <th>MSTP - Married</th>\n",
       "      <th>RLHP - Husband, Wife or Partner in a registered marriage</th>\n",
       "      <th>RELP - Christianity</th>\n",
       "      <th>RELP - Islam</th>\n",
       "      <th>RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.211056</td>\n",
       "      <td>-0.232745</td>\n",
       "      <td>-0.205949</td>\n",
       "      <td>0.218063</td>\n",
       "      <td>-0.169172</td>\n",
       "      <td>-0.20466</td>\n",
       "      <td>-0.637355</td>\n",
       "      <td>-0.205417</td>\n",
       "      <td>0.617044</td>\n",
       "      <td>-0.197312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.211056</td>\n",
       "      <td>-0.232745</td>\n",
       "      <td>-0.205949</td>\n",
       "      <td>0.218063</td>\n",
       "      <td>-0.169172</td>\n",
       "      <td>-0.20466</td>\n",
       "      <td>-0.637355</td>\n",
       "      <td>-0.205417</td>\n",
       "      <td>0.617044</td>\n",
       "      <td>-0.197312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   INDP - Not applicable  LFHRP - Not in the labour force  \\\n",
       "0              -0.211056                        -0.232745   \n",
       "1              -0.211056                        -0.232745   \n",
       "\n",
       "   GNGP - Not applicable  MSTP - Never married  MSTP - Married  \\\n",
       "0              -0.205949              0.218063       -0.169172   \n",
       "1              -0.205949              0.218063       -0.169172   \n",
       "\n",
       "   RLHP - Husband, Wife or Partner in a registered marriage  \\\n",
       "0                                           -0.20466          \n",
       "1                                           -0.20466          \n",
       "\n",
       "   RELP - Christianity  RELP - Islam  \\\n",
       "0            -0.637355     -0.205417   \n",
       "1            -0.637355     -0.205417   \n",
       "\n",
       "   RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation  \\\n",
       "0                                           0.617044                                 \n",
       "1                                           0.617044                                 \n",
       "\n",
       "   MDCP - Married in a registered marriage  \n",
       "0                                -0.197312  \n",
       "1                                -0.197312  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_DF = pd.DataFrame(list(map(np.ravel, rfe.estimator_.coef_)), columns=X_train.columns[rfe.support_], index=rfe.estimator_.classes_)\n",
    "coefficient_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e480a54",
   "metadata": {},
   "source": [
    "# 4.1 Interpretation of Marginal seats (61% Accurate)\n",
    "\n",
    "\n",
    "## Strongest determinants\n",
    "- Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation (0.617044)\n",
    "- Christianity (-0.637355)\n",
    "\n",
    "## Weaker determinants\n",
    "- Not in the labour force (-0.232745)\n",
    "- Never married (0.218063)\n",
    "- INDP - Not applicable (-0.211056)\n",
    "- GNGP - Not applicable (-0.205949)\n",
    "- Islam (-0.205417)\n",
    "- Husband, Wife or Partner in a registered marriage (-0.20466)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2467c8d",
   "metadata": {},
   "source": [
    "# 4.2 Fairly Safe ALP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36941a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 152)\n",
      "(31, 152)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(adjusted_master, test_size=0.2, random_state=142)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7e79840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_train = train['is_Fairly_Safe_ALP']\n",
    "X_test = test.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_test = test['is_Fairly_Safe_ALP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a7426379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set:  0.8833333333333333\n",
      "Accuracy score on testing set:  0.8709677419354839\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "print(\"Accuracy score on training set: \", accuracy_score(y_train, y_hat_train))\n",
    "print(\"Accuracy score on testing set: \", accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "57c30f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using 1 features:  0.8709677419354839\n",
      "Accuracy on test set using 2 features:  0.8709677419354839\n",
      "Accuracy on test set using 3 features:  0.8709677419354839\n",
      "Accuracy on test set using 4 features:  0.8709677419354839\n",
      "Accuracy on test set using 5 features:  0.8709677419354839\n",
      "Accuracy on test set using 6 features:  0.8709677419354839\n",
      "Accuracy on test set using 7 features:  0.8709677419354839\n",
      "Accuracy on test set using 8 features:  0.8709677419354839\n",
      "Accuracy on test set using 9 features:  0.8709677419354839\n",
      "Accuracy on test set using 10 features:  0.8709677419354839\n",
      "Accuracy on test set using 11 features:  0.8709677419354839\n",
      "Accuracy on test set using 12 features:  0.8709677419354839\n",
      "Accuracy on test set using 13 features:  0.8709677419354839\n",
      "Accuracy on test set using 14 features:  0.8709677419354839\n",
      "Accuracy on test set using 15 features:  0.8709677419354839\n",
      "Accuracy on test set using 16 features:  0.8709677419354839\n",
      "Accuracy on test set using 17 features:  0.8709677419354839\n",
      "Accuracy on test set using 18 features:  0.8709677419354839\n",
      "Accuracy on test set using 19 features:  0.8709677419354839\n",
      "Accuracy on test set using 20 features:  0.8709677419354839\n",
      "Accuracy on test set using 21 features:  0.8709677419354839\n",
      "Accuracy on test set using 22 features:  0.8709677419354839\n",
      "Accuracy on test set using 23 features:  0.8709677419354839\n",
      "Accuracy on test set using 24 features:  0.8709677419354839\n",
      "Accuracy on test set using 25 features:  0.8709677419354839\n",
      "Accuracy on test set using 26 features:  0.8709677419354839\n",
      "Accuracy on test set using 27 features:  0.8709677419354839\n",
      "Accuracy on test set using 28 features:  0.8709677419354839\n",
      "Accuracy on test set using 29 features:  0.8709677419354839\n",
      "Accuracy on test set using 30 features:  0.8709677419354839\n",
      "Accuracy on test set using 31 features:  0.8709677419354839\n",
      "Accuracy on test set using 32 features:  0.8709677419354839\n",
      "Accuracy on test set using 33 features:  0.8709677419354839\n",
      "Accuracy on test set using 34 features:  0.8709677419354839\n",
      "Accuracy on test set using 35 features:  0.8709677419354839\n",
      "Accuracy on test set using 36 features:  0.8709677419354839\n",
      "Accuracy on test set using 37 features:  0.8709677419354839\n",
      "Accuracy on test set using 38 features:  0.8709677419354839\n",
      "Accuracy on test set using 39 features:  0.8709677419354839\n",
      "Accuracy on test set using 40 features:  0.8709677419354839\n",
      "Accuracy on test set using 41 features:  0.8709677419354839\n",
      "Accuracy on test set using 42 features:  0.8709677419354839\n",
      "Accuracy on test set using 43 features:  0.8709677419354839\n",
      "Accuracy on test set using 44 features:  0.8709677419354839\n",
      "Accuracy on test set using 45 features:  0.8709677419354839\n",
      "Accuracy on test set using 46 features:  0.8709677419354839\n",
      "Accuracy on test set using 47 features:  0.8709677419354839\n",
      "Accuracy on test set using 48 features:  0.8709677419354839\n",
      "Accuracy on test set using 49 features:  0.8709677419354839\n",
      "Accuracy on test set using 50 features:  0.8709677419354839\n",
      "Accuracy on test set using 51 features:  0.8709677419354839\n",
      "Accuracy on test set using 52 features:  0.8709677419354839\n",
      "Accuracy on test set using 53 features:  0.8709677419354839\n",
      "Accuracy on test set using 54 features:  0.8709677419354839\n",
      "Accuracy on test set using 55 features:  0.8709677419354839\n",
      "Accuracy on test set using 56 features:  0.8709677419354839\n",
      "Accuracy on test set using 57 features:  0.8709677419354839\n",
      "Accuracy on test set using 58 features:  0.8709677419354839\n",
      "Accuracy on test set using 59 features:  0.8709677419354839\n",
      "Accuracy on test set using 60 features:  0.8709677419354839\n",
      "Accuracy on test set using 61 features:  0.8709677419354839\n",
      "Accuracy on test set using 62 features:  0.8709677419354839\n",
      "Accuracy on test set using 63 features:  0.8709677419354839\n",
      "Accuracy on test set using 64 features:  0.8709677419354839\n",
      "Accuracy on test set using 65 features:  0.8709677419354839\n",
      "Accuracy on test set using 66 features:  0.8709677419354839\n",
      "Accuracy on test set using 67 features:  0.8709677419354839\n",
      "Accuracy on test set using 68 features:  0.8709677419354839\n",
      "Accuracy on test set using 69 features:  0.8709677419354839\n",
      "Accuracy on test set using 70 features:  0.8709677419354839\n",
      "Accuracy on test set using 71 features:  0.8709677419354839\n",
      "Accuracy on test set using 72 features:  0.8709677419354839\n",
      "Accuracy on test set using 73 features:  0.8709677419354839\n",
      "Accuracy on test set using 74 features:  0.8709677419354839\n",
      "Accuracy on test set using 75 features:  0.8709677419354839\n",
      "Accuracy on test set using 76 features:  0.8709677419354839\n",
      "Accuracy on test set using 77 features:  0.8709677419354839\n",
      "Accuracy on test set using 78 features:  0.8709677419354839\n",
      "Accuracy on test set using 79 features:  0.8709677419354839\n",
      "Accuracy on test set using 80 features:  0.8709677419354839\n",
      "Accuracy on test set using 81 features:  0.8709677419354839\n",
      "Accuracy on test set using 82 features:  0.8709677419354839\n",
      "Accuracy on test set using 83 features:  0.8709677419354839\n",
      "Accuracy on test set using 84 features:  0.8709677419354839\n",
      "Accuracy on test set using 85 features:  0.8709677419354839\n",
      "Accuracy on test set using 86 features:  0.8709677419354839\n",
      "Accuracy on test set using 87 features:  0.8709677419354839\n",
      "Accuracy on test set using 88 features:  0.8709677419354839\n",
      "Accuracy on test set using 89 features:  0.8709677419354839\n",
      "Accuracy on test set using 90 features:  0.8709677419354839\n",
      "Accuracy on test set using 91 features:  0.8709677419354839\n",
      "Accuracy on test set using 92 features:  0.8709677419354839\n",
      "Accuracy on test set using 93 features:  0.8709677419354839\n",
      "Accuracy on test set using 94 features:  0.8709677419354839\n",
      "Accuracy on test set using 95 features:  0.8709677419354839\n",
      "Accuracy on test set using 96 features:  0.8709677419354839\n",
      "Accuracy on test set using 97 features:  0.8709677419354839\n",
      "Accuracy on test set using 98 features:  0.8709677419354839\n",
      "Accuracy on test set using 99 features:  0.8709677419354839\n",
      "Accuracy on test set using 100 features:  0.8709677419354839\n",
      "Accuracy on test set using 101 features:  0.8709677419354839\n",
      "Accuracy on test set using 102 features:  0.8709677419354839\n",
      "Accuracy on test set using 103 features:  0.8709677419354839\n",
      "Accuracy on test set using 104 features:  0.8709677419354839\n",
      "Accuracy on test set using 105 features:  0.8709677419354839\n",
      "Accuracy on test set using 106 features:  0.8709677419354839\n",
      "Accuracy on test set using 107 features:  0.8709677419354839\n",
      "Accuracy on test set using 108 features:  0.8709677419354839\n",
      "Accuracy on test set using 109 features:  0.8709677419354839\n",
      "Accuracy on test set using 110 features:  0.8709677419354839\n",
      "Accuracy on test set using 111 features:  0.8709677419354839\n",
      "Accuracy on test set using 112 features:  0.8709677419354839\n",
      "Accuracy on test set using 113 features:  0.8709677419354839\n",
      "Accuracy on test set using 114 features:  0.8709677419354839\n",
      "Accuracy on test set using 115 features:  0.8709677419354839\n",
      "Accuracy on test set using 116 features:  0.8709677419354839\n",
      "Accuracy on test set using 117 features:  0.8709677419354839\n",
      "Accuracy on test set using 118 features:  0.8709677419354839\n",
      "Accuracy on test set using 119 features:  0.8709677419354839\n",
      "Accuracy on test set using 120 features:  0.8709677419354839\n",
      "Accuracy on test set using 121 features:  0.8709677419354839\n",
      "Accuracy on test set using 122 features:  0.8709677419354839\n",
      "Accuracy on test set using 123 features:  0.8709677419354839\n",
      "Accuracy on test set using 124 features:  0.8709677419354839\n",
      "Accuracy on test set using 125 features:  0.8709677419354839\n",
      "Accuracy on test set using 126 features:  0.8709677419354839\n",
      "Accuracy on test set using 127 features:  0.8709677419354839\n",
      "Accuracy on test set using 128 features:  0.8709677419354839\n",
      "Accuracy on test set using 129 features:  0.8709677419354839\n",
      "Accuracy on test set using 130 features:  0.8709677419354839\n",
      "Accuracy on test set using 131 features:  0.8709677419354839\n",
      "Accuracy on test set using 132 features:  0.8709677419354839\n",
      "Accuracy on test set using 133 features:  0.8709677419354839\n",
      "Accuracy on test set using 134 features:  0.8709677419354839\n",
      "Accuracy on test set using 135 features:  0.8709677419354839\n",
      "Accuracy on test set using 136 features:  0.8709677419354839\n",
      "Accuracy on test set using 137 features:  0.8709677419354839\n",
      "Accuracy on test set using 138 features:  0.8709677419354839\n",
      "Accuracy on test set using 139 features:  0.8709677419354839\n",
      "1 columns provide the highest accuracy at: 87.10 %\n"
     ]
    }
   ],
   "source": [
    "acc_scores = []\n",
    "for i in range(1,140):\n",
    "    clf = LogisticRegression()\n",
    "    rfe = RFE(estimator=clf, n_features_to_select=i)\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfe.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy on test set using\", i, \"features: \", acc_score)\n",
    "    \n",
    "    acc_scores.append(acc_score)\n",
    "\n",
    "best = acc_scores[0]\n",
    "best_number_of_columns = 0;\n",
    "for i in range(len(acc_scores)):\n",
    "    if acc_scores[i] > best:\n",
    "        best = acc_scores[i]\n",
    "        best_number_of_columns = i\n",
    "        \n",
    "print(best_number_of_columns + 1, 'columns provide the highest accuracy at: %.2f' % (best*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0a7f79ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test set:  0.8709677419354839\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "rfe = RFE(estimator=lr_model, n_features_to_select=5, step=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "y_test_hat = rfe.predict(X_test)\n",
    "print(\"accuracy score on test set: \", accuracy_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c8478b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected False, Rank: 36.000\n",
      "Column: 1, Selected False, Rank: 87.000\n",
      "Column: 2, Selected False, Rank: 33.000\n",
      "Column: 3, Selected False, Rank: 50.000\n",
      "Column: 4, Selected False, Rank: 26.000\n",
      "Column: 5, Selected False, Rank: 8.000\n",
      "Column: 6, Selected False, Rank: 7.000\n",
      "Column: 7, Selected False, Rank: 11.000\n",
      "Column: 8, Selected False, Rank: 60.000\n",
      "Column: 9, Selected False, Rank: 95.000\n",
      "Column: 10, Selected False, Rank: 45.000\n",
      "Column: 11, Selected False, Rank: 27.000\n",
      "Column: 12, Selected False, Rank: 17.000\n",
      "Column: 13, Selected False, Rank: 15.000\n",
      "Column: 14, Selected False, Rank: 24.000\n",
      "Column: 15, Selected False, Rank: 44.000\n",
      "Column: 16, Selected False, Rank: 72.000\n",
      "Column: 17, Selected False, Rank: 94.000\n",
      "Column: 18, Selected False, Rank: 105.000\n",
      "Column: 19, Selected False, Rank: 114.000\n",
      "Column: 20, Selected False, Rank: 127.000\n",
      "Column: 21, Selected False, Rank: 34.000\n",
      "Column: 22, Selected False, Rank: 99.000\n",
      "Column: 23, Selected False, Rank: 12.000\n",
      "Column: 24, Selected False, Rank: 82.000\n",
      "Column: 25, Selected False, Rank: 13.000\n",
      "Column: 26, Selected False, Rank: 77.000\n",
      "Column: 27, Selected False, Rank: 123.000\n",
      "Column: 28, Selected False, Rank: 71.000\n",
      "Column: 29, Selected False, Rank: 51.000\n",
      "Column: 30, Selected False, Rank: 23.000\n",
      "Column: 31, Selected False, Rank: 64.000\n",
      "Column: 32, Selected False, Rank: 31.000\n",
      "Column: 33, Selected False, Rank: 37.000\n",
      "Column: 34, Selected False, Rank: 62.000\n",
      "Column: 35, Selected False, Rank: 91.000\n",
      "Column: 36, Selected False, Rank: 98.000\n",
      "Column: 37, Selected False, Rank: 84.000\n",
      "Column: 38, Selected False, Rank: 92.000\n",
      "Column: 39, Selected False, Rank: 29.000\n",
      "Column: 40, Selected False, Rank: 52.000\n",
      "Column: 41, Selected False, Rank: 46.000\n",
      "Column: 42, Selected False, Rank: 55.000\n",
      "Column: 43, Selected False, Rank: 132.000\n",
      "Column: 44, Selected False, Rank: 35.000\n",
      "Column: 45, Selected False, Rank: 117.000\n",
      "Column: 46, Selected False, Rank: 18.000\n",
      "Column: 47, Selected False, Rank: 63.000\n",
      "Column: 48, Selected False, Rank: 32.000\n",
      "Column: 49, Selected False, Rank: 81.000\n",
      "Column: 50, Selected False, Rank: 80.000\n",
      "Column: 51, Selected False, Rank: 10.000\n",
      "Column: 52, Selected False, Rank: 74.000\n",
      "Column: 53, Selected False, Rank: 59.000\n",
      "Column: 54, Selected False, Rank: 119.000\n",
      "Column: 55, Selected False, Rank: 118.000\n",
      "Column: 56, Selected False, Rank: 38.000\n",
      "Column: 57, Selected False, Rank: 73.000\n",
      "Column: 58, Selected False, Rank: 79.000\n",
      "Column: 59, Selected False, Rank: 76.000\n",
      "Column: 60, Selected False, Rank: 20.000\n",
      "Column: 61, Selected False, Rank: 43.000\n",
      "Column: 62, Selected False, Rank: 3.000\n",
      "Column: 63, Selected False, Rank: 110.000\n",
      "Column: 64, Selected False, Rank: 100.000\n",
      "Column: 65, Selected False, Rank: 58.000\n",
      "Column: 66, Selected False, Rank: 53.000\n",
      "Column: 67, Selected False, Rank: 111.000\n",
      "Column: 68, Selected False, Rank: 61.000\n",
      "Column: 69, Selected False, Rank: 47.000\n",
      "Column: 70, Selected False, Rank: 54.000\n",
      "Column: 71, Selected False, Rank: 2.000\n",
      "Column: 72, Selected False, Rank: 115.000\n",
      "Column: 73, Selected False, Rank: 101.000\n",
      "Column: 74, Selected False, Rank: 121.000\n",
      "Column: 75, Selected False, Rank: 83.000\n",
      "Column: 76, Selected False, Rank: 85.000\n",
      "Column: 77, Selected False, Rank: 9.000\n",
      "Column: 78, Selected False, Rank: 41.000\n",
      "Column: 79, Selected False, Rank: 65.000\n",
      "Column: 80, Selected False, Rank: 128.000\n",
      "Column: 81, Selected False, Rank: 93.000\n",
      "Column: 82, Selected False, Rank: 88.000\n",
      "Column: 83, Selected True, Rank: 1.000\n",
      "Column: 84, Selected False, Rank: 109.000\n",
      "Column: 85, Selected False, Rank: 5.000\n",
      "Column: 86, Selected True, Rank: 1.000\n",
      "Column: 87, Selected False, Rank: 49.000\n",
      "Column: 88, Selected False, Rank: 40.000\n",
      "Column: 89, Selected False, Rank: 97.000\n",
      "Column: 90, Selected False, Rank: 6.000\n",
      "Column: 91, Selected False, Rank: 66.000\n",
      "Column: 92, Selected False, Rank: 19.000\n",
      "Column: 93, Selected False, Rank: 67.000\n",
      "Column: 94, Selected False, Rank: 103.000\n",
      "Column: 95, Selected False, Rank: 102.000\n",
      "Column: 96, Selected False, Rank: 90.000\n",
      "Column: 97, Selected False, Rank: 89.000\n",
      "Column: 98, Selected False, Rank: 96.000\n",
      "Column: 99, Selected False, Rank: 113.000\n",
      "Column: 100, Selected False, Rank: 106.000\n",
      "Column: 101, Selected False, Rank: 122.000\n",
      "Column: 102, Selected False, Rank: 126.000\n",
      "Column: 103, Selected False, Rank: 68.000\n",
      "Column: 104, Selected False, Rank: 124.000\n",
      "Column: 105, Selected False, Rank: 131.000\n",
      "Column: 106, Selected False, Rank: 25.000\n",
      "Column: 107, Selected False, Rank: 120.000\n",
      "Column: 108, Selected False, Rank: 133.000\n",
      "Column: 109, Selected False, Rank: 56.000\n",
      "Column: 110, Selected False, Rank: 107.000\n",
      "Column: 111, Selected False, Rank: 134.000\n",
      "Column: 112, Selected False, Rank: 129.000\n",
      "Column: 113, Selected False, Rank: 108.000\n",
      "Column: 114, Selected False, Rank: 125.000\n",
      "Column: 115, Selected False, Rank: 116.000\n",
      "Column: 116, Selected False, Rank: 130.000\n",
      "Column: 117, Selected False, Rank: 78.000\n",
      "Column: 118, Selected False, Rank: 22.000\n",
      "Column: 119, Selected False, Rank: 75.000\n",
      "Column: 120, Selected False, Rank: 30.000\n",
      "Column: 121, Selected False, Rank: 86.000\n",
      "Column: 122, Selected False, Rank: 14.000\n",
      "Column: 123, Selected False, Rank: 39.000\n",
      "Column: 124, Selected True, Rank: 1.000\n",
      "Column: 125, Selected False, Rank: 42.000\n",
      "Column: 126, Selected True, Rank: 1.000\n",
      "Column: 127, Selected False, Rank: 21.000\n",
      "Column: 128, Selected False, Rank: 104.000\n",
      "Column: 129, Selected False, Rank: 28.000\n",
      "Column: 130, Selected False, Rank: 112.000\n",
      "Column: 131, Selected False, Rank: 57.000\n",
      "Column: 132, Selected False, Rank: 69.000\n",
      "Column: 133, Selected False, Rank: 70.000\n",
      "Column: 134, Selected False, Rank: 16.000\n",
      "Column: 135, Selected False, Rank: 48.000\n",
      "Column: 136, Selected True, Rank: 1.000\n",
      "Column: 137, Selected False, Rank: 4.000\n",
      "[83, 86, 124, 126, 136]\n"
     ]
    }
   ],
   "source": [
    "relevant_cols = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    if rfe.support_[i] == True:\n",
    "        relevant_cols.append(i)\n",
    "        \n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83b45c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [83, 86, 124, 126, 136]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent\n",
       "3         0          0         56.26\n",
       "70        0          0         58.36\n",
       "92        0          0         53.28\n",
       "124       0          0         57.52\n",
       "56        1          0         62.21\n",
       "134       1          0         54.45\n",
       "107       0          0         71.83\n",
       "78        1          0         52.98\n",
       "128       0          0         53.18\n",
       "65        1          0         52.80\n",
       "42        1          0         51.50\n",
       "123       1          0         55.04\n",
       "111       0          0         65.36\n",
       "0         1          0         58.18\n",
       "136       1          0         64.13\n",
       "98        1          0         50.19\n",
       "90        0          0         55.04\n",
       "114       0          0         64.36\n",
       "108       0          0         57.53\n",
       "131       0          0         54.24\n",
       "101       0          0         72.49\n",
       "68        0          0         70.48\n",
       "95        1          0         58.40\n",
       "119       1          0         56.39\n",
       "64        1          0         66.30\n",
       "77        0          0         62.99\n",
       "71        0          0         53.88\n",
       "150       0          0         64.58\n",
       "7         1          0         57.52\n",
       "103       0          0         55.14\n",
       "53        0          0         58.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(test['is_ALP'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(X_test))\n",
    "result.insert(2, \"Pref Percent\", test['PreferencePercent'].values)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "456e322c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GNGP - Private sector</th>\n",
       "      <th>MSTP - Never married</th>\n",
       "      <th>RELP - Christianity</th>\n",
       "      <th>RELP - Islam</th>\n",
       "      <th>MDCP - Not married</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.189134</td>\n",
       "      <td>0.275993</td>\n",
       "      <td>-0.424978</td>\n",
       "      <td>0.198243</td>\n",
       "      <td>0.220149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.189134</td>\n",
       "      <td>0.275993</td>\n",
       "      <td>-0.424978</td>\n",
       "      <td>0.198243</td>\n",
       "      <td>0.220149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GNGP - Private sector  MSTP - Never married  RELP - Christianity  \\\n",
       "0               0.189134              0.275993            -0.424978   \n",
       "1               0.189134              0.275993            -0.424978   \n",
       "\n",
       "   RELP - Islam  MDCP - Not married  \n",
       "0      0.198243            0.220149  \n",
       "1      0.198243            0.220149  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_DF = pd.DataFrame(list(map(np.ravel, rfe.estimator_.coef_)), columns=X_train.columns[rfe.support_], index=rfe.estimator_.classes_)\n",
    "coefficient_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0b3ba",
   "metadata": {},
   "source": [
    "# 4.2) Fairly Safe ALP Interpretation (87% Accurate)\n",
    "\n",
    "## Strongest determinant\n",
    "- Christianity (-0.424978)\n",
    "\n",
    "## Weaker determinants\n",
    "- Never married (0.275993)\n",
    "- Not married (0.220149)\n",
    "- Islam (0.198243)\n",
    "- Private sector (0.189134)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058505f3",
   "metadata": {},
   "source": [
    "# 4.3) Fairly Safe LNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3c77cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 152)\n",
      "(31, 152)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(adjusted_master, test_size=0.2, random_state=142)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7171eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_train = train['is_Fairly_Safe_LNP']\n",
    "X_test = test.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_test = test['is_Fairly_Safe_LNP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "974878ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set:  0.9\n",
      "Accuracy score on testing set:  0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "print(\"Accuracy score on training set: \", accuracy_score(y_train, y_hat_train))\n",
    "print(\"Accuracy score on testing set: \", accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "83ad7ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using 1 features:  0.8387096774193549\n",
      "Accuracy on test set using 2 features:  0.8387096774193549\n",
      "Accuracy on test set using 3 features:  0.8387096774193549\n",
      "Accuracy on test set using 4 features:  0.8387096774193549\n",
      "Accuracy on test set using 5 features:  0.8387096774193549\n",
      "Accuracy on test set using 6 features:  0.8387096774193549\n",
      "Accuracy on test set using 7 features:  0.8387096774193549\n",
      "Accuracy on test set using 8 features:  0.8387096774193549\n",
      "Accuracy on test set using 9 features:  0.8387096774193549\n",
      "Accuracy on test set using 10 features:  0.8387096774193549\n",
      "Accuracy on test set using 11 features:  0.8387096774193549\n",
      "Accuracy on test set using 12 features:  0.8387096774193549\n",
      "Accuracy on test set using 13 features:  0.8387096774193549\n",
      "Accuracy on test set using 14 features:  0.8387096774193549\n",
      "Accuracy on test set using 15 features:  0.8387096774193549\n",
      "Accuracy on test set using 16 features:  0.8387096774193549\n",
      "Accuracy on test set using 17 features:  0.8387096774193549\n",
      "Accuracy on test set using 18 features:  0.8387096774193549\n",
      "Accuracy on test set using 19 features:  0.8387096774193549\n",
      "Accuracy on test set using 20 features:  0.8387096774193549\n",
      "Accuracy on test set using 21 features:  0.8387096774193549\n",
      "Accuracy on test set using 22 features:  0.8387096774193549\n",
      "Accuracy on test set using 23 features:  0.8387096774193549\n",
      "Accuracy on test set using 24 features:  0.8387096774193549\n",
      "Accuracy on test set using 25 features:  0.8387096774193549\n",
      "Accuracy on test set using 26 features:  0.8387096774193549\n",
      "Accuracy on test set using 27 features:  0.8387096774193549\n",
      "Accuracy on test set using 28 features:  0.8387096774193549\n",
      "Accuracy on test set using 29 features:  0.8387096774193549\n",
      "Accuracy on test set using 30 features:  0.8387096774193549\n",
      "Accuracy on test set using 31 features:  0.8387096774193549\n",
      "Accuracy on test set using 32 features:  0.8387096774193549\n",
      "Accuracy on test set using 33 features:  0.8387096774193549\n",
      "Accuracy on test set using 34 features:  0.8387096774193549\n",
      "Accuracy on test set using 35 features:  0.8387096774193549\n",
      "Accuracy on test set using 36 features:  0.8387096774193549\n",
      "Accuracy on test set using 37 features:  0.8387096774193549\n",
      "Accuracy on test set using 38 features:  0.8387096774193549\n",
      "Accuracy on test set using 39 features:  0.8387096774193549\n",
      "Accuracy on test set using 40 features:  0.8387096774193549\n",
      "Accuracy on test set using 41 features:  0.8387096774193549\n",
      "Accuracy on test set using 42 features:  0.8387096774193549\n",
      "Accuracy on test set using 43 features:  0.8387096774193549\n",
      "Accuracy on test set using 44 features:  0.8387096774193549\n",
      "Accuracy on test set using 45 features:  0.8387096774193549\n",
      "Accuracy on test set using 46 features:  0.8387096774193549\n",
      "Accuracy on test set using 47 features:  0.8387096774193549\n",
      "Accuracy on test set using 48 features:  0.8387096774193549\n",
      "Accuracy on test set using 49 features:  0.8387096774193549\n",
      "Accuracy on test set using 50 features:  0.8387096774193549\n",
      "Accuracy on test set using 51 features:  0.8387096774193549\n",
      "Accuracy on test set using 52 features:  0.8387096774193549\n",
      "Accuracy on test set using 53 features:  0.8387096774193549\n",
      "Accuracy on test set using 54 features:  0.8387096774193549\n",
      "Accuracy on test set using 55 features:  0.8387096774193549\n",
      "Accuracy on test set using 56 features:  0.8387096774193549\n",
      "Accuracy on test set using 57 features:  0.8387096774193549\n",
      "Accuracy on test set using 58 features:  0.8387096774193549\n",
      "Accuracy on test set using 59 features:  0.8387096774193549\n",
      "Accuracy on test set using 60 features:  0.8387096774193549\n",
      "Accuracy on test set using 61 features:  0.8387096774193549\n",
      "Accuracy on test set using 62 features:  0.8387096774193549\n",
      "Accuracy on test set using 63 features:  0.8387096774193549\n",
      "Accuracy on test set using 64 features:  0.8387096774193549\n",
      "Accuracy on test set using 65 features:  0.8387096774193549\n",
      "Accuracy on test set using 66 features:  0.8387096774193549\n",
      "Accuracy on test set using 67 features:  0.8387096774193549\n",
      "Accuracy on test set using 68 features:  0.8387096774193549\n",
      "Accuracy on test set using 69 features:  0.8387096774193549\n",
      "Accuracy on test set using 70 features:  0.8387096774193549\n",
      "Accuracy on test set using 71 features:  0.8387096774193549\n",
      "Accuracy on test set using 72 features:  0.8387096774193549\n",
      "Accuracy on test set using 73 features:  0.8387096774193549\n",
      "Accuracy on test set using 74 features:  0.8387096774193549\n",
      "Accuracy on test set using 75 features:  0.8387096774193549\n",
      "Accuracy on test set using 76 features:  0.8387096774193549\n",
      "Accuracy on test set using 77 features:  0.8387096774193549\n",
      "Accuracy on test set using 78 features:  0.8387096774193549\n",
      "Accuracy on test set using 79 features:  0.8387096774193549\n",
      "Accuracy on test set using 80 features:  0.8387096774193549\n",
      "Accuracy on test set using 81 features:  0.8387096774193549\n",
      "Accuracy on test set using 82 features:  0.8387096774193549\n",
      "Accuracy on test set using 83 features:  0.8387096774193549\n",
      "Accuracy on test set using 84 features:  0.8387096774193549\n",
      "Accuracy on test set using 85 features:  0.8387096774193549\n",
      "Accuracy on test set using 86 features:  0.8387096774193549\n",
      "Accuracy on test set using 87 features:  0.8387096774193549\n",
      "Accuracy on test set using 88 features:  0.8387096774193549\n",
      "Accuracy on test set using 89 features:  0.8387096774193549\n",
      "Accuracy on test set using 90 features:  0.8387096774193549\n",
      "Accuracy on test set using 91 features:  0.8387096774193549\n",
      "Accuracy on test set using 92 features:  0.8387096774193549\n",
      "Accuracy on test set using 93 features:  0.8387096774193549\n",
      "Accuracy on test set using 94 features:  0.8387096774193549\n",
      "Accuracy on test set using 95 features:  0.8387096774193549\n",
      "Accuracy on test set using 96 features:  0.8387096774193549\n",
      "Accuracy on test set using 97 features:  0.8387096774193549\n",
      "Accuracy on test set using 98 features:  0.8387096774193549\n",
      "Accuracy on test set using 99 features:  0.8387096774193549\n",
      "Accuracy on test set using 100 features:  0.8387096774193549\n",
      "Accuracy on test set using 101 features:  0.8387096774193549\n",
      "Accuracy on test set using 102 features:  0.8387096774193549\n",
      "Accuracy on test set using 103 features:  0.8387096774193549\n",
      "Accuracy on test set using 104 features:  0.8387096774193549\n",
      "Accuracy on test set using 105 features:  0.8387096774193549\n",
      "Accuracy on test set using 106 features:  0.8387096774193549\n",
      "Accuracy on test set using 107 features:  0.8387096774193549\n",
      "Accuracy on test set using 108 features:  0.8387096774193549\n",
      "Accuracy on test set using 109 features:  0.8387096774193549\n",
      "Accuracy on test set using 110 features:  0.8387096774193549\n",
      "Accuracy on test set using 111 features:  0.8387096774193549\n",
      "Accuracy on test set using 112 features:  0.8387096774193549\n",
      "Accuracy on test set using 113 features:  0.8387096774193549\n",
      "Accuracy on test set using 114 features:  0.8387096774193549\n",
      "Accuracy on test set using 115 features:  0.8387096774193549\n",
      "Accuracy on test set using 116 features:  0.8387096774193549\n",
      "Accuracy on test set using 117 features:  0.8387096774193549\n",
      "Accuracy on test set using 118 features:  0.8387096774193549\n",
      "Accuracy on test set using 119 features:  0.8387096774193549\n",
      "Accuracy on test set using 120 features:  0.8387096774193549\n",
      "Accuracy on test set using 121 features:  0.8387096774193549\n",
      "Accuracy on test set using 122 features:  0.8387096774193549\n",
      "Accuracy on test set using 123 features:  0.8387096774193549\n",
      "Accuracy on test set using 124 features:  0.8387096774193549\n",
      "Accuracy on test set using 125 features:  0.8387096774193549\n",
      "Accuracy on test set using 126 features:  0.8387096774193549\n",
      "Accuracy on test set using 127 features:  0.8387096774193549\n",
      "Accuracy on test set using 128 features:  0.8387096774193549\n",
      "Accuracy on test set using 129 features:  0.8387096774193549\n",
      "Accuracy on test set using 130 features:  0.8387096774193549\n",
      "Accuracy on test set using 131 features:  0.8387096774193549\n",
      "Accuracy on test set using 132 features:  0.8387096774193549\n",
      "Accuracy on test set using 133 features:  0.8387096774193549\n",
      "Accuracy on test set using 134 features:  0.8387096774193549\n",
      "Accuracy on test set using 135 features:  0.8387096774193549\n",
      "Accuracy on test set using 136 features:  0.8387096774193549\n",
      "Accuracy on test set using 137 features:  0.8387096774193549\n",
      "Accuracy on test set using 138 features:  0.8387096774193549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using 139 features:  0.8387096774193549\n",
      "1 columns provide the highest accuracy at: 83.87 %\n"
     ]
    }
   ],
   "source": [
    "acc_scores = []\n",
    "for i in range(1,140):\n",
    "    clf = LogisticRegression()\n",
    "    rfe = RFE(estimator=clf, n_features_to_select=i)\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfe.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy on test set using\", i, \"features: \", acc_score)\n",
    "    \n",
    "    acc_scores.append(acc_score)\n",
    "\n",
    "best = acc_scores[0]\n",
    "best_number_of_columns = 0;\n",
    "for i in range(len(acc_scores)):\n",
    "    if acc_scores[i] > best:\n",
    "        best = acc_scores[i]\n",
    "        best_number_of_columns = i\n",
    "        \n",
    "print(best_number_of_columns + 1, 'columns provide the highest accuracy at: %.2f' % (best*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5dbb3d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test set:  0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "rfe = RFE(estimator=lr_model, n_features_to_select=5, step=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "y_test_hat = rfe.predict(X_test)\n",
    "print(\"accuracy score on test set: \", accuracy_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f396923d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected False, Rank: 40.000\n",
      "Column: 1, Selected False, Rank: 108.000\n",
      "Column: 2, Selected False, Rank: 85.000\n",
      "Column: 3, Selected False, Rank: 99.000\n",
      "Column: 4, Selected False, Rank: 31.000\n",
      "Column: 5, Selected False, Rank: 9.000\n",
      "Column: 6, Selected False, Rank: 12.000\n",
      "Column: 7, Selected False, Rank: 48.000\n",
      "Column: 8, Selected False, Rank: 93.000\n",
      "Column: 9, Selected False, Rank: 84.000\n",
      "Column: 10, Selected False, Rank: 70.000\n",
      "Column: 11, Selected False, Rank: 71.000\n",
      "Column: 12, Selected False, Rank: 46.000\n",
      "Column: 13, Selected False, Rank: 34.000\n",
      "Column: 14, Selected False, Rank: 38.000\n",
      "Column: 15, Selected False, Rank: 56.000\n",
      "Column: 16, Selected False, Rank: 69.000\n",
      "Column: 17, Selected False, Rank: 58.000\n",
      "Column: 18, Selected False, Rank: 75.000\n",
      "Column: 19, Selected False, Rank: 111.000\n",
      "Column: 20, Selected False, Rank: 129.000\n",
      "Column: 21, Selected False, Rank: 5.000\n",
      "Column: 22, Selected False, Rank: 54.000\n",
      "Column: 23, Selected True, Rank: 1.000\n",
      "Column: 24, Selected False, Rank: 68.000\n",
      "Column: 25, Selected False, Rank: 11.000\n",
      "Column: 26, Selected False, Rank: 4.000\n",
      "Column: 27, Selected False, Rank: 123.000\n",
      "Column: 28, Selected False, Rank: 16.000\n",
      "Column: 29, Selected False, Rank: 60.000\n",
      "Column: 30, Selected False, Rank: 8.000\n",
      "Column: 31, Selected False, Rank: 76.000\n",
      "Column: 32, Selected False, Rank: 55.000\n",
      "Column: 33, Selected False, Rank: 100.000\n",
      "Column: 34, Selected False, Rank: 65.000\n",
      "Column: 35, Selected False, Rank: 45.000\n",
      "Column: 36, Selected False, Rank: 94.000\n",
      "Column: 37, Selected False, Rank: 37.000\n",
      "Column: 38, Selected False, Rank: 10.000\n",
      "Column: 39, Selected False, Rank: 6.000\n",
      "Column: 40, Selected False, Rank: 53.000\n",
      "Column: 41, Selected False, Rank: 132.000\n",
      "Column: 42, Selected False, Rank: 89.000\n",
      "Column: 43, Selected False, Rank: 50.000\n",
      "Column: 44, Selected False, Rank: 30.000\n",
      "Column: 45, Selected False, Rank: 103.000\n",
      "Column: 46, Selected False, Rank: 124.000\n",
      "Column: 47, Selected False, Rank: 80.000\n",
      "Column: 48, Selected False, Rank: 13.000\n",
      "Column: 49, Selected False, Rank: 64.000\n",
      "Column: 50, Selected False, Rank: 90.000\n",
      "Column: 51, Selected False, Rank: 119.000\n",
      "Column: 52, Selected False, Rank: 106.000\n",
      "Column: 53, Selected False, Rank: 29.000\n",
      "Column: 54, Selected False, Rank: 27.000\n",
      "Column: 55, Selected False, Rank: 101.000\n",
      "Column: 56, Selected False, Rank: 24.000\n",
      "Column: 57, Selected False, Rank: 51.000\n",
      "Column: 58, Selected False, Rank: 74.000\n",
      "Column: 59, Selected False, Rank: 67.000\n",
      "Column: 60, Selected False, Rank: 26.000\n",
      "Column: 61, Selected False, Rank: 66.000\n",
      "Column: 62, Selected False, Rank: 33.000\n",
      "Column: 63, Selected False, Rank: 92.000\n",
      "Column: 64, Selected False, Rank: 98.000\n",
      "Column: 65, Selected False, Rank: 7.000\n",
      "Column: 66, Selected False, Rank: 44.000\n",
      "Column: 67, Selected False, Rank: 82.000\n",
      "Column: 68, Selected False, Rank: 61.000\n",
      "Column: 69, Selected False, Rank: 62.000\n",
      "Column: 70, Selected False, Rank: 105.000\n",
      "Column: 71, Selected False, Rank: 57.000\n",
      "Column: 72, Selected False, Rank: 43.000\n",
      "Column: 73, Selected False, Rank: 107.000\n",
      "Column: 74, Selected False, Rank: 86.000\n",
      "Column: 75, Selected False, Rank: 73.000\n",
      "Column: 76, Selected False, Rank: 112.000\n",
      "Column: 77, Selected False, Rank: 22.000\n",
      "Column: 78, Selected False, Rank: 18.000\n",
      "Column: 79, Selected False, Rank: 79.000\n",
      "Column: 80, Selected False, Rank: 96.000\n",
      "Column: 81, Selected False, Rank: 59.000\n",
      "Column: 82, Selected False, Rank: 120.000\n",
      "Column: 83, Selected False, Rank: 39.000\n",
      "Column: 84, Selected False, Rank: 91.000\n",
      "Column: 85, Selected False, Rank: 32.000\n",
      "Column: 86, Selected False, Rank: 2.000\n",
      "Column: 87, Selected False, Rank: 49.000\n",
      "Column: 88, Selected False, Rank: 83.000\n",
      "Column: 89, Selected False, Rank: 81.000\n",
      "Column: 90, Selected False, Rank: 3.000\n",
      "Column: 91, Selected False, Rank: 78.000\n",
      "Column: 92, Selected True, Rank: 1.000\n",
      "Column: 93, Selected False, Rank: 41.000\n",
      "Column: 94, Selected False, Rank: 115.000\n",
      "Column: 95, Selected False, Rank: 125.000\n",
      "Column: 96, Selected False, Rank: 72.000\n",
      "Column: 97, Selected False, Rank: 102.000\n",
      "Column: 98, Selected False, Rank: 121.000\n",
      "Column: 99, Selected False, Rank: 131.000\n",
      "Column: 100, Selected False, Rank: 110.000\n",
      "Column: 101, Selected False, Rank: 117.000\n",
      "Column: 102, Selected False, Rank: 126.000\n",
      "Column: 103, Selected False, Rank: 63.000\n",
      "Column: 104, Selected False, Rank: 130.000\n",
      "Column: 105, Selected False, Rank: 134.000\n",
      "Column: 106, Selected False, Rank: 28.000\n",
      "Column: 107, Selected False, Rank: 104.000\n",
      "Column: 108, Selected False, Rank: 133.000\n",
      "Column: 109, Selected False, Rank: 88.000\n",
      "Column: 110, Selected False, Rank: 116.000\n",
      "Column: 111, Selected False, Rank: 114.000\n",
      "Column: 112, Selected False, Rank: 128.000\n",
      "Column: 113, Selected False, Rank: 113.000\n",
      "Column: 114, Selected False, Rank: 122.000\n",
      "Column: 115, Selected False, Rank: 109.000\n",
      "Column: 116, Selected False, Rank: 118.000\n",
      "Column: 117, Selected False, Rank: 97.000\n",
      "Column: 118, Selected False, Rank: 77.000\n",
      "Column: 119, Selected False, Rank: 42.000\n",
      "Column: 120, Selected False, Rank: 87.000\n",
      "Column: 121, Selected False, Rank: 25.000\n",
      "Column: 122, Selected False, Rank: 95.000\n",
      "Column: 123, Selected False, Rank: 14.000\n",
      "Column: 124, Selected True, Rank: 1.000\n",
      "Column: 125, Selected False, Rank: 52.000\n",
      "Column: 126, Selected True, Rank: 1.000\n",
      "Column: 127, Selected False, Rank: 35.000\n",
      "Column: 128, Selected False, Rank: 47.000\n",
      "Column: 129, Selected False, Rank: 21.000\n",
      "Column: 130, Selected False, Rank: 127.000\n",
      "Column: 131, Selected False, Rank: 17.000\n",
      "Column: 132, Selected False, Rank: 19.000\n",
      "Column: 133, Selected False, Rank: 20.000\n",
      "Column: 134, Selected True, Rank: 1.000\n",
      "Column: 135, Selected False, Rank: 36.000\n",
      "Column: 136, Selected False, Rank: 23.000\n",
      "Column: 137, Selected False, Rank: 15.000\n",
      "[23, 92, 124, 126, 134]\n"
     ]
    }
   ],
   "source": [
    "relevant_cols = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    if rfe.support_[i] == True:\n",
    "        relevant_cols.append(i)\n",
    "        \n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "00978f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [23, 92, 124, 126, 134]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent\n",
       "3         0          0         56.26\n",
       "70        0          0         58.36\n",
       "92        0          0         53.28\n",
       "124       0          0         57.52\n",
       "56        1          0         62.21\n",
       "134       1          0         54.45\n",
       "107       0          0         71.83\n",
       "78        1          0         52.98\n",
       "128       0          0         53.18\n",
       "65        1          0         52.80\n",
       "42        1          0         51.50\n",
       "123       1          0         55.04\n",
       "111       0          0         65.36\n",
       "0         1          0         58.18\n",
       "136       1          0         64.13\n",
       "98        1          0         50.19\n",
       "90        0          0         55.04\n",
       "114       0          0         64.36\n",
       "108       0          0         57.53\n",
       "131       0          0         54.24\n",
       "101       0          0         72.49\n",
       "68        0          0         70.48\n",
       "95        1          0         58.40\n",
       "119       1          0         56.39\n",
       "64        1          0         66.30\n",
       "77        0          0         62.99\n",
       "71        0          0         53.88\n",
       "150       0          0         64.58\n",
       "7         1          0         57.52\n",
       "103       0          0         55.14\n",
       "53        0          0         58.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(test['is_ALP'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(X_test))\n",
    "result.insert(2, \"Pref Percent\", test['PreferencePercent'].values)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1f673d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HEAP - Bachelor Degree Level</th>\n",
       "      <th>RLHP - Husband, Wife or Partner in a registered marriage</th>\n",
       "      <th>RELP - Christianity</th>\n",
       "      <th>RELP - Islam</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.26231</td>\n",
       "      <td>0.180321</td>\n",
       "      <td>0.300811</td>\n",
       "      <td>-0.200075</td>\n",
       "      <td>0.182068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.26231</td>\n",
       "      <td>0.180321</td>\n",
       "      <td>0.300811</td>\n",
       "      <td>-0.200075</td>\n",
       "      <td>0.182068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HEAP - Bachelor Degree Level  \\\n",
       "0                       0.26231   \n",
       "1                       0.26231   \n",
       "\n",
       "   RLHP - Husband, Wife or Partner in a registered marriage  \\\n",
       "0                                           0.180321          \n",
       "1                                           0.180321          \n",
       "\n",
       "   RELP - Christianity  RELP - Islam  MDCP - Married in a registered marriage  \n",
       "0             0.300811     -0.200075                                 0.182068  \n",
       "1             0.300811     -0.200075                                 0.182068  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_DF = pd.DataFrame(list(map(np.ravel, rfe.estimator_.coef_)), columns=X_train.columns[rfe.support_], index=rfe.estimator_.classes_)\n",
    "coefficient_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ab3ae",
   "metadata": {},
   "source": [
    "# 4.3 Fairly Safe LNP (84% Accurate)\n",
    "\n",
    "## Determinants (All are fairly weak)\n",
    "\n",
    "- Christianity (0.300811)\n",
    "- Bachelor Degree Level (0.26231)\n",
    "- Islam (-0.200075)\n",
    "- Married in a registered marriage (0.182068)\n",
    "- Husband, Wife or Partner in a registered marriage (0.180321)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7974e33",
   "metadata": {},
   "source": [
    "# 4.4 Safe ALP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "081a1825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 152)\n",
      "(31, 152)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(adjusted_master, test_size=0.2, random_state=142)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "64354bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_train = train['is_Safe_ALP']\n",
    "X_test = test.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_test = test['is_Safe_ALP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6a03673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set:  0.8166666666666667\n",
      "Accuracy score on testing set:  0.9032258064516129\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "print(\"Accuracy score on training set: \", accuracy_score(y_train, y_hat_train))\n",
    "print(\"Accuracy score on testing set: \", accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e303a069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using 1 features:  0.9032258064516129\n",
      "Accuracy on test set using 2 features:  0.9032258064516129\n",
      "Accuracy on test set using 3 features:  0.9032258064516129\n",
      "Accuracy on test set using 4 features:  0.9032258064516129\n",
      "Accuracy on test set using 5 features:  0.9032258064516129\n",
      "Accuracy on test set using 6 features:  0.9032258064516129\n",
      "Accuracy on test set using 7 features:  0.9032258064516129\n",
      "Accuracy on test set using 8 features:  0.9032258064516129\n",
      "Accuracy on test set using 9 features:  0.9032258064516129\n",
      "Accuracy on test set using 10 features:  0.9032258064516129\n",
      "Accuracy on test set using 11 features:  0.9032258064516129\n",
      "Accuracy on test set using 12 features:  0.9032258064516129\n",
      "Accuracy on test set using 13 features:  0.9032258064516129\n",
      "Accuracy on test set using 14 features:  0.9032258064516129\n",
      "Accuracy on test set using 15 features:  0.9032258064516129\n",
      "Accuracy on test set using 16 features:  0.9032258064516129\n",
      "Accuracy on test set using 17 features:  0.9032258064516129\n",
      "Accuracy on test set using 18 features:  0.9032258064516129\n",
      "Accuracy on test set using 19 features:  0.9032258064516129\n",
      "Accuracy on test set using 20 features:  0.9032258064516129\n",
      "Accuracy on test set using 21 features:  0.9032258064516129\n",
      "Accuracy on test set using 22 features:  0.9032258064516129\n",
      "Accuracy on test set using 23 features:  0.9032258064516129\n",
      "Accuracy on test set using 24 features:  0.9032258064516129\n",
      "Accuracy on test set using 25 features:  0.9032258064516129\n",
      "Accuracy on test set using 26 features:  0.9032258064516129\n",
      "Accuracy on test set using 27 features:  0.9032258064516129\n",
      "Accuracy on test set using 28 features:  0.9032258064516129\n",
      "Accuracy on test set using 29 features:  0.9032258064516129\n",
      "Accuracy on test set using 30 features:  0.9032258064516129\n",
      "Accuracy on test set using 31 features:  0.9032258064516129\n",
      "Accuracy on test set using 32 features:  0.9032258064516129\n",
      "Accuracy on test set using 33 features:  0.9032258064516129\n",
      "Accuracy on test set using 34 features:  0.9032258064516129\n",
      "Accuracy on test set using 35 features:  0.9032258064516129\n",
      "Accuracy on test set using 36 features:  0.9032258064516129\n",
      "Accuracy on test set using 37 features:  0.9032258064516129\n",
      "Accuracy on test set using 38 features:  0.9032258064516129\n",
      "Accuracy on test set using 39 features:  0.9032258064516129\n",
      "Accuracy on test set using 40 features:  0.9032258064516129\n",
      "Accuracy on test set using 41 features:  0.9032258064516129\n",
      "Accuracy on test set using 42 features:  0.9032258064516129\n",
      "Accuracy on test set using 43 features:  0.9032258064516129\n",
      "Accuracy on test set using 44 features:  0.9032258064516129\n",
      "Accuracy on test set using 45 features:  0.9032258064516129\n",
      "Accuracy on test set using 46 features:  0.9032258064516129\n",
      "Accuracy on test set using 47 features:  0.9032258064516129\n",
      "Accuracy on test set using 48 features:  0.9032258064516129\n",
      "Accuracy on test set using 49 features:  0.9032258064516129\n",
      "Accuracy on test set using 50 features:  0.9032258064516129\n",
      "Accuracy on test set using 51 features:  0.9032258064516129\n",
      "Accuracy on test set using 52 features:  0.9032258064516129\n",
      "Accuracy on test set using 53 features:  0.9032258064516129\n",
      "Accuracy on test set using 54 features:  0.9032258064516129\n",
      "Accuracy on test set using 55 features:  0.9032258064516129\n",
      "Accuracy on test set using 56 features:  0.9032258064516129\n",
      "Accuracy on test set using 57 features:  0.9032258064516129\n",
      "Accuracy on test set using 58 features:  0.9032258064516129\n",
      "Accuracy on test set using 59 features:  0.9032258064516129\n",
      "Accuracy on test set using 60 features:  0.9032258064516129\n",
      "Accuracy on test set using 61 features:  0.9032258064516129\n",
      "Accuracy on test set using 62 features:  0.9032258064516129\n",
      "Accuracy on test set using 63 features:  0.9032258064516129\n",
      "Accuracy on test set using 64 features:  0.9032258064516129\n",
      "Accuracy on test set using 65 features:  0.9032258064516129\n",
      "Accuracy on test set using 66 features:  0.9032258064516129\n",
      "Accuracy on test set using 67 features:  0.9032258064516129\n",
      "Accuracy on test set using 68 features:  0.9032258064516129\n",
      "Accuracy on test set using 69 features:  0.9032258064516129\n",
      "Accuracy on test set using 70 features:  0.9032258064516129\n",
      "Accuracy on test set using 71 features:  0.9032258064516129\n",
      "Accuracy on test set using 72 features:  0.9032258064516129\n",
      "Accuracy on test set using 73 features:  0.9032258064516129\n",
      "Accuracy on test set using 74 features:  0.9032258064516129\n",
      "Accuracy on test set using 75 features:  0.9032258064516129\n",
      "Accuracy on test set using 76 features:  0.9032258064516129\n",
      "Accuracy on test set using 77 features:  0.9032258064516129\n",
      "Accuracy on test set using 78 features:  0.9032258064516129\n",
      "Accuracy on test set using 79 features:  0.9032258064516129\n",
      "Accuracy on test set using 80 features:  0.9032258064516129\n",
      "Accuracy on test set using 81 features:  0.9032258064516129\n",
      "Accuracy on test set using 82 features:  0.9032258064516129\n",
      "Accuracy on test set using 83 features:  0.9032258064516129\n",
      "Accuracy on test set using 84 features:  0.9032258064516129\n",
      "Accuracy on test set using 85 features:  0.9032258064516129\n",
      "Accuracy on test set using 86 features:  0.9032258064516129\n",
      "Accuracy on test set using 87 features:  0.9032258064516129\n",
      "Accuracy on test set using 88 features:  0.9032258064516129\n",
      "Accuracy on test set using 89 features:  0.9032258064516129\n",
      "Accuracy on test set using 90 features:  0.9032258064516129\n",
      "Accuracy on test set using 91 features:  0.9032258064516129\n",
      "Accuracy on test set using 92 features:  0.9032258064516129\n",
      "Accuracy on test set using 93 features:  0.9032258064516129\n",
      "Accuracy on test set using 94 features:  0.9032258064516129\n",
      "Accuracy on test set using 95 features:  0.9032258064516129\n",
      "Accuracy on test set using 96 features:  0.9032258064516129\n",
      "Accuracy on test set using 97 features:  0.9032258064516129\n",
      "Accuracy on test set using 98 features:  0.9032258064516129\n",
      "Accuracy on test set using 99 features:  0.9032258064516129\n",
      "Accuracy on test set using 100 features:  0.9032258064516129\n",
      "Accuracy on test set using 101 features:  0.9032258064516129\n",
      "Accuracy on test set using 102 features:  0.9032258064516129\n",
      "Accuracy on test set using 103 features:  0.9032258064516129\n",
      "Accuracy on test set using 104 features:  0.9032258064516129\n",
      "Accuracy on test set using 105 features:  0.9032258064516129\n",
      "Accuracy on test set using 106 features:  0.9032258064516129\n",
      "Accuracy on test set using 107 features:  0.9032258064516129\n",
      "Accuracy on test set using 108 features:  0.9032258064516129\n",
      "Accuracy on test set using 109 features:  0.9032258064516129\n",
      "Accuracy on test set using 110 features:  0.9032258064516129\n",
      "Accuracy on test set using 111 features:  0.9032258064516129\n",
      "Accuracy on test set using 112 features:  0.9032258064516129\n",
      "Accuracy on test set using 113 features:  0.9032258064516129\n",
      "Accuracy on test set using 114 features:  0.9032258064516129\n",
      "Accuracy on test set using 115 features:  0.9032258064516129\n",
      "Accuracy on test set using 116 features:  0.9032258064516129\n",
      "Accuracy on test set using 117 features:  0.9032258064516129\n",
      "Accuracy on test set using 118 features:  0.9032258064516129\n",
      "Accuracy on test set using 119 features:  0.9032258064516129\n",
      "Accuracy on test set using 120 features:  0.9032258064516129\n",
      "Accuracy on test set using 121 features:  0.9032258064516129\n",
      "Accuracy on test set using 122 features:  0.9032258064516129\n",
      "Accuracy on test set using 123 features:  0.9032258064516129\n",
      "Accuracy on test set using 124 features:  0.9032258064516129\n",
      "Accuracy on test set using 125 features:  0.9032258064516129\n",
      "Accuracy on test set using 126 features:  0.9032258064516129\n",
      "Accuracy on test set using 127 features:  0.9032258064516129\n",
      "Accuracy on test set using 128 features:  0.9032258064516129\n",
      "Accuracy on test set using 129 features:  0.9032258064516129\n",
      "Accuracy on test set using 130 features:  0.9032258064516129\n",
      "Accuracy on test set using 131 features:  0.9032258064516129\n",
      "Accuracy on test set using 132 features:  0.9032258064516129\n",
      "Accuracy on test set using 133 features:  0.9032258064516129\n",
      "Accuracy on test set using 134 features:  0.9032258064516129\n",
      "Accuracy on test set using 135 features:  0.9032258064516129\n",
      "Accuracy on test set using 136 features:  0.9032258064516129\n",
      "Accuracy on test set using 137 features:  0.9032258064516129\n",
      "Accuracy on test set using 138 features:  0.9032258064516129\n",
      "Accuracy on test set using 139 features:  0.9032258064516129\n",
      "1 columns provide the highest accuracy at: 90.32 %\n"
     ]
    }
   ],
   "source": [
    "acc_scores = []\n",
    "for i in range(1,140):\n",
    "    clf = LogisticRegression()\n",
    "    rfe = RFE(estimator=clf, n_features_to_select=i)\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfe.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy on test set using\", i, \"features: \", acc_score)\n",
    "    \n",
    "    acc_scores.append(acc_score)\n",
    "\n",
    "best = acc_scores[0]\n",
    "best_number_of_columns = 0;\n",
    "for i in range(len(acc_scores)):\n",
    "    if acc_scores[i] > best:\n",
    "        best = acc_scores[i]\n",
    "        best_number_of_columns = i\n",
    "        \n",
    "print(best_number_of_columns + 1, 'columns provide the highest accuracy at: %.2f' % (best*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c1eb9fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test set:  0.9032258064516129\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "rfe = RFE(estimator=lr_model, n_features_to_select=5, step=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "y_test_hat = rfe.predict(X_test)\n",
    "print(\"accuracy score on test set: \", accuracy_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "afcd5857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected False, Rank: 38.000\n",
      "Column: 1, Selected False, Rank: 114.000\n",
      "Column: 2, Selected False, Rank: 68.000\n",
      "Column: 3, Selected False, Rank: 115.000\n",
      "Column: 4, Selected False, Rank: 8.000\n",
      "Column: 5, Selected False, Rank: 6.000\n",
      "Column: 6, Selected False, Rank: 9.000\n",
      "Column: 7, Selected False, Rank: 31.000\n",
      "Column: 8, Selected False, Rank: 109.000\n",
      "Column: 9, Selected False, Rank: 61.000\n",
      "Column: 10, Selected False, Rank: 50.000\n",
      "Column: 11, Selected False, Rank: 39.000\n",
      "Column: 12, Selected False, Rank: 27.000\n",
      "Column: 13, Selected False, Rank: 20.000\n",
      "Column: 14, Selected False, Rank: 25.000\n",
      "Column: 15, Selected False, Rank: 47.000\n",
      "Column: 16, Selected False, Rank: 65.000\n",
      "Column: 17, Selected False, Rank: 73.000\n",
      "Column: 18, Selected False, Rank: 93.000\n",
      "Column: 19, Selected False, Rank: 111.000\n",
      "Column: 20, Selected False, Rank: 131.000\n",
      "Column: 21, Selected False, Rank: 41.000\n",
      "Column: 22, Selected False, Rank: 113.000\n",
      "Column: 23, Selected False, Rank: 86.000\n",
      "Column: 24, Selected False, Rank: 79.000\n",
      "Column: 25, Selected False, Rank: 4.000\n",
      "Column: 26, Selected False, Rank: 117.000\n",
      "Column: 27, Selected False, Rank: 127.000\n",
      "Column: 28, Selected False, Rank: 15.000\n",
      "Column: 29, Selected False, Rank: 44.000\n",
      "Column: 30, Selected False, Rank: 33.000\n",
      "Column: 31, Selected False, Rank: 66.000\n",
      "Column: 32, Selected False, Rank: 103.000\n",
      "Column: 33, Selected False, Rank: 116.000\n",
      "Column: 34, Selected False, Rank: 77.000\n",
      "Column: 35, Selected False, Rank: 72.000\n",
      "Column: 36, Selected False, Rank: 104.000\n",
      "Column: 37, Selected False, Rank: 34.000\n",
      "Column: 38, Selected False, Rank: 46.000\n",
      "Column: 39, Selected False, Rank: 18.000\n",
      "Column: 40, Selected False, Rank: 59.000\n",
      "Column: 41, Selected False, Rank: 97.000\n",
      "Column: 42, Selected False, Rank: 63.000\n",
      "Column: 43, Selected False, Rank: 120.000\n",
      "Column: 44, Selected False, Rank: 82.000\n",
      "Column: 45, Selected False, Rank: 99.000\n",
      "Column: 46, Selected False, Rank: 10.000\n",
      "Column: 47, Selected False, Rank: 67.000\n",
      "Column: 48, Selected False, Rank: 48.000\n",
      "Column: 49, Selected False, Rank: 90.000\n",
      "Column: 50, Selected False, Rank: 102.000\n",
      "Column: 51, Selected False, Rank: 17.000\n",
      "Column: 52, Selected False, Rank: 130.000\n",
      "Column: 53, Selected False, Rank: 56.000\n",
      "Column: 54, Selected False, Rank: 57.000\n",
      "Column: 55, Selected False, Rank: 101.000\n",
      "Column: 56, Selected False, Rank: 84.000\n",
      "Column: 57, Selected False, Rank: 58.000\n",
      "Column: 58, Selected False, Rank: 110.000\n",
      "Column: 59, Selected False, Rank: 96.000\n",
      "Column: 60, Selected False, Rank: 62.000\n",
      "Column: 61, Selected False, Rank: 32.000\n",
      "Column: 62, Selected False, Rank: 16.000\n",
      "Column: 63, Selected False, Rank: 87.000\n",
      "Column: 64, Selected False, Rank: 100.000\n",
      "Column: 65, Selected False, Rank: 78.000\n",
      "Column: 66, Selected False, Rank: 23.000\n",
      "Column: 67, Selected False, Rank: 80.000\n",
      "Column: 68, Selected False, Rank: 112.000\n",
      "Column: 69, Selected False, Rank: 40.000\n",
      "Column: 70, Selected False, Rank: 98.000\n",
      "Column: 71, Selected False, Rank: 55.000\n",
      "Column: 72, Selected False, Rank: 36.000\n",
      "Column: 73, Selected False, Rank: 83.000\n",
      "Column: 74, Selected False, Rank: 89.000\n",
      "Column: 75, Selected False, Rank: 95.000\n",
      "Column: 76, Selected False, Rank: 71.000\n",
      "Column: 77, Selected False, Rank: 30.000\n",
      "Column: 78, Selected False, Rank: 60.000\n",
      "Column: 79, Selected False, Rank: 69.000\n",
      "Column: 80, Selected False, Rank: 5.000\n",
      "Column: 81, Selected False, Rank: 37.000\n",
      "Column: 82, Selected False, Rank: 85.000\n",
      "Column: 83, Selected False, Rank: 3.000\n",
      "Column: 84, Selected False, Rank: 88.000\n",
      "Column: 85, Selected False, Rank: 13.000\n",
      "Column: 86, Selected False, Rank: 2.000\n",
      "Column: 87, Selected False, Rank: 54.000\n",
      "Column: 88, Selected False, Rank: 42.000\n",
      "Column: 89, Selected False, Rank: 108.000\n",
      "Column: 90, Selected False, Rank: 7.000\n",
      "Column: 91, Selected False, Rank: 64.000\n",
      "Column: 92, Selected False, Rank: 26.000\n",
      "Column: 93, Selected False, Rank: 12.000\n",
      "Column: 94, Selected False, Rank: 94.000\n",
      "Column: 95, Selected False, Rank: 106.000\n",
      "Column: 96, Selected False, Rank: 53.000\n",
      "Column: 97, Selected False, Rank: 43.000\n",
      "Column: 98, Selected False, Rank: 92.000\n",
      "Column: 99, Selected False, Rank: 119.000\n",
      "Column: 100, Selected False, Rank: 121.000\n",
      "Column: 101, Selected False, Rank: 126.000\n",
      "Column: 102, Selected False, Rank: 125.000\n",
      "Column: 103, Selected False, Rank: 49.000\n",
      "Column: 104, Selected False, Rank: 118.000\n",
      "Column: 105, Selected False, Rank: 133.000\n",
      "Column: 106, Selected False, Rank: 11.000\n",
      "Column: 107, Selected False, Rank: 124.000\n",
      "Column: 108, Selected False, Rank: 134.000\n",
      "Column: 109, Selected False, Rank: 70.000\n",
      "Column: 110, Selected False, Rank: 91.000\n",
      "Column: 111, Selected False, Rank: 132.000\n",
      "Column: 112, Selected False, Rank: 128.000\n",
      "Column: 113, Selected False, Rank: 107.000\n",
      "Column: 114, Selected False, Rank: 123.000\n",
      "Column: 115, Selected False, Rank: 105.000\n",
      "Column: 116, Selected False, Rank: 122.000\n",
      "Column: 117, Selected False, Rank: 74.000\n",
      "Column: 118, Selected False, Rank: 28.000\n",
      "Column: 119, Selected False, Rank: 21.000\n",
      "Column: 120, Selected False, Rank: 51.000\n",
      "Column: 121, Selected False, Rank: 81.000\n",
      "Column: 122, Selected False, Rank: 29.000\n",
      "Column: 123, Selected True, Rank: 1.000\n",
      "Column: 124, Selected True, Rank: 1.000\n",
      "Column: 125, Selected False, Rank: 14.000\n",
      "Column: 126, Selected True, Rank: 1.000\n",
      "Column: 127, Selected False, Rank: 52.000\n",
      "Column: 128, Selected False, Rank: 35.000\n",
      "Column: 129, Selected True, Rank: 1.000\n",
      "Column: 130, Selected False, Rank: 129.000\n",
      "Column: 131, Selected False, Rank: 45.000\n",
      "Column: 132, Selected False, Rank: 75.000\n",
      "Column: 133, Selected False, Rank: 76.000\n",
      "Column: 134, Selected False, Rank: 22.000\n",
      "Column: 135, Selected False, Rank: 24.000\n",
      "Column: 136, Selected True, Rank: 1.000\n",
      "Column: 137, Selected False, Rank: 19.000\n",
      "[123, 124, 126, 129, 136]\n"
     ]
    }
   ],
   "source": [
    "relevant_cols = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    if rfe.support_[i] == True:\n",
    "        relevant_cols.append(i)\n",
    "        \n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "df1ef464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [123, 124, 126, 129, 136]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent\n",
       "3         0          0         56.26\n",
       "70        0          0         58.36\n",
       "92        0          0         53.28\n",
       "124       0          0         57.52\n",
       "56        1          0         62.21\n",
       "134       0          0         54.45\n",
       "107       0          0         71.83\n",
       "78        0          0         52.98\n",
       "128       0          0         53.18\n",
       "65        0          0         52.80\n",
       "42        0          0         51.50\n",
       "123       0          0         55.04\n",
       "111       0          0         65.36\n",
       "0         0          0         58.18\n",
       "136       1          0         64.13\n",
       "98        0          0         50.19\n",
       "90        0          0         55.04\n",
       "114       0          0         64.36\n",
       "108       0          0         57.53\n",
       "131       0          0         54.24\n",
       "101       0          0         72.49\n",
       "68        0          0         70.48\n",
       "95        0          0         58.40\n",
       "119       0          0         56.39\n",
       "64        1          0         66.30\n",
       "77        0          0         62.99\n",
       "71        0          0         53.88\n",
       "150       0          0         64.58\n",
       "7         0          0         57.52\n",
       "103       0          0         55.14\n",
       "53        0          0         58.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(test['is_Safe_ALP'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(X_test))\n",
    "result.insert(2, \"Pref Percent\", test['PreferencePercent'].values)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "739805e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RELP - Buddhism</th>\n",
       "      <th>RELP - Christianity</th>\n",
       "      <th>RELP - Islam</th>\n",
       "      <th>RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation</th>\n",
       "      <th>MDCP - Not married</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.420441</td>\n",
       "      <td>-0.691813</td>\n",
       "      <td>0.851753</td>\n",
       "      <td>-0.735348</td>\n",
       "      <td>0.490631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.420441</td>\n",
       "      <td>-0.691813</td>\n",
       "      <td>0.851753</td>\n",
       "      <td>-0.735348</td>\n",
       "      <td>0.490631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RELP - Buddhism  RELP - Christianity  RELP - Islam  \\\n",
       "0         0.420441            -0.691813      0.851753   \n",
       "1         0.420441            -0.691813      0.851753   \n",
       "\n",
       "   RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation  \\\n",
       "0                                          -0.735348                                 \n",
       "1                                          -0.735348                                 \n",
       "\n",
       "   MDCP - Not married  \n",
       "0            0.490631  \n",
       "1            0.490631  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_DF = pd.DataFrame(list(map(np.ravel, rfe.estimator_.coef_)), columns=X_train.columns[rfe.support_], index=rfe.estimator_.classes_)\n",
    "coefficient_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0aabe",
   "metadata": {},
   "source": [
    "# 4.4) Safe ALP (90% Accurate)\n",
    "\n",
    "## Strongest Determinants\n",
    "- Islam (0.851753)\n",
    "- Not married (0.490631)\n",
    "- Buddhism (0.420441)\n",
    "\n",
    "## Negative determinants\n",
    "- Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation (-0.735348)\n",
    "- Christianity (-0.691813)\n",
    "\n",
    "Interesting how religion is so important in determining whether an electorate is a safe ALP seat or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05638b",
   "metadata": {},
   "source": [
    "# 4.5) Safe LNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "900fb7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 152)\n",
      "(31, 152)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(adjusted_master, test_size=0.2, random_state=142)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7fa9a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_train = train['is_Safe_LNP']\n",
    "X_test = test.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_test = test['is_Safe_LNP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d7d24f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set:  0.725\n",
      "Accuracy score on testing set:  0.8064516129032258\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "print(\"Accuracy score on training set: \", accuracy_score(y_train, y_hat_train))\n",
    "print(\"Accuracy score on testing set: \", accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4feba406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using 1 features:  0.8064516129032258\n",
      "Accuracy on test set using 2 features:  0.8064516129032258\n",
      "Accuracy on test set using 3 features:  0.8064516129032258\n",
      "Accuracy on test set using 4 features:  0.8064516129032258\n",
      "Accuracy on test set using 5 features:  0.8064516129032258\n",
      "Accuracy on test set using 6 features:  0.8064516129032258\n",
      "Accuracy on test set using 7 features:  0.8064516129032258\n",
      "Accuracy on test set using 8 features:  0.8064516129032258\n",
      "Accuracy on test set using 9 features:  0.8064516129032258\n",
      "Accuracy on test set using 10 features:  0.8064516129032258\n",
      "Accuracy on test set using 11 features:  0.8064516129032258\n",
      "Accuracy on test set using 12 features:  0.8064516129032258\n",
      "Accuracy on test set using 13 features:  0.8064516129032258\n",
      "Accuracy on test set using 14 features:  0.8064516129032258\n",
      "Accuracy on test set using 15 features:  0.8064516129032258\n",
      "Accuracy on test set using 16 features:  0.8064516129032258\n",
      "Accuracy on test set using 17 features:  0.8064516129032258\n",
      "Accuracy on test set using 18 features:  0.8064516129032258\n",
      "Accuracy on test set using 19 features:  0.8064516129032258\n",
      "Accuracy on test set using 20 features:  0.8064516129032258\n",
      "Accuracy on test set using 21 features:  0.8064516129032258\n",
      "Accuracy on test set using 22 features:  0.8064516129032258\n",
      "Accuracy on test set using 23 features:  0.8064516129032258\n",
      "Accuracy on test set using 24 features:  0.8064516129032258\n",
      "Accuracy on test set using 25 features:  0.8064516129032258\n",
      "Accuracy on test set using 26 features:  0.8064516129032258\n",
      "Accuracy on test set using 27 features:  0.8064516129032258\n",
      "Accuracy on test set using 28 features:  0.8064516129032258\n",
      "Accuracy on test set using 29 features:  0.8064516129032258\n",
      "Accuracy on test set using 30 features:  0.8064516129032258\n",
      "Accuracy on test set using 31 features:  0.8064516129032258\n",
      "Accuracy on test set using 32 features:  0.8064516129032258\n",
      "Accuracy on test set using 33 features:  0.8064516129032258\n",
      "Accuracy on test set using 34 features:  0.8064516129032258\n",
      "Accuracy on test set using 35 features:  0.8064516129032258\n",
      "Accuracy on test set using 36 features:  0.8064516129032258\n",
      "Accuracy on test set using 37 features:  0.8064516129032258\n",
      "Accuracy on test set using 38 features:  0.8064516129032258\n",
      "Accuracy on test set using 39 features:  0.8064516129032258\n",
      "Accuracy on test set using 40 features:  0.8064516129032258\n",
      "Accuracy on test set using 41 features:  0.8064516129032258\n",
      "Accuracy on test set using 42 features:  0.8064516129032258\n",
      "Accuracy on test set using 43 features:  0.8064516129032258\n",
      "Accuracy on test set using 44 features:  0.8064516129032258\n",
      "Accuracy on test set using 45 features:  0.8064516129032258\n",
      "Accuracy on test set using 46 features:  0.8064516129032258\n",
      "Accuracy on test set using 47 features:  0.8064516129032258\n",
      "Accuracy on test set using 48 features:  0.8064516129032258\n",
      "Accuracy on test set using 49 features:  0.8064516129032258\n",
      "Accuracy on test set using 50 features:  0.8064516129032258\n",
      "Accuracy on test set using 51 features:  0.8064516129032258\n",
      "Accuracy on test set using 52 features:  0.8064516129032258\n",
      "Accuracy on test set using 53 features:  0.8064516129032258\n",
      "Accuracy on test set using 54 features:  0.8064516129032258\n",
      "Accuracy on test set using 55 features:  0.8064516129032258\n",
      "Accuracy on test set using 56 features:  0.8064516129032258\n",
      "Accuracy on test set using 57 features:  0.8064516129032258\n",
      "Accuracy on test set using 58 features:  0.8064516129032258\n",
      "Accuracy on test set using 59 features:  0.8064516129032258\n",
      "Accuracy on test set using 60 features:  0.8064516129032258\n",
      "Accuracy on test set using 61 features:  0.8064516129032258\n",
      "Accuracy on test set using 62 features:  0.8064516129032258\n",
      "Accuracy on test set using 63 features:  0.8064516129032258\n",
      "Accuracy on test set using 64 features:  0.8064516129032258\n",
      "Accuracy on test set using 65 features:  0.8064516129032258\n",
      "Accuracy on test set using 66 features:  0.8064516129032258\n",
      "Accuracy on test set using 67 features:  0.8064516129032258\n",
      "Accuracy on test set using 68 features:  0.8064516129032258\n",
      "Accuracy on test set using 69 features:  0.8064516129032258\n",
      "Accuracy on test set using 70 features:  0.8064516129032258\n",
      "Accuracy on test set using 71 features:  0.8064516129032258\n",
      "Accuracy on test set using 72 features:  0.8064516129032258\n",
      "Accuracy on test set using 73 features:  0.8064516129032258\n",
      "Accuracy on test set using 74 features:  0.8064516129032258\n",
      "Accuracy on test set using 75 features:  0.8064516129032258\n",
      "Accuracy on test set using 76 features:  0.8064516129032258\n",
      "Accuracy on test set using 77 features:  0.8064516129032258\n",
      "Accuracy on test set using 78 features:  0.8064516129032258\n",
      "Accuracy on test set using 79 features:  0.8064516129032258\n",
      "Accuracy on test set using 80 features:  0.8064516129032258\n",
      "Accuracy on test set using 81 features:  0.8064516129032258\n",
      "Accuracy on test set using 82 features:  0.8064516129032258\n",
      "Accuracy on test set using 83 features:  0.8064516129032258\n",
      "Accuracy on test set using 84 features:  0.8064516129032258\n",
      "Accuracy on test set using 85 features:  0.8064516129032258\n",
      "Accuracy on test set using 86 features:  0.8064516129032258\n",
      "Accuracy on test set using 87 features:  0.8064516129032258\n",
      "Accuracy on test set using 88 features:  0.8064516129032258\n",
      "Accuracy on test set using 89 features:  0.8064516129032258\n",
      "Accuracy on test set using 90 features:  0.8064516129032258\n",
      "Accuracy on test set using 91 features:  0.8064516129032258\n",
      "Accuracy on test set using 92 features:  0.8064516129032258\n",
      "Accuracy on test set using 93 features:  0.8064516129032258\n",
      "Accuracy on test set using 94 features:  0.8064516129032258\n",
      "Accuracy on test set using 95 features:  0.8064516129032258\n",
      "Accuracy on test set using 96 features:  0.8064516129032258\n",
      "Accuracy on test set using 97 features:  0.8064516129032258\n",
      "Accuracy on test set using 98 features:  0.8064516129032258\n",
      "Accuracy on test set using 99 features:  0.8064516129032258\n",
      "Accuracy on test set using 100 features:  0.8064516129032258\n",
      "Accuracy on test set using 101 features:  0.8064516129032258\n",
      "Accuracy on test set using 102 features:  0.8064516129032258\n",
      "Accuracy on test set using 103 features:  0.8064516129032258\n",
      "Accuracy on test set using 104 features:  0.8064516129032258\n",
      "Accuracy on test set using 105 features:  0.8064516129032258\n",
      "Accuracy on test set using 106 features:  0.8064516129032258\n",
      "Accuracy on test set using 107 features:  0.8064516129032258\n",
      "Accuracy on test set using 108 features:  0.8064516129032258\n",
      "Accuracy on test set using 109 features:  0.8064516129032258\n",
      "Accuracy on test set using 110 features:  0.8064516129032258\n",
      "Accuracy on test set using 111 features:  0.8064516129032258\n",
      "Accuracy on test set using 112 features:  0.8064516129032258\n",
      "Accuracy on test set using 113 features:  0.8064516129032258\n",
      "Accuracy on test set using 114 features:  0.8064516129032258\n",
      "Accuracy on test set using 115 features:  0.8064516129032258\n",
      "Accuracy on test set using 116 features:  0.8064516129032258\n",
      "Accuracy on test set using 117 features:  0.8064516129032258\n",
      "Accuracy on test set using 118 features:  0.8064516129032258\n",
      "Accuracy on test set using 119 features:  0.8064516129032258\n",
      "Accuracy on test set using 120 features:  0.8064516129032258\n",
      "Accuracy on test set using 121 features:  0.8064516129032258\n",
      "Accuracy on test set using 122 features:  0.8064516129032258\n",
      "Accuracy on test set using 123 features:  0.8064516129032258\n",
      "Accuracy on test set using 124 features:  0.8064516129032258\n",
      "Accuracy on test set using 125 features:  0.8064516129032258\n",
      "Accuracy on test set using 126 features:  0.8064516129032258\n",
      "Accuracy on test set using 127 features:  0.8064516129032258\n",
      "Accuracy on test set using 128 features:  0.8064516129032258\n",
      "Accuracy on test set using 129 features:  0.8064516129032258\n",
      "Accuracy on test set using 130 features:  0.8064516129032258\n",
      "Accuracy on test set using 131 features:  0.8064516129032258\n",
      "Accuracy on test set using 132 features:  0.8064516129032258\n",
      "Accuracy on test set using 133 features:  0.8064516129032258\n",
      "Accuracy on test set using 134 features:  0.8064516129032258\n",
      "Accuracy on test set using 135 features:  0.8064516129032258\n",
      "Accuracy on test set using 136 features:  0.8064516129032258\n",
      "Accuracy on test set using 137 features:  0.8064516129032258\n",
      "Accuracy on test set using 138 features:  0.8064516129032258\n",
      "Accuracy on test set using 139 features:  0.8064516129032258\n",
      "1 columns provide the highest accuracy at: 80.65 %\n"
     ]
    }
   ],
   "source": [
    "acc_scores = []\n",
    "for i in range(1,140):\n",
    "    clf = LogisticRegression()\n",
    "    rfe = RFE(estimator=clf, n_features_to_select=i)\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfe.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy on test set using\", i, \"features: \", acc_score)\n",
    "    \n",
    "    acc_scores.append(acc_score)\n",
    "\n",
    "best = acc_scores[0]\n",
    "best_number_of_columns = 0;\n",
    "for i in range(len(acc_scores)):\n",
    "    if acc_scores[i] > best:\n",
    "        best = acc_scores[i]\n",
    "        best_number_of_columns = i\n",
    "        \n",
    "print(best_number_of_columns + 1, 'columns provide the highest accuracy at: %.2f' % (best*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "21dbb824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test set:  0.8064516129032258\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "rfe = RFE(estimator=lr_model, n_features_to_select=10, step=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "y_test_hat = rfe.predict(X_test)\n",
    "print(\"accuracy score on test set: \", accuracy_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0d52c4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected False, Rank: 30.000\n",
      "Column: 1, Selected False, Rank: 81.000\n",
      "Column: 2, Selected False, Rank: 39.000\n",
      "Column: 3, Selected False, Rank: 76.000\n",
      "Column: 4, Selected False, Rank: 7.000\n",
      "Column: 5, Selected True, Rank: 1.000\n",
      "Column: 6, Selected True, Rank: 1.000\n",
      "Column: 7, Selected False, Rank: 18.000\n",
      "Column: 8, Selected False, Rank: 59.000\n",
      "Column: 9, Selected False, Rank: 72.000\n",
      "Column: 10, Selected False, Rank: 35.000\n",
      "Column: 11, Selected False, Rank: 25.000\n",
      "Column: 12, Selected False, Rank: 22.000\n",
      "Column: 13, Selected False, Rank: 13.000\n",
      "Column: 14, Selected False, Rank: 21.000\n",
      "Column: 15, Selected False, Rank: 28.000\n",
      "Column: 16, Selected False, Rank: 46.000\n",
      "Column: 17, Selected False, Rank: 65.000\n",
      "Column: 18, Selected False, Rank: 88.000\n",
      "Column: 19, Selected False, Rank: 104.000\n",
      "Column: 20, Selected False, Rank: 121.000\n",
      "Column: 21, Selected False, Rank: 6.000\n",
      "Column: 22, Selected False, Rank: 54.000\n",
      "Column: 23, Selected True, Rank: 1.000\n",
      "Column: 24, Selected False, Rank: 60.000\n",
      "Column: 25, Selected True, Rank: 1.000\n",
      "Column: 26, Selected False, Rank: 16.000\n",
      "Column: 27, Selected False, Rank: 128.000\n",
      "Column: 28, Selected False, Rank: 64.000\n",
      "Column: 29, Selected False, Rank: 56.000\n",
      "Column: 30, Selected False, Rank: 12.000\n",
      "Column: 31, Selected False, Rank: 107.000\n",
      "Column: 32, Selected False, Rank: 37.000\n",
      "Column: 33, Selected False, Rank: 43.000\n",
      "Column: 34, Selected False, Rank: 62.000\n",
      "Column: 35, Selected False, Rank: 99.000\n",
      "Column: 36, Selected False, Rank: 119.000\n",
      "Column: 37, Selected False, Rank: 67.000\n",
      "Column: 38, Selected False, Rank: 75.000\n",
      "Column: 39, Selected False, Rank: 38.000\n",
      "Column: 40, Selected False, Rank: 41.000\n",
      "Column: 41, Selected False, Rank: 42.000\n",
      "Column: 42, Selected False, Rank: 45.000\n",
      "Column: 43, Selected False, Rank: 86.000\n",
      "Column: 44, Selected False, Rank: 55.000\n",
      "Column: 45, Selected False, Rank: 110.000\n",
      "Column: 46, Selected False, Rank: 14.000\n",
      "Column: 47, Selected False, Rank: 113.000\n",
      "Column: 48, Selected False, Rank: 26.000\n",
      "Column: 49, Selected False, Rank: 78.000\n",
      "Column: 50, Selected False, Rank: 74.000\n",
      "Column: 51, Selected True, Rank: 1.000\n",
      "Column: 52, Selected False, Rank: 77.000\n",
      "Column: 53, Selected False, Rank: 100.000\n",
      "Column: 54, Selected False, Rank: 124.000\n",
      "Column: 55, Selected False, Rank: 115.000\n",
      "Column: 56, Selected False, Rank: 44.000\n",
      "Column: 57, Selected False, Rank: 73.000\n",
      "Column: 58, Selected False, Rank: 91.000\n",
      "Column: 59, Selected False, Rank: 68.000\n",
      "Column: 60, Selected False, Rank: 63.000\n",
      "Column: 61, Selected False, Rank: 17.000\n",
      "Column: 62, Selected False, Rank: 15.000\n",
      "Column: 63, Selected False, Rank: 98.000\n",
      "Column: 64, Selected False, Rank: 120.000\n",
      "Column: 65, Selected False, Rank: 27.000\n",
      "Column: 66, Selected False, Rank: 23.000\n",
      "Column: 67, Selected False, Rank: 112.000\n",
      "Column: 68, Selected False, Rank: 125.000\n",
      "Column: 69, Selected False, Rank: 36.000\n",
      "Column: 70, Selected False, Rank: 94.000\n",
      "Column: 71, Selected False, Rank: 10.000\n",
      "Column: 72, Selected False, Rank: 93.000\n",
      "Column: 73, Selected False, Rank: 83.000\n",
      "Column: 74, Selected False, Rank: 103.000\n",
      "Column: 75, Selected False, Rank: 87.000\n",
      "Column: 76, Selected False, Rank: 61.000\n",
      "Column: 77, Selected False, Rank: 24.000\n",
      "Column: 78, Selected False, Rank: 40.000\n",
      "Column: 79, Selected False, Rank: 111.000\n",
      "Column: 80, Selected False, Rank: 11.000\n",
      "Column: 81, Selected False, Rank: 69.000\n",
      "Column: 82, Selected False, Rank: 80.000\n",
      "Column: 83, Selected False, Rank: 52.000\n",
      "Column: 84, Selected False, Rank: 97.000\n",
      "Column: 85, Selected False, Rank: 20.000\n",
      "Column: 86, Selected True, Rank: 1.000\n",
      "Column: 87, Selected False, Rank: 32.000\n",
      "Column: 88, Selected False, Rank: 53.000\n",
      "Column: 89, Selected False, Rank: 89.000\n",
      "Column: 90, Selected True, Rank: 1.000\n",
      "Column: 91, Selected False, Rank: 108.000\n",
      "Column: 92, Selected False, Rank: 4.000\n",
      "Column: 93, Selected False, Rank: 90.000\n",
      "Column: 94, Selected False, Rank: 85.000\n",
      "Column: 95, Selected False, Rank: 96.000\n",
      "Column: 96, Selected False, Rank: 58.000\n",
      "Column: 97, Selected False, Rank: 66.000\n",
      "Column: 98, Selected False, Rank: 84.000\n",
      "Column: 99, Selected False, Rank: 106.000\n",
      "Column: 100, Selected False, Rank: 109.000\n",
      "Column: 101, Selected False, Rank: 116.000\n",
      "Column: 102, Selected False, Rank: 122.000\n",
      "Column: 103, Selected False, Rank: 47.000\n",
      "Column: 104, Selected False, Rank: 118.000\n",
      "Column: 105, Selected False, Rank: 127.000\n",
      "Column: 106, Selected False, Rank: 31.000\n",
      "Column: 107, Selected False, Rank: 117.000\n",
      "Column: 108, Selected False, Rank: 129.000\n",
      "Column: 109, Selected False, Rank: 48.000\n",
      "Column: 110, Selected False, Rank: 92.000\n",
      "Column: 111, Selected False, Rank: 123.000\n",
      "Column: 112, Selected False, Rank: 126.000\n",
      "Column: 113, Selected False, Rank: 102.000\n",
      "Column: 114, Selected False, Rank: 114.000\n",
      "Column: 115, Selected False, Rank: 101.000\n",
      "Column: 116, Selected False, Rank: 105.000\n",
      "Column: 117, Selected False, Rank: 50.000\n",
      "Column: 118, Selected False, Rank: 8.000\n",
      "Column: 119, Selected False, Rank: 51.000\n",
      "Column: 120, Selected False, Rank: 34.000\n",
      "Column: 121, Selected False, Rank: 82.000\n",
      "Column: 122, Selected False, Rank: 19.000\n",
      "Column: 123, Selected False, Rank: 5.000\n",
      "Column: 124, Selected True, Rank: 1.000\n",
      "Column: 125, Selected False, Rank: 9.000\n",
      "Column: 126, Selected True, Rank: 1.000\n",
      "Column: 127, Selected False, Rank: 57.000\n",
      "Column: 128, Selected False, Rank: 33.000\n",
      "Column: 129, Selected False, Rank: 29.000\n",
      "Column: 130, Selected False, Rank: 95.000\n",
      "Column: 131, Selected False, Rank: 49.000\n",
      "Column: 132, Selected False, Rank: 71.000\n",
      "Column: 133, Selected False, Rank: 70.000\n",
      "Column: 134, Selected False, Rank: 3.000\n",
      "Column: 135, Selected False, Rank: 79.000\n",
      "Column: 136, Selected True, Rank: 1.000\n",
      "Column: 137, Selected False, Rank: 2.000\n",
      "[5, 6, 23, 25, 51, 86, 90, 124, 126, 136]\n"
     ]
    }
   ],
   "source": [
    "relevant_cols = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    if rfe.support_[i] == True:\n",
    "        relevant_cols.append(i)\n",
    "        \n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "20f28de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [5, 6, 23, 25, 51, 86, 90, 124, 126, 136]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent\n",
       "3         0          0         56.26\n",
       "70        0          0         58.36\n",
       "92        0          0         53.28\n",
       "124       0          0         57.52\n",
       "56        0          0         62.21\n",
       "134       0          0         54.45\n",
       "107       0          0         71.83\n",
       "78        0          0         52.98\n",
       "128       0          0         53.18\n",
       "65        0          0         52.80\n",
       "42        0          0         51.50\n",
       "123       0          0         55.04\n",
       "111       1          0         65.36\n",
       "0         0          0         58.18\n",
       "136       0          0         64.13\n",
       "98        0          0         50.19\n",
       "90        0          0         55.04\n",
       "114       1          0         64.36\n",
       "108       0          0         57.53\n",
       "131       0          0         54.24\n",
       "101       1          0         72.49\n",
       "68        1          0         70.48\n",
       "95        0          0         58.40\n",
       "119       0          0         56.39\n",
       "64        0          0         66.30\n",
       "77        1          0         62.99\n",
       "71        0          0         53.88\n",
       "150       1          0         64.58\n",
       "7         0          0         57.52\n",
       "103       0          0         55.14\n",
       "53        0          0         58.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(test['is_Safe_LNP'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(X_test))\n",
    "result.insert(2, \"Pref Percent\", test['PreferencePercent'].values)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cecc4feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE5P - 25-29 years</th>\n",
       "      <th>AGE5P - 30-34 years</th>\n",
       "      <th>HEAP - Bachelor Degree Level</th>\n",
       "      <th>HEAP - Certificate III &amp; IV Level</th>\n",
       "      <th>INDP - Agriculture, Forestry and Fishing</th>\n",
       "      <th>MSTP - Never married</th>\n",
       "      <th>MSTP - Married</th>\n",
       "      <th>RELP - Christianity</th>\n",
       "      <th>RELP - Islam</th>\n",
       "      <th>MDCP - Not married</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.380901</td>\n",
       "      <td>-0.375461</td>\n",
       "      <td>-0.525477</td>\n",
       "      <td>0.465158</td>\n",
       "      <td>0.384531</td>\n",
       "      <td>-0.738896</td>\n",
       "      <td>0.500072</td>\n",
       "      <td>1.403129</td>\n",
       "      <td>-0.560254</td>\n",
       "      <td>-0.657768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.380901</td>\n",
       "      <td>-0.375461</td>\n",
       "      <td>-0.525477</td>\n",
       "      <td>0.465158</td>\n",
       "      <td>0.384531</td>\n",
       "      <td>-0.738896</td>\n",
       "      <td>0.500072</td>\n",
       "      <td>1.403129</td>\n",
       "      <td>-0.560254</td>\n",
       "      <td>-0.657768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGE5P - 25-29 years  AGE5P - 30-34 years  HEAP - Bachelor Degree Level  \\\n",
       "0            -0.380901            -0.375461                     -0.525477   \n",
       "1            -0.380901            -0.375461                     -0.525477   \n",
       "\n",
       "   HEAP - Certificate III & IV Level  \\\n",
       "0                           0.465158   \n",
       "1                           0.465158   \n",
       "\n",
       "   INDP - Agriculture, Forestry and Fishing  MSTP - Never married  \\\n",
       "0                                  0.384531             -0.738896   \n",
       "1                                  0.384531             -0.738896   \n",
       "\n",
       "   MSTP - Married  RELP - Christianity  RELP - Islam  MDCP - Not married  \n",
       "0        0.500072             1.403129     -0.560254           -0.657768  \n",
       "1        0.500072             1.403129     -0.560254           -0.657768  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_DF = pd.DataFrame(list(map(np.ravel, rfe.estimator_.coef_)), columns=X_train.columns[rfe.support_], index=rfe.estimator_.classes_)\n",
    "coefficient_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f4ff8",
   "metadata": {},
   "source": [
    "# 4.5) Safe LNP Interpretation (81% Accuracy)\n",
    "\n",
    "## Main determinants\n",
    "- Christianity (1.438735)\n",
    "- Married (0.500072)\n",
    "\n",
    "## Negative determinants\n",
    "- Never married (-0.772616)\n",
    "- Not married (-0.675473)\n",
    "- Islam (-0.571603)\n",
    "- Bachelor Degree Level (-0.554918)\n",
    "\n",
    "The change of relatively higher percentages of those with their highest level of education being a Bachelors degree being a positive for Fairly safe LNP seat to a very strong negative for Safe LNP seat is interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31922b8",
   "metadata": {},
   "source": [
    "# 4.6) Other Model (strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8e058f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 152)\n",
      "(31, 152)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(adjusted_master, test_size=0.2, random_state=142)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "33aa3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_train = train['is_Strength_Other']\n",
    "X_test = test.drop(['index', 'LGA','PartyNm', 'PreferencePercent', 'Strength', 'is_ALP', 'is_LNP', 'is_Other', 'is_Marginal',\n",
    "                     'is_Fairly_Safe_ALP', 'is_Fairly_Safe_LNP', 'is_Safe_ALP', 'is_Safe_LNP', 'is_Strength_Other'], axis=1)\n",
    "y_test = test['is_Strength_Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "16563f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set:  0.975\n",
      "Accuracy score on testing set:  0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "print(\"Accuracy score on training set: \", accuracy_score(y_train, y_hat_train))\n",
    "print(\"Accuracy score on testing set: \", accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0dc481d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using 1 features:  0.967741935483871\n",
      "Accuracy on test set using 2 features:  0.967741935483871\n",
      "Accuracy on test set using 3 features:  0.967741935483871\n",
      "Accuracy on test set using 4 features:  0.967741935483871\n",
      "Accuracy on test set using 5 features:  0.967741935483871\n",
      "Accuracy on test set using 6 features:  0.967741935483871\n",
      "Accuracy on test set using 7 features:  0.967741935483871\n",
      "Accuracy on test set using 8 features:  0.967741935483871\n",
      "Accuracy on test set using 9 features:  0.967741935483871\n",
      "Accuracy on test set using 10 features:  0.967741935483871\n",
      "Accuracy on test set using 11 features:  0.967741935483871\n",
      "Accuracy on test set using 12 features:  0.967741935483871\n",
      "Accuracy on test set using 13 features:  0.967741935483871\n",
      "Accuracy on test set using 14 features:  0.967741935483871\n",
      "Accuracy on test set using 15 features:  0.967741935483871\n",
      "Accuracy on test set using 16 features:  0.967741935483871\n",
      "Accuracy on test set using 17 features:  0.967741935483871\n",
      "Accuracy on test set using 18 features:  0.967741935483871\n",
      "Accuracy on test set using 19 features:  0.967741935483871\n",
      "Accuracy on test set using 20 features:  0.967741935483871\n",
      "Accuracy on test set using 21 features:  0.967741935483871\n",
      "Accuracy on test set using 22 features:  0.967741935483871\n",
      "Accuracy on test set using 23 features:  0.967741935483871\n",
      "Accuracy on test set using 24 features:  0.967741935483871\n",
      "Accuracy on test set using 25 features:  0.967741935483871\n",
      "Accuracy on test set using 26 features:  0.967741935483871\n",
      "Accuracy on test set using 27 features:  0.967741935483871\n",
      "Accuracy on test set using 28 features:  0.967741935483871\n",
      "Accuracy on test set using 29 features:  0.967741935483871\n",
      "Accuracy on test set using 30 features:  0.967741935483871\n",
      "Accuracy on test set using 31 features:  0.967741935483871\n",
      "Accuracy on test set using 32 features:  0.967741935483871\n",
      "Accuracy on test set using 33 features:  0.967741935483871\n",
      "Accuracy on test set using 34 features:  0.967741935483871\n",
      "Accuracy on test set using 35 features:  0.967741935483871\n",
      "Accuracy on test set using 36 features:  0.967741935483871\n",
      "Accuracy on test set using 37 features:  0.967741935483871\n",
      "Accuracy on test set using 38 features:  0.967741935483871\n",
      "Accuracy on test set using 39 features:  0.967741935483871\n",
      "Accuracy on test set using 40 features:  0.967741935483871\n",
      "Accuracy on test set using 41 features:  0.967741935483871\n",
      "Accuracy on test set using 42 features:  0.967741935483871\n",
      "Accuracy on test set using 43 features:  0.967741935483871\n",
      "Accuracy on test set using 44 features:  0.967741935483871\n",
      "Accuracy on test set using 45 features:  0.967741935483871\n",
      "Accuracy on test set using 46 features:  0.967741935483871\n",
      "Accuracy on test set using 47 features:  0.967741935483871\n",
      "Accuracy on test set using 48 features:  0.967741935483871\n",
      "Accuracy on test set using 49 features:  0.967741935483871\n",
      "Accuracy on test set using 50 features:  0.967741935483871\n",
      "Accuracy on test set using 51 features:  0.967741935483871\n",
      "Accuracy on test set using 52 features:  0.967741935483871\n",
      "Accuracy on test set using 53 features:  0.967741935483871\n",
      "Accuracy on test set using 54 features:  0.967741935483871\n",
      "Accuracy on test set using 55 features:  0.967741935483871\n",
      "Accuracy on test set using 56 features:  0.967741935483871\n",
      "Accuracy on test set using 57 features:  0.967741935483871\n",
      "Accuracy on test set using 58 features:  0.967741935483871\n",
      "Accuracy on test set using 59 features:  0.967741935483871\n",
      "Accuracy on test set using 60 features:  0.967741935483871\n",
      "Accuracy on test set using 61 features:  0.967741935483871\n",
      "Accuracy on test set using 62 features:  0.967741935483871\n",
      "Accuracy on test set using 63 features:  0.967741935483871\n",
      "Accuracy on test set using 64 features:  0.967741935483871\n",
      "Accuracy on test set using 65 features:  0.967741935483871\n",
      "Accuracy on test set using 66 features:  0.967741935483871\n",
      "Accuracy on test set using 67 features:  0.967741935483871\n",
      "Accuracy on test set using 68 features:  0.967741935483871\n",
      "Accuracy on test set using 69 features:  0.967741935483871\n",
      "Accuracy on test set using 70 features:  0.967741935483871\n",
      "Accuracy on test set using 71 features:  0.967741935483871\n",
      "Accuracy on test set using 72 features:  0.967741935483871\n",
      "Accuracy on test set using 73 features:  0.967741935483871\n",
      "Accuracy on test set using 74 features:  0.967741935483871\n",
      "Accuracy on test set using 75 features:  0.967741935483871\n",
      "Accuracy on test set using 76 features:  0.967741935483871\n",
      "Accuracy on test set using 77 features:  0.967741935483871\n",
      "Accuracy on test set using 78 features:  0.967741935483871\n",
      "Accuracy on test set using 79 features:  0.967741935483871\n",
      "Accuracy on test set using 80 features:  0.967741935483871\n",
      "Accuracy on test set using 81 features:  0.967741935483871\n",
      "Accuracy on test set using 82 features:  0.967741935483871\n",
      "Accuracy on test set using 83 features:  0.967741935483871\n",
      "Accuracy on test set using 84 features:  0.967741935483871\n",
      "Accuracy on test set using 85 features:  0.967741935483871\n",
      "Accuracy on test set using 86 features:  0.967741935483871\n",
      "Accuracy on test set using 87 features:  0.967741935483871\n",
      "Accuracy on test set using 88 features:  0.967741935483871\n",
      "Accuracy on test set using 89 features:  0.967741935483871\n",
      "Accuracy on test set using 90 features:  0.967741935483871\n",
      "Accuracy on test set using 91 features:  0.967741935483871\n",
      "Accuracy on test set using 92 features:  0.967741935483871\n",
      "Accuracy on test set using 93 features:  0.967741935483871\n",
      "Accuracy on test set using 94 features:  0.967741935483871\n",
      "Accuracy on test set using 95 features:  0.967741935483871\n",
      "Accuracy on test set using 96 features:  0.967741935483871\n",
      "Accuracy on test set using 97 features:  0.967741935483871\n",
      "Accuracy on test set using 98 features:  0.967741935483871\n",
      "Accuracy on test set using 99 features:  0.967741935483871\n",
      "Accuracy on test set using 100 features:  0.967741935483871\n",
      "Accuracy on test set using 101 features:  0.967741935483871\n",
      "Accuracy on test set using 102 features:  0.967741935483871\n",
      "Accuracy on test set using 103 features:  0.967741935483871\n",
      "Accuracy on test set using 104 features:  0.967741935483871\n",
      "Accuracy on test set using 105 features:  0.967741935483871\n",
      "Accuracy on test set using 106 features:  0.967741935483871\n",
      "Accuracy on test set using 107 features:  0.967741935483871\n",
      "Accuracy on test set using 108 features:  0.967741935483871\n",
      "Accuracy on test set using 109 features:  0.967741935483871\n",
      "Accuracy on test set using 110 features:  0.967741935483871\n",
      "Accuracy on test set using 111 features:  0.967741935483871\n",
      "Accuracy on test set using 112 features:  0.967741935483871\n",
      "Accuracy on test set using 113 features:  0.967741935483871\n",
      "Accuracy on test set using 114 features:  0.967741935483871\n",
      "Accuracy on test set using 115 features:  0.967741935483871\n",
      "Accuracy on test set using 116 features:  0.967741935483871\n",
      "Accuracy on test set using 117 features:  0.967741935483871\n",
      "Accuracy on test set using 118 features:  0.967741935483871\n",
      "Accuracy on test set using 119 features:  0.967741935483871\n",
      "Accuracy on test set using 120 features:  0.967741935483871\n",
      "Accuracy on test set using 121 features:  0.967741935483871\n",
      "Accuracy on test set using 122 features:  0.967741935483871\n",
      "Accuracy on test set using 123 features:  0.967741935483871\n",
      "Accuracy on test set using 124 features:  0.967741935483871\n",
      "Accuracy on test set using 125 features:  0.967741935483871\n",
      "Accuracy on test set using 126 features:  0.967741935483871\n",
      "Accuracy on test set using 127 features:  0.967741935483871\n",
      "Accuracy on test set using 128 features:  0.967741935483871\n",
      "Accuracy on test set using 129 features:  0.967741935483871\n",
      "Accuracy on test set using 130 features:  0.967741935483871\n",
      "Accuracy on test set using 131 features:  0.967741935483871\n",
      "Accuracy on test set using 132 features:  0.967741935483871\n",
      "Accuracy on test set using 133 features:  0.967741935483871\n",
      "Accuracy on test set using 134 features:  0.967741935483871\n",
      "Accuracy on test set using 135 features:  0.967741935483871\n",
      "Accuracy on test set using 136 features:  0.967741935483871\n",
      "Accuracy on test set using 137 features:  0.967741935483871\n",
      "Accuracy on test set using 138 features:  0.967741935483871\n",
      "Accuracy on test set using 139 features:  0.967741935483871\n",
      "1 columns provide the highest accuracy at: 96.77 %\n"
     ]
    }
   ],
   "source": [
    "acc_scores = []\n",
    "for i in range(1,140):\n",
    "    clf = LogisticRegression()\n",
    "    rfe = RFE(estimator=clf, n_features_to_select=i)\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfe.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy on test set using\", i, \"features: \", acc_score)\n",
    "    \n",
    "    acc_scores.append(acc_score)\n",
    "\n",
    "best = acc_scores[0]\n",
    "best_number_of_columns = 0;\n",
    "for i in range(len(acc_scores)):\n",
    "    if acc_scores[i] > best:\n",
    "        best = acc_scores[i]\n",
    "        best_number_of_columns = i\n",
    "        \n",
    "print(best_number_of_columns + 1, 'columns provide the highest accuracy at: %.2f' % (best*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "30f909e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test set:  0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "rfe = RFE(estimator=lr_model, n_features_to_select=5, step=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "y_test_hat = rfe.predict(X_test)\n",
    "print(\"accuracy score on test set: \", accuracy_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3e08f188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected False, Rank: 63.000\n",
      "Column: 1, Selected False, Rank: 72.000\n",
      "Column: 2, Selected False, Rank: 65.000\n",
      "Column: 3, Selected False, Rank: 56.000\n",
      "Column: 4, Selected False, Rank: 29.000\n",
      "Column: 5, Selected False, Rank: 44.000\n",
      "Column: 6, Selected False, Rank: 45.000\n",
      "Column: 7, Selected False, Rank: 97.000\n",
      "Column: 8, Selected False, Rank: 51.000\n",
      "Column: 9, Selected False, Rank: 61.000\n",
      "Column: 10, Selected False, Rank: 59.000\n",
      "Column: 11, Selected False, Rank: 50.000\n",
      "Column: 12, Selected False, Rank: 91.000\n",
      "Column: 13, Selected False, Rank: 101.000\n",
      "Column: 14, Selected False, Rank: 71.000\n",
      "Column: 15, Selected False, Rank: 105.000\n",
      "Column: 16, Selected False, Rank: 129.000\n",
      "Column: 17, Selected False, Rank: 109.000\n",
      "Column: 18, Selected False, Rank: 123.000\n",
      "Column: 19, Selected False, Rank: 130.000\n",
      "Column: 20, Selected False, Rank: 132.000\n",
      "Column: 21, Selected False, Rank: 14.000\n",
      "Column: 22, Selected False, Rank: 70.000\n",
      "Column: 23, Selected False, Rank: 3.000\n",
      "Column: 24, Selected False, Rank: 34.000\n",
      "Column: 25, Selected False, Rank: 12.000\n",
      "Column: 26, Selected False, Rank: 9.000\n",
      "Column: 27, Selected False, Rank: 116.000\n",
      "Column: 28, Selected False, Rank: 31.000\n",
      "Column: 29, Selected False, Rank: 52.000\n",
      "Column: 30, Selected False, Rank: 35.000\n",
      "Column: 31, Selected False, Rank: 74.000\n",
      "Column: 32, Selected False, Rank: 66.000\n",
      "Column: 33, Selected False, Rank: 115.000\n",
      "Column: 34, Selected False, Rank: 73.000\n",
      "Column: 35, Selected False, Rank: 69.000\n",
      "Column: 36, Selected False, Rank: 62.000\n",
      "Column: 37, Selected False, Rank: 39.000\n",
      "Column: 38, Selected False, Rank: 25.000\n",
      "Column: 39, Selected True, Rank: 1.000\n",
      "Column: 40, Selected False, Rank: 26.000\n",
      "Column: 41, Selected False, Rank: 60.000\n",
      "Column: 42, Selected False, Rank: 98.000\n",
      "Column: 43, Selected False, Rank: 86.000\n",
      "Column: 44, Selected False, Rank: 55.000\n",
      "Column: 45, Selected False, Rank: 102.000\n",
      "Column: 46, Selected False, Rank: 8.000\n",
      "Column: 47, Selected False, Rank: 76.000\n",
      "Column: 48, Selected False, Rank: 40.000\n",
      "Column: 49, Selected False, Rank: 81.000\n",
      "Column: 50, Selected False, Rank: 99.000\n",
      "Column: 51, Selected False, Rank: 22.000\n",
      "Column: 52, Selected False, Rank: 85.000\n",
      "Column: 53, Selected False, Rank: 18.000\n",
      "Column: 54, Selected False, Rank: 54.000\n",
      "Column: 55, Selected False, Rank: 111.000\n",
      "Column: 56, Selected False, Rank: 28.000\n",
      "Column: 57, Selected False, Rank: 100.000\n",
      "Column: 58, Selected False, Rank: 93.000\n",
      "Column: 59, Selected False, Rank: 43.000\n",
      "Column: 60, Selected False, Rank: 20.000\n",
      "Column: 61, Selected False, Rank: 64.000\n",
      "Column: 62, Selected False, Rank: 27.000\n",
      "Column: 63, Selected False, Rank: 95.000\n",
      "Column: 64, Selected False, Rank: 92.000\n",
      "Column: 65, Selected False, Rank: 16.000\n",
      "Column: 66, Selected False, Rank: 49.000\n",
      "Column: 67, Selected False, Rank: 104.000\n",
      "Column: 68, Selected False, Rank: 46.000\n",
      "Column: 69, Selected False, Rank: 33.000\n",
      "Column: 70, Selected False, Rank: 75.000\n",
      "Column: 71, Selected False, Rank: 23.000\n",
      "Column: 72, Selected False, Rank: 119.000\n",
      "Column: 73, Selected False, Rank: 106.000\n",
      "Column: 74, Selected False, Rank: 103.000\n",
      "Column: 75, Selected False, Rank: 80.000\n",
      "Column: 76, Selected False, Rank: 67.000\n",
      "Column: 77, Selected False, Rank: 13.000\n",
      "Column: 78, Selected False, Rank: 37.000\n",
      "Column: 79, Selected False, Rank: 78.000\n",
      "Column: 80, Selected False, Rank: 41.000\n",
      "Column: 81, Selected False, Rank: 21.000\n",
      "Column: 82, Selected False, Rank: 87.000\n",
      "Column: 83, Selected False, Rank: 42.000\n",
      "Column: 84, Selected False, Rank: 94.000\n",
      "Column: 85, Selected False, Rank: 24.000\n",
      "Column: 86, Selected False, Rank: 4.000\n",
      "Column: 87, Selected False, Rank: 114.000\n",
      "Column: 88, Selected False, Rank: 32.000\n",
      "Column: 89, Selected False, Rank: 83.000\n",
      "Column: 90, Selected True, Rank: 1.000\n",
      "Column: 91, Selected False, Rank: 82.000\n",
      "Column: 92, Selected True, Rank: 1.000\n",
      "Column: 93, Selected False, Rank: 5.000\n",
      "Column: 94, Selected False, Rank: 121.000\n",
      "Column: 95, Selected False, Rank: 126.000\n",
      "Column: 96, Selected False, Rank: 110.000\n",
      "Column: 97, Selected False, Rank: 68.000\n",
      "Column: 98, Selected False, Rank: 127.000\n",
      "Column: 99, Selected False, Rank: 112.000\n",
      "Column: 100, Selected False, Rank: 128.000\n",
      "Column: 101, Selected False, Rank: 124.000\n",
      "Column: 102, Selected False, Rank: 125.000\n",
      "Column: 103, Selected False, Rank: 36.000\n",
      "Column: 104, Selected False, Rank: 108.000\n",
      "Column: 105, Selected False, Rank: 134.000\n",
      "Column: 106, Selected False, Rank: 17.000\n",
      "Column: 107, Selected False, Rank: 96.000\n",
      "Column: 108, Selected False, Rank: 133.000\n",
      "Column: 109, Selected False, Rank: 77.000\n",
      "Column: 110, Selected False, Rank: 79.000\n",
      "Column: 111, Selected False, Rank: 131.000\n",
      "Column: 112, Selected False, Rank: 118.000\n",
      "Column: 113, Selected False, Rank: 117.000\n",
      "Column: 114, Selected False, Rank: 122.000\n",
      "Column: 115, Selected False, Rank: 107.000\n",
      "Column: 116, Selected False, Rank: 120.000\n",
      "Column: 117, Selected False, Rank: 84.000\n",
      "Column: 118, Selected False, Rank: 48.000\n",
      "Column: 119, Selected False, Rank: 6.000\n",
      "Column: 120, Selected False, Rank: 57.000\n",
      "Column: 121, Selected False, Rank: 30.000\n",
      "Column: 122, Selected False, Rank: 38.000\n",
      "Column: 123, Selected False, Rank: 11.000\n",
      "Column: 124, Selected False, Rank: 90.000\n",
      "Column: 125, Selected False, Rank: 15.000\n",
      "Column: 126, Selected False, Rank: 2.000\n",
      "Column: 127, Selected False, Rank: 53.000\n",
      "Column: 128, Selected False, Rank: 58.000\n",
      "Column: 129, Selected True, Rank: 1.000\n",
      "Column: 130, Selected False, Rank: 113.000\n",
      "Column: 131, Selected False, Rank: 19.000\n",
      "Column: 132, Selected False, Rank: 88.000\n",
      "Column: 133, Selected False, Rank: 89.000\n",
      "Column: 134, Selected True, Rank: 1.000\n",
      "Column: 135, Selected False, Rank: 7.000\n",
      "Column: 136, Selected False, Rank: 47.000\n",
      "Column: 137, Selected False, Rank: 10.000\n",
      "[39, 90, 92, 129, 134]\n"
     ]
    }
   ],
   "source": [
    "relevant_cols = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    if rfe.support_[i] == True:\n",
    "        relevant_cols.append(i)\n",
    "        \n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "12ce8029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that provide the best are: [39, 90, 92, 129, 134]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Pref Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted  Pref Percent\n",
       "3         0          0         56.26\n",
       "70        0          0         58.36\n",
       "92        0          0         53.28\n",
       "124       0          0         57.52\n",
       "56        0          0         62.21\n",
       "134       0          0         54.45\n",
       "107       0          0         71.83\n",
       "78        0          0         52.98\n",
       "128       0          0         53.18\n",
       "65        0          0         52.80\n",
       "42        0          0         51.50\n",
       "123       0          0         55.04\n",
       "111       1          0         65.36\n",
       "0         0          0         58.18\n",
       "136       0          0         64.13\n",
       "98        0          0         50.19\n",
       "90        0          0         55.04\n",
       "114       1          0         64.36\n",
       "108       0          0         57.53\n",
       "131       0          0         54.24\n",
       "101       1          0         72.49\n",
       "68        1          0         70.48\n",
       "95        0          0         58.40\n",
       "119       0          0         56.39\n",
       "64        0          0         66.30\n",
       "77        1          0         62.99\n",
       "71        0          0         53.88\n",
       "150       1          0         64.58\n",
       "7         0          0         57.52\n",
       "103       0          0         55.14\n",
       "53        0          0         58.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Columns that provide the best are:\", relevant_cols)\n",
    "result = pd.DataFrame(test['is_Safe_LNP'])\n",
    "result.columns = [\"Actual\"]\n",
    "result.insert(1, \"Predicted\", rfe.predict(X_test))\n",
    "result.insert(2, \"Pref Percent\", test['PreferencePercent'].values)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e2f6e6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INCP - $3,000 or more ($156,000 or more)</th>\n",
       "      <th>MSTP - Married</th>\n",
       "      <th>RLHP - Husband, Wife or Partner in a registered marriage</th>\n",
       "      <th>RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation</th>\n",
       "      <th>MDCP - Married in a registered marriage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061699</td>\n",
       "      <td>-0.072724</td>\n",
       "      <td>-0.077377</td>\n",
       "      <td>0.120757</td>\n",
       "      <td>-0.07698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.061699</td>\n",
       "      <td>-0.072724</td>\n",
       "      <td>-0.077377</td>\n",
       "      <td>0.120757</td>\n",
       "      <td>-0.07698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   INCP - $3,000 or more ($156,000 or more)  MSTP - Married  \\\n",
       "0                                  0.061699       -0.072724   \n",
       "1                                  0.061699       -0.072724   \n",
       "\n",
       "   RLHP - Husband, Wife or Partner in a registered marriage  \\\n",
       "0                                          -0.077377          \n",
       "1                                          -0.077377          \n",
       "\n",
       "   RELP - Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation  \\\n",
       "0                                           0.120757                                 \n",
       "1                                           0.120757                                 \n",
       "\n",
       "   MDCP - Married in a registered marriage  \n",
       "0                                 -0.07698  \n",
       "1                                 -0.07698  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_DF = pd.DataFrame(list(map(np.ravel, rfe.estimator_.coef_)), columns=X_train.columns[rfe.support_], index=rfe.estimator_.classes_)\n",
    "coefficient_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505bf49b",
   "metadata": {},
   "source": [
    "# 4.6 Other Interpretation (97% Accuracy)\n",
    "\n",
    "## Strongest determinants\n",
    "- Secular Beliefs and Other Spiritual Beliefs and No Religious Affiliation (0.120757)\n",
    "- 3,000 or more( 156,000 or more) (0.061699)\n",
    "\n",
    "## Negative\n",
    "- Husband, Wife or Partner in a registered marriage (-0.077377)\n",
    "- Married in a registered marriage (-0.07698)\n",
    "- Married (-0.072724)\n",
    "\n",
    "Note, like predicting other in the 3 model for other, the coefficients are all very low/ weak hence even though we supposedly have a 97% accuracy, this is due to having very few data points in this category. Would not use this evidence that other is well predicted for reasons as stated in the 3- model of the other category, they are very similar.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aca98b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
